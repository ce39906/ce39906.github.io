{"posts":[{"title":"Sedona 示例","content":"Sedona 尝鲜 背景 Sedona 是一个正在Apache孵化中的用于处理海量空间数据项目，在加入Apache前，Sedona名为Geospark。 Sedona可以利用Maven,SBT构建Java,Scala 项目，同时提供了Python及R语言API。 Sedona架构如下: 示例程序 利用Sedona搜索距离最近的十个公交站。 主要pom.xml配置如下： &lt;properties&gt; &lt;spark.version&gt;3.0.0&lt;/spark.version&gt; &lt;gt-geometry.version&gt; 20.1 &lt;/gt-geometry.version&gt; &lt;scala.tools.version&gt;2.13&lt;/scala.tools.version&gt; &lt;scala.version&gt;2.12.10&lt;/scala.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;${scala.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.locationtech.jts&lt;/groupId&gt; &lt;artifactId&gt;jts-core&lt;/artifactId&gt; &lt;version&gt;1.16.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.geotools&lt;/groupId&gt; &lt;artifactId&gt;gt-geometry&lt;/artifactId&gt; &lt;version&gt;${gt-geometry.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.geotools&lt;/groupId&gt; &lt;artifactId&gt;gt-epsg-hsql&lt;/artifactId&gt; &lt;version&gt;${gt-geometry.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.geotools&lt;/groupId&gt; &lt;artifactId&gt;gt-referencing&lt;/artifactId&gt; &lt;version&gt;${gt-geometry.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.4&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalatest&lt;/groupId&gt; &lt;artifactId&gt;scalatest_${scala.tools.version}&lt;/artifactId&gt; &lt;version&gt;3.2.9&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt; &lt;artifactId&gt;sedona-python-adapter-3.0_2.12&lt;/artifactId&gt; &lt;version&gt;1.0.1-incubating&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt; &lt;artifactId&gt;sedona-viz-3.0_2.12&lt;/artifactId&gt; &lt;version&gt;1.0.1-incubating&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 主要代码如下： import org.apache.sedona.core.serde.SedonaKryoRegistrator import org.apache.sedona.core.spatialOperator.KNNQuery import org.apache.spark.serializer.KryoSerializer import org.apache.spark.sql.SparkSession import org.apache.sedona.sql.utils.{Adapter, SedonaSQLRegistrator} import org.apache.spark.sql.types.StringType import org.apache.spark.sql.functions.{col, udf} /** * @time 9/15/21 5:25 PM * @author ce39906 * @email ce39906@163.com */ object GeoSparkDemo { def main(args: Array[String]): Unit = { val sparkSession = SparkSession.builder() .enableHiveSupport() .config(&quot;spark.serializer&quot;, classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer .config(&quot;spark.kryo.registrator&quot;, classOf[SedonaKryoRegistrator].getName) .getOrCreate() import sparkSession.implicits._ sparkSession.sql(&quot;set hive.exec.dynamic.partition=true&quot;) sparkSession.sql(&quot;set hive.exec.dynamic.partition.mode=nonstrick&quot;) SedonaSQLRegistrator.registerAll(sparkSession) val table = &quot;test_database.test_table&quot; val sql = &quot;&quot;&quot;select city_id, name, location from |%s where dt = '20210630' &quot;&quot;&quot;.stripMargin.format(table) var platformDf = sparkSession.sql(sql) def locationValid(location: String) : Int = { val latlon = location.split(&quot;,&quot;) if (latlon.length != 2) { return 0 } val lat = latlon(0).toDouble val lon = latlon(1).toDouble val res = lon &gt; -180.0 &amp;&amp; lon &lt; 180.0 &amp;&amp; lat &gt; -90.0 &amp;&amp; lat &lt; 90.0 if (res) 1 else 0 } val UDFLocationValid = udf(locationValid _) platformDf = platformDf.withColumn(&quot;valid&quot;, UDFLocationValid(col(&quot;location&quot;))) .where(&quot;valid == 1&quot;) def createWktPoint(latlon: String) : String = { val latlonArr = latlon.split(&quot;,&quot;) val lat = latlonArr(0) val lon = latlonArr(1) val lonlat = lon + &quot;,&quot; + lat GeoUtils.createWKTPoint(latlon) } val UDFCreateWktPoint = udf(createWktPoint _) val df = platformDf.withColumn(&quot;wkt_point&quot;, UDFCreateWktPoint(col(&quot;location&quot;))) df.createOrReplaceTempView(&quot;rawdf&quot;) val spatialDf = sparkSession.sql( &quot;&quot;&quot;select ST_Transform(ST_GeomFromWKT(wkt_point), &quot;epsg:4326&quot;, &quot;epsg:3857&quot;) as platform_location, | name, city_id from rawdf &quot;&quot;&quot;.stripMargin ) spatialDf.createOrReplaceTempView(&quot;spatialdf&quot;) val targetPlatform = &quot;POINT (40.04004494170224 116.42194976190544)&quot; val knnSql = &quot;&quot;&quot;select name, |ST_Distance(ST_Transform(ST_GeomFromWKT(&quot;%s&quot;), &quot;epsg:4326&quot;, &quot;epsg:3857&quot;), platform_location) as distance |from spatialdf |order by distance |limit 10 &quot;&quot;&quot;.stripMargin.format(targetPlatform) val knnDf = sparkSession.sql(knnSql) knnDf.show(10, truncate = false) } } 需要注意的是，Sedona计算点间距离是基于平面坐标系的，如果是GCJ02坐标的话需要首先转为WGS84坐标，然后将WGS84坐标转为平面坐标，关于坐标系介绍见epsg 上述代码段中ST_GeomFromWKT(&quot;%s&quot;), &quot;epsg:4326&quot;, &quot;epsg:3857&quot;)的作用就是将WGS84坐标转为平面坐标。 ⚠️ Sedona在处理经纬度坐标的格式是先纬度后经度 执行结果如下: 参考资料 https://sedona.apache.org/ https://epsg.io/ ","link":"https://ce39906.github.io/post/Sedona-示例/"},{"title":"Python 隐藏函数内print输出","content":"让python 程序清爽一些 背景 在使用一些第三方python 库时,偶尔会出现一些不需要的信息打印到终端。更改第三方代码能暂时解决问题，但是在第三方库更新之后可能又会出现。 方法 利用with方法将在with作用域内的所有函数的stdout都指向到devnull, 离开with作用域后将stdout恢复。 定义如下类： class HiddenPrints: def __enter__(self): self._original_stdout = sys.stdout sys.stdout = open(os.devnull, 'w') def __exit__(self, exc_type, exc_val, exc_tb): sys.stdout.close() sys.stdout = self._original_stdout 定义一个测试方法： class StaticCls(): call_cnt = 0 def test_print(self): StaticCls.call_cnt += 1 print(&quot;This function has been called %s times&quot; % StaticCls.call_cnt) with HiddenPrints(): cls = StaticCls() cls.test_print() cls = StaticCls() cls.test_print() 运行结果如下: 可见虽然test_print() 被执行两次，但是我们在终端只看到第二次调用的输出。 ","link":"https://ce39906.github.io/post/Python-隐藏函数内print输出/"},{"title":"读书短评-《金字塔原理》","content":"结论先行，上下对应，分类清楚，逻辑递进 此图摘自 https://zhuanlan.zhihu.com/p/129970972 这本书将几个观点整理成了一套理论体系，但是本身描述的内容存在很多冗余啰嗦的地方，读起来并不是很好读。 但是不得不承认，书中几个观点确实在工作中的文档或PPT中都很实用，可以提升工作效率。比如: 结论先行：因为老板的时间非常宝贵，没时间看你的文档罗里吧嗦在说什么东西。 上下对应：论据要在论点下展开，不然读者不清楚论据是在说明什么 分类清楚：论点之间要做到互斥原则，不然就会产生冗余信息 逻辑递进：可以按照时间逻辑、结构逻辑、重要性逻辑推进 ","link":"https://ce39906.github.io/post/du-shu-duan-ping-lesslessjin-zi-ta-yuan-li-greatergreater1/"},{"title":"2021新年Flag","content":"步入而立之年，立下一些Flag 待读图书 《金字塔原理》 完成进度 100% 《高效能人士的七个习惯》 完成进度 50% 《学会提问》 完成进度 0% 《用图表说活》 完成进度 0% 《基因传》 完成进度 1% 个人 体重恢复到70kg以内，拒绝油腻 完成进度 -3% ","link":"https://ce39906.github.io/post/2021-xin-nian-flag/"},{"title":"程序员应该拒绝使用baidu","content":"RT 给出几个搜索词感受一下 python C++ 机器学习 ","link":"https://ce39906.github.io/post/程序员应该拒绝使用baidu/"},{"title":"Spark collect_list 保留空值","content":"Spark collect_list 的坑 collect_list 的坑 Spark 按照某列group 之后，使用collect_list 将特定列收集到一个数组的时候，默认情况会过滤掉空值(包括null 和 空字符串)。 比如我们有如下dataframe name sex age John male 18 null male 19 Lily female null null female 20 Tom male 35 根据sex字段 group df.groupBy(&quot;sex&quot;).agg(collect_list(&quot;name&quot;).alias(&quot;names&quot;), collect_list(&quot;age&quot;).alias(&quot;ages&quot;)) 会得到以下结果: sex names ages male [&quot;John&quot;, &quot;Tom&quot;] [18, 35, 19] female [&quot;Lily&quot;] [20] 也就是说，spark 在collect_list 的时候把空值过滤掉。 解决方案 解决方案比较简单粗暴，就是将dataframe 里面的空值替换为非空值。 最简单的方法是使用下面的语句，但是直接使用na.fill 只能将schemaType 和fill 传入参数类型一致的列空值替换，schemaType不一致的null 值无法生效。 df = df.na.fill(&quot;#&quot;) 正确的做法是针对每一个不同的列类型指定替换后的数值 val typeMap = df.dtypes.map(column =&gt; column._2 match { case &quot;IntegerType&quot; =&gt; (column._1 -&gt; 0) case &quot;StringType&quot; =&gt; (column._1 -&gt; &quot;&quot;) case &quot;DoubleType&quot; =&gt; (column._1 -&gt; 0.0) }).toMap df = df.na.fill(typeMap) 使用以下语句检查df 中是否存在空值 val filtered = df.filter(row =&gt; !row.anyNull) assert(filtered.count() == df.count()) 再次group 之后，会得到以下结果: sex names ages male [&quot;John&quot;, &quot;Tom&quot;, &quot;#&quot;] [18, 35, 19] female [&quot;Lily&quot;,&quot;#&quot;] [0, 20] ","link":"https://ce39906.github.io/post/Spark-collect-list-keep-null/"},{"title":"Spark repartition 和 coalesce","content":"Spark 的两个常用操作 repartition repartition 将全局数据进行shuffle为指定分区大小。spark 可以根据用户指定的列或者特定的分区数目以及指定的hash函数进行数据分区。 coalesce coalesce 通过分区数据移动的方式达到降低数据分区的效果，不会增加数据分区。相比repartition 而言，coalesce发生的数据混洗更小，是一个更高效的操作。 示例 假设原始分区如下 node1 = 1, 2, 3 node2 = 4, 5, 6 node3 = 7, 8, 9 node4 = 10, 11, 12 repartition(2) 之后可能会变成如下 node1 = 4, 1, 3, 2, 5, 6 node2 = 8, 9, 7, 12, 10, 11 coalesce(2) 之后可能会变成如下 node1 = 1, 2, 3 + (10, 11, 12) node3 = 7, 8, 9 + (4, 5, 6) 基于上面的原理，我们封装一个函数用于将DataFrame 转化为指定分区 def repartition_df(df, num_partitions): cur_partitions = df.rdd.getNumPartitions() if cur_partitions = num_partitions: return df if cur_partitions &gt; num_partitions: df = df.coalesce(num_partitions) else: df = df.repartition(num_partitions) return df ","link":"https://ce39906.github.io/post/Spark-repartition-和-coalesce/"},{"title":"XGBoost4j 调用python训练好的模型","content":"XGBoost工程化中的一个问题 概述 使用python xgboost 训练lambda rank 排序模型，线上工程化时使用XGBoost4j 调用python 已经训练好的模型进行线上预测。 python xgb 训练以及保存模型 def train_xgb_model(x_train, y_train, group_train, x_valid, y_valid, group_valid): params = { 'objective' : 'rank:ndcg', 'learning_rate' : 0.1, 'gamma' : 1.0, 'nthread': 60, 'min_child_weight' : 0.1, 'max_depth' : 8, 'n_estimators' : 5, 'reg_alpha' : 0.01, 'reg_lambda' : 0.01 } model = xgb.sklearn.XGBRanker(**params) model.fit(x=x_train, y=y_train, group=group_train, eval_set=[(x_valid, y_valid)], eval_group=[group_valid], verbose=True) #保存模型 model.save_model('xgb_model.bin') return model python 加载模型及预测 # 加载模型 model = xgb.Booster(model_file='xgb_model.bin') # 进行预测 test_data = np.asarray(feature_arr) test_data = xgb.DMatrix(test_data) rank_score = model.predict(test_data) XGBoost4j 调用模型及预测 //加载模型 Booster booster = XGBoost.loadModel(&quot;xgb_model.bin&quot;); //使用特征数据构造Dmatrix，多行样本flatten 为一维数组，row, col 代表二维数组长宽。第四个参数代表missing value 的初始值，python 默认值为Nan，java默认值是0。这个参数不传的话可能会导致python 与java 预测结果不一致。 DMatrix dMatrix = new DMatrix(test_data, row, col, Float.NaN); //进行预测 float[][] predicts = booster.predict(dMatrix); ","link":"https://ce39906.github.io/post/XGBoost4j-调用python训练好的模型/"},{"title":"读书短评-《以色列：一个国家的诞生》","content":"一个令人敬佩的国度 大英帝国在没落之后无力管辖众多殖民地，为了获得美国犹太人的支持，英国索性将巴勒斯坦地区扔给联合国，联合国简单的将巴勒斯坦地区均分给巴勒斯坦人和犹太人，允许犹太人建立属于自己的国家。 有联合国一纸协议并没办法建国，政权还是从枪杆子里出来的(毛主席说的)。在没有资金、军队、武器装备、军工企业的情况下，以色列出现各种强人，依托强大的民族主义精神以及强大宗教信仰，在周边十几个伊斯兰国家包围下历经五次中东战争艰难生存下来。 最后说两个和书里内容无关的两个小段子，一个是前段时间川大统领承认了耶鲁撒冷是以色列的首都(这让伊斯兰国家都炸毛了)，老川也是聪明人，承认这个之后那些犹太参议员瞬间清净不弹劾大统领了； 另一个小段子是关于长者的，以色列地方沙漠化很严重，但是有先进的滴灌技术，反而成为欧洲主要水果出口国。长者用一句不否认以色列是一个独立的国家换取了先进的滴灌技术在新疆得到了广泛应用。 ","link":"https://ce39906.github.io/post/读书短评-《以色列：一个国家的诞生》/"},{"title":"Floyd Warshall Algorithm","content":"经典路径规划算法之一 Floyd算法是为了解决有向图中所有点对之间最短距离问题。 算法描述 Floyd算法属于动态规划算法，算法的思路是我们以遍历每个节点k 作为中间节点，对于每个起终点对(i， j)来说，有以下两种可能情况： k 不是从i 到 j 最短路径的中间节点，不更新 dist[i][j] k 是从i 到 j 最短路径的中间节点，将dist[i][j] 更新为 dist[i][k] + dist[k][j] 示例 我们以以下有向图为例： 我们使用一下二维数组表示该图 vector&lt;vector&lt;int&gt;&gt; graph = {{0, 5, INT_MAX, 10}, {INT_MAX, 0, 3, INT_MAX}, {INT_MAX, INT_MAX, 0, 1}, {INT_MAX, INT_MAX, INT_MAX, 0}}; C++实现 /************************************************************************* &gt; File Name: floyd.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2019-05-17 11:05:58 ************************************************************************/ #include &lt;vector&gt; #include &lt;iostream&gt; #include &lt;iomanip&gt; using namespace std; void printResult(const vector&lt;vector&lt;int&gt;&gt;&amp; dist) { cout &lt;&lt; &quot;The following matrix shows the shortest distances &quot; &quot;between each pair of vertices&quot; &lt;&lt; endl; const int n = dist.size(); for (int i = 0; i &lt; n; i++) { for (int j = 0; j &lt; n; j++) { if (dist[i][j] == INT_MAX) { cout &lt;&lt; setw(10) &lt;&lt; &quot;INT_MAX&quot;; } else { cout &lt;&lt; setw(10) &lt;&lt; dist[i][j]; } } cout &lt;&lt; endl; } } void floydWarshall(const vector&lt;vector&lt;int&gt;&gt;&amp; graph) { const int v = graph.size(); vector&lt;vector&lt;int&gt;&gt; dist = graph; // main dp loop for (int k = 0; k &lt; v; k++) { for (int i = 0; i &lt; v; i++) { if (dist[i][k] == INT_MAX) { continue; } for (int j = 0; j &lt; v; j++) { if (dist[k][j] == INT_MAX) { continue; } if (dist[i][k] + dist[k][j] &lt; dist[i][j]) { dist[i][j] = dist[i][k] + dist[k][j]; } } } } printResult(dist); } int main() { vector&lt;vector&lt;int&gt;&gt; graph = {{0, 5, INT_MAX, 10}, {INT_MAX, 0, 3, INT_MAX}, {INT_MAX, INT_MAX, 0, 1}, {INT_MAX, INT_MAX, INT_MAX, 0}}; floydWarshall(graph); return 0; } 运行结果 编译 g++ --std=c++11 floyd.cpp -o floyd 运行结果 ","link":"https://ce39906.github.io/post/Floyd-Warshall-Algorithm/"},{"title":"配置jupyter web server","content":"流行的在线notebook工具 背景 开发机操作系统为centos 6.5, 在需要使用python 绘图功能时，需要生成图片再把图片download 到本地机器查看，使用jupyter 配置linux server 可以直接在本地通过浏览器运行python 任务，绘图的结果也可以通过浏览器直接查看。本博客介绍如何在linux环境下配置jupyter web server 配置 首先生成jupyter 配置 jupyter notebook --generate-config 生成sha 秘钥 &gt;&gt;&gt; from notebook.auth import passwd &gt;&gt;&gt; passwd() &gt;&gt;&gt; Enter password: &gt;&gt;&gt; Verify password: &gt;&gt;&gt; 'sha1:5eb7241****************8e91d9ca9d53b' 编辑jupyter 配置文件 vim ~/.jupyter/jupyter_notebook_config.py 配置ip, passwd, 以及open_browser 为False (因为我们的jupyter服务部署在服务器), 找到此三项配置后注释掉前面的#，变更为以下配置 c.NotebookApp.ip='0.0.0.0' c.NotebookApp.password = u'sha1:5eb7241****************8e91d9ca9d53b' c.NotebookApp.open_browser = False 保存配置文件，启动服务 nohup jupyter notebook &gt; jupyter.log 2&gt;&amp;1 &amp; 从本地浏览器登录jupyter 安装插件 安装jupyter-tabnine插件实现jupyter notebook 自动补全 pip3 install jupyter-tabnine jupyter nbextension install --py jupyter_tabnine jupyter nbextension enable --py jupyter_tabnine jupyter serverextension enable --py jupyter_tabnine 结果如下: ","link":"https://ce39906.github.io/post/pei-zhi-jupyter-web-server/"},{"title":"C++11新特性总结","content":"C++11 已经过去十年了 C++11相对C++03带来的改进主要包括核心语言以及标准库扩充两方面 核心语言 语言现代化简洁化 类型推导 auto : 自动类型推导，本质是类型修饰符，常用于声明迭代器类型，滥用会导致代码可读性下降 delctype : 是一个类型推导的运算符，常用于模板实例化时返回值类型推导 模板细节改进 连续两个右尖括号：之前会与右移运算符冲突(新标准前已被一些编译器修复) 模板别名：using 可以完全替代typedef, 且改善之前typedef 和模板配合不好的问题 函数模板默认参数：C++03 只有类模板允许有默认参数，C++11 函数模板也支持默认参数 列表初始化 以初始化一个vector 为例 //c++03 vector&lt;int&gt; v; v.push_back(2); v.push_back(3); v.push_back(5); //c++11 vector&lt;int&gt; v = {2, 3, 5}; foreach loop 以遍历一个vector为例 //c++03 vector&lt;int&gt; v; for (inst i = 0; i &lt; v.size(); i++) { } //c++11 for (int a : v) { } std::function 以及bind std::function : c++ 可调用对象包括函数指针，函数对象，可被转化为函数指针的类对象, std::function 将这些可调用对象统一包装起来 bind: 将可调用对象与函数参数对象进行绑定。可以用于将不同函数转化为同一函数签名(用于线程池) lambda 表达式 函数声明变的简洁 方便配合标准库 实现功能闭包 tuple 元组 pair 只能支持两个元素，tuple 支持多个元素 提升程序性能 右值引用和移动构造 右值引用：右值引用解决了左值引用无法传递临时对象和常引用对象。右值引用允许传递一个可变的临时对象引用。主要配合移动构造。 移动构造：使用移动而非复制语义完成构造过程，主要用于解决从函数返回值时的大量深拷贝开销。使用右值引用和移动构造，在按值传递和返回临时对象时，即可免去不必要的内存分配和数据拷贝开销 move: 将左值转化为右值 forward以及完美转发：一个右值引用的参数，在函数内部转发该参数的时候，这个参数变为左值。所谓完美转发是指在函数模板中，完全依照模板的参数类型将参数传递给函数调用的另一个函数 语言改进 成员默认初始值 class A { int a = 1; int b = 2; }; override 和final override : 用于标识指定的虚方法重载了基类中的同名虚方法。该修饰符导致编译时严格检查。避免因函数签名不同而重载失败 final: 用于修饰类或方案，修饰类时表示这个类不能被继承，修饰方法时表示此虚方法已经是最终实现 nullptr 严格代表空指针, 解决了NULL 在int 型以及 指针型参数的函数重载异议的问题 Enum class 强枚举类型杜绝了不同枚举类型之间的比较或枚举与整形之间的比较，能够在编译时发现更多错误 模板不定长参数 为模板方法提供了类似普通函数的可变数量参数（…）支持，可极大地方便一些特殊模板的构建，例如：用于实现智能指针模板类中的“operator-&gt;*()”（成员指针解引用）操作，以及一些类似于“printf”的模板方法。 新字符串类型 C++11 中新增了 char16_t 和 char32_t 两中类型，用以应对 wchar_t 位宽不确定的问题。又新增了 UTF-8（u8）、UTF-16（u）和 UTF-32（U）字符串字面值定义方法。 thread_local 增加线程本地存储，不必使用编译器提供的thread_local 显示默认和禁用构造方法 C++11 中，用户可以通过“= default”后缀修饰符，为每个类显式指定默认构造函数。也可以通过“= delete”后缀修饰构造函数和赋值等操作（通常用来实现禁止复制的语义）。增加代码可读性。 static_assert assert是运行时检测，static_assert 用于模板实例化时检测 标准库扩充 容器 为容器新增移动构造函数以及在容器中新增元素的移动版本，emplace，emplace_back 新增hash 型容器，unordered_map, unodered_set 新增算法 all_of, any_of, none_of find_if_not copy_if max_element, min_element is_sorted, is_sorted_until 线程支持 新标准库提供了线程，互斥量，条件变量，原子量以及内存屏障的实现 正则表达式 新的标准库支持正则搜索和替换 智能指针 unique_ptr : 保证指针对对象的唯一绑定，只能通过移动语义交换所有权 shared_ptr: 使用一个原子变量进行引用计数，计数为0时销毁对象，在多线程情况下拷贝会增加新能损耗 weak_ptr: 辅助shared_ptr 使用，解决shared_ptr 循环引用问题，也用于观察者模式 随机数生成器 新标准库提供了右值伪随机数生成器 时间库 新标准库提供时间库chrono，支持设置地区时区 ","link":"https://ce39906.github.io/post/C-11新特性总结/"},{"title":"neural-style生成毕加索风格图片","content":"深度学习也可以用来搞艺术 介绍 尝试https://github.com/anishathalye/neural-style.git 生成毕加索风格图片。 结果 原图片 style图片 合成图片 ","link":"https://ce39906.github.io/post/neural-style生成毕加索风格图片/"},{"title":"抓取北邮人论坛招聘版及跳槽版每日帖数","content":"互联网寒冬下的暗中观察 前两周趋势图 模拟登陆 headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, compress', 'Accept-Language': 'en-us;q=0.5,en;q=0.3', 'Cache-Control': 'max-age=0', 'Connection': 'keep-alive', 'X-Requested-With': 'XMLHttpRequest', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36' } def login(): url = 'https://bbs.byr.cn/user/ajax_login.json' login_data = {'id': '****', 'passwd': '******'} session = requests.Session() req = session.post(url, data=login_data, headers=headers) return session 解析页面 使用BeautifulSoup快速定位html标签地址 def parse_page(content): soup = BeautifulSoup(content, features=&quot;lxml&quot;) spans = soup.findAll('span', {'class' : 'n-left'}) text = str(spans[0]) cur_user_prefix = &quot;共有&quot; cur_user_suffix = &quot;人&quot; prefix_idx = text.find(cur_user_prefix) suffix_idx = text.find(cur_user_suffix, prefix_idx) cur_user_num = text[prefix_idx + len(cur_user_prefix) : suffix_idx] max_user_prefix = &quot;最高&quot; max_user_suffix = &quot;人&quot; prefix_idx = text.find(max_user_prefix, suffix_idx) suffix_idx = text.find(max_user_suffix, prefix_idx) max_user_num = text[prefix_idx + len(max_user_prefix) : suffix_idx] cur_post_prefix = &quot;帖数&quot; cur_post_suffix = &quot;&lt;&quot; prefix_idx = text.find(cur_post_prefix, suffix_idx) suffix_idx = text.find(cur_post_suffix, prefix_idx) post_num = text[prefix_idx + len(cur_post_prefix) : suffix_idx] return int(cur_user_num), int(max_user_num), int(post_num) 存储至mysql #-*- coding: utf-8 -*- &quot;&quot;&quot; File Name: hire_info_db.py Author: ce39906 Mail: ce39906@163.com Created Time: 2019-04-13 07:46:29 &quot;&quot;&quot; from db import DB class HireInfo: def __init__(self, board, date, cur_user_num, max_user_num, cur_post_num): self.board = board self.date = date self.cur_user_num = cur_user_num self.max_user_num = max_user_num self.cur_post_num = cur_post_num class HireInfoDB(DB): def __init__(self): DB.__init__(self) self.table = 'hire_info' def insert(self, hire_info): query = (&quot;insert into %s&quot; &quot;(board,date,cur_user_num,max_user_num,cur_post_num)&quot; &quot;values('%s','%s',%d, %d, %d)&quot; ) % (self.table,hire_info.board, hire_info.date, hire_info.cur_user_num, hire_info.max_user_num, hire_info.cur_post_num) self.execute(query) def select_by_board(self, board, days): query = (&quot;select * from %s &quot; &quot;where board = '%s' and to_days(now()) - to_days(date) &lt;= %d &quot; ) % (self.table, board, days) res = self.execute(query) return res 绘制折线图 def plot(data, board): dates = [str(x[2]) for x in data] cur_user_nums = [x[3] for x in data] cur_post_nums = [x[5] for x in data] d = {'dates' : dates, 'cur_user_num' : cur_user_nums, 'cur_post_num' : cur_post_nums} df = pd.DataFrame(d) plt.cla() ax = plt.gca() df.plot(title=board, kind='line', x='dates', y='cur_user_num', color='red', marker='o', label='online_user_num', ax=ax) df.plot(kind='line', x='dates', y='cur_post_num', color='green', marker='x', label='post_num', ax=ax) plt.savefig(board + '.png') 上传至腾讯云 #-*- coding: utf-8 -*- &quot;&quot;&quot; File Name: cos_client.py Author: ce39906 Mail: ce39906@163.com Created Time: 2019-04-12 15:54:31 &quot;&quot;&quot; from qcloud_cos import CosConfig from qcloud_cos import CosS3Client import sys import logging class TencentCloudClient: def __init__(self): secret_id = '***************************' secret_key = '*****************' region = 'ap-beijing' token = None scheme = 'https' config = CosConfig(Region=region, SecretId=secret_id, SecretKey=secret_key, Token=token, Scheme=scheme) self.client = CosS3Client(config) self.url_prefix = 'https://myblog-********.cos.ap-beijing.myqcloud.com/' def upload_file(self, filename): self.client.put_object_from_local_file(Bucket='myblog-*******', LocalFilePath=filename, Key=filename, EnableMD5=False) 完整代码 https://github.com/ce39906/self-practices/tree/master/pycode/crawl_byr_forum ","link":"https://ce39906.github.io/post/抓取北邮人论坛招聘版及跳槽版每日帖数/"},{"title":"读书短评-《动物农场》","content":"讽刺意味极强的童话故事 从这个&quot;童话故事中&quot;看到了革命，纲领，洗脑，政治斗争，个人崇拜，特权阶级。想一想小红粉们挺可怜的，他们最后很可能会像书中&quot;拳击手&quot;的结局一样，但是&quot;拳击手&quot;是不是一直挺快乐的？ ","link":"https://ce39906.github.io/post/读书短评-《动物农场》/"},{"title":"Python 绘制指数相关性热力图","content":"韭菜的自我修养 数据获取 全部指数数据从中证指数公司获取 加载数据 使用pandas 解析所有excel, 保存每个指数对应的成分股的权重，数据中的一些指数只包含前十大权重股的权重，对这些指数中其他成分股的权重按照平均分配的方式设置。 load_xls.py #-*- coding: utf-8 -*- &quot;&quot;&quot; File Name: load_xls.py Author: ce39906 mail: ce39906@163.com Created Time: 2019-03-01 11:35:07 &quot;&quot;&quot; import pandas as pd import numpy as np def load_close_weight_xls(filename): print(&quot;loading &quot;, filename) df = pd.read_excel(filename, converters={'指数代码Index Code' : lambda x: str(x)}) index_name = df['指数名称Index Name'].values[0] index_code = df['指数代码Index Code'].values[0] constituent_code = df['成分券代码Constituent Code'].values weight = df['权重(%)Weight(%)'].values n = weight.shape[0] constituent_code_2_weight = {} for i in range(n): constituent_code_2_weight[constituent_code[i]] = weight[i] return index_name, index_code, constituent_code_2_weight def load_cons_xls(filename): print(&quot;loading &quot;, filename) sheet1_df = pd.read_excel(filename, converters={'指数代码Index Code' : lambda x: str(x)}) index_name = sheet1_df['指数名称Index Name'].values[0] index_code = sheet1_df['指数代码Index Code'].values[0] constituent_code = sheet1_df['成分券代码Constituent Code'].values sheet2_df = pd.read_excel(filename, sheet_name='weight') top10_code_2_weight = {} for index, row in sheet2_df.iterrows(): code = row['代码'] weight = row['权重'] top10_code_2_weight[code] = weight total_count = constituent_code.shape[0] top10_weight_count = len(top10_code_2_weight.keys()) top10_weight_sum = 0.0 for top10_weight in top10_code_2_weight.values(): top10_weight_sum += top10_weight default_weight = (100.0 - top10_weight_sum) / (total_count - top10_weight_count) constituent_code_2_weight = {} for i in range(total_count): code = constituent_code[i] if code in top10_code_2_weight: constituent_code_2_weight[code] = top10_code_2_weight[code] else: constituent_code_2_weight[code] = default_weight return index_name, index_code, constituent_code_2_weigh 向量化以及绘制热力图 向量化的方式是统计出所有指数全部成分股作为向量的全部维度。 针对每个指数对应的向量，首先将向量值全部设置为0，然后在向量上为成分股对应的维度赋值。 将每个指数都向量化后，合并到一个pandas dataframe中，dataframe中的每一列代表一个指数对应的向量。获取dataframe后使用**corr()**方法获得协方差矩阵，然后使用seaborn 绘制。 cal_stock_indices_correlation.py #-*- coding: utf-8 -*- &quot;&quot;&quot; File Name: cal_stock_indices_correlation.py Author: ce39906 mail: ce39906@163.com Created Time: 2019-03-01 14:31:30 &quot;&quot;&quot; import load_xls import os import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt plt.rcParams['font.sans-serif'] = ['SimHei'] plt.rcParams['axes.unicode_minus'] = False def load_data(): stock_indices_data = {} constituent_codes = set() for filename in os.listdir('data'): if not filename.endswith('.xls'): continue if len(filename) == 21: index_name, index_code, constituent_code_2_weight = load_xls.load_close_weight_xls('data/' + filename) else: index_name, index_code, constituent_code_2_weight = load_xls.load_cons_xls('data/' + filename) for constituent_code in constituent_code_2_weight.keys(): constituent_codes.add(constituent_code) stock_indices_data[index_name + '_' + str(index_code)] = constituent_code_2_weight return list(constituent_codes), stock_indices_data def vectorization(constituent_codes, stock_indices_data): n = len(constituent_codes) df = pd.DataFrame() for stock_index_name, weights in stock_indices_data.items(): vector = [0.0] * n for i in range(n): code = constituent_codes[i] if code in weights: vector[i] = weights[code] / 100.0 df[stock_index_name] = vector return df def plot_corr_heatmap(df): corr = df.corr() f, ax = plt.subplots(figsize=(15, 15)) sns.heatmap(corr, cmap='rainbow', linewidths=0.05, ax=ax, square=True, annot=True) f.savefig('stock_indices_corr_heatmap.png') def main(): constituent_codes, stock_indices_data = load_data() df = vectorization(constituent_codes, stock_indices_data) plot_corr_heatmap(df) if __name__ == '__main__': main() 热力图 ","link":"https://ce39906.github.io/post/Python-绘制指数相关性热力图/"},{"title":"C++ DFA实现敏感词过滤","content":"我也是一个懂状态机的男人了 DFA 算法简介 DFA算法全称是Deterministic finite automaton，翻译为中文是”确定有限状态机“。 此算法可以用于敏感字符的模式匹配，敏感词集合可以确定有限个状态，每个敏感词中的一个字符代表状态机中的一个状态。敏感词中的相邻字符代表相邻状态，前一个字符对应的状态可以转移到后一个字符对应的状态。除此之外，敏感词的最后一个字符对应的状态还需一个结束标识。 在进行匹配时顺序遍历文本，在状态机中查找相应状态，遇到结束标识时表示匹配到一个敏感词，记录匹配的长度，文本偏移相应的长度继续查找直到遍历到文本结尾。 C++ 代码实现 words_filter.hpp /************************************************************************* &gt; File Name: words_filter.hpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2019-02-25 15:05:00 ************************************************************************/ #ifndef WORDS_FILTER_HPP #define WORDS_FILTER_HPP #include &lt;map&gt; #include &lt;vector&gt; #include &lt;set&gt; #include &lt;string&gt; struct TreeNode { using TreeMap = std::map&lt;unsigned char, TreeNode*&gt;; TreeNode() { c_ = '0'; is_end_ = false; } TreeNode(unsigned char c, bool is_end) : c_(c), is_end_(is_end) { } TreeNode* findChild(const unsigned char next_char) const { if (subtree_map_.count(next_char)) { return subtree_map_.at(next_char); } return nullptr; } // insert and return child node TreeNode* insertChild(const unsigned char next_char) { // already have the child if (findChild(next_char)) { return nullptr; } TreeNode* child = new TreeNode(next_char, false); if (child == nullptr) { return nullptr; } subtree_map_.insert(std::make_pair(next_char, child)); return child; } // keyword unsigned char c_; // end flag bool is_end_; // subtree TreeMap subtree_map_; }; class WordsFilterTree { public: WordsFilterTree(const std::vector&lt;std::string&gt;&amp; sensitive_words); bool addSensitiveWord(const std::string&amp; sensitive_word); std::set&lt;std::string&gt; findAllSensitiveWords(const std::string&amp; text, const int match_type = 2) const; std::string replaceAllSensitiveWords(const std::string&amp; text, const bool unix_shell_colored = true, const int match_type = 2, const unsigned char replaced_char = '*') const; private: TreeNode root_; bool insert(TreeNode* parent, const std::string&amp; sensitive_word); int checkSensitiveWord(const TreeNode* node, const std::string&amp; text, const int begin_index, const int match_type) const; }; static const int kMinMatch = 1; static const int kMaxMatch = 2; static const int kBoldRedANSIColorCodeLen = 11; static const std::string kBoldRedANSIColorCodePrefix = &quot;\\033[1;31m&quot;; static const std::string kBoldRedANSIColorCodeSuffix = &quot;\\033[0m&quot;; static int utf8StringLen(const std::string&amp; word) { const char* s = word.c_str(); int len = 0; while (*s) len += (*s++ &amp; 0xc0) != 0x80; return len; } #endif words_filter.cpp /************************************************************************* &gt; File Name: words_filter.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2019-02-26 14:17:18 ************************************************************************/ #include &quot;words_filter.hpp&quot; WordsFilterTree::WordsFilterTree(const std::vector&lt;std::string&gt;&amp; sensitive_words) { for (const std::string&amp; sensitive_word : sensitive_words) { insert(&amp;root_, sensitive_word); } } bool WordsFilterTree::addSensitiveWord(const std::string&amp; sensitive_word) { return insert(&amp;root_, sensitive_word); } std::set&lt;std::string&gt; WordsFilterTree::findAllSensitiveWords(const std::string&amp; text, const int match_type) const { std::set&lt;std::string&gt; matched_words; int begin_index = 0; const int n = text.size(); while (begin_index &lt; n) { int match_len = checkSensitiveWord(&amp;root_, text, begin_index, match_type); if (match_len == 0) { begin_index++; } else { matched_words.insert(text.substr(begin_index, match_len)); begin_index += match_len; } } return matched_words; } std::string WordsFilterTree::replaceAllSensitiveWords(const std::string&amp; text, const bool unix_shell_colored, const int match_type, const unsigned char replaced_char) const { //get one copy std::string replaced_text = text; int begin_index = 0; int shift_len = 0; const int n = text.size(); while (begin_index &lt; n) { int match_len = checkSensitiveWord(&amp;root_, text, begin_index, match_type); if (match_len == 0) { begin_index++; } else { const std::string matched_word = text.substr(begin_index, match_len); int utf8Len = utf8StringLen(matched_word); std::string replaced_str = std::string(utf8Len, replaced_char); if (unix_shell_colored) { replaced_str = kBoldRedANSIColorCodePrefix + replaced_str + kBoldRedANSIColorCodeSuffix; } replaced_text = replaced_text.substr(0, begin_index - shift_len) + replaced_str + replaced_text.substr(begin_index + match_len - shift_len); begin_index += match_len; shift_len += match_len - utf8Len; shift_len -= unix_shell_colored ? kBoldRedANSIColorCodeLen : 0; } } return replaced_text; } bool WordsFilterTree::insert(TreeNode* parent, const std::string&amp; sensitive_word) { const int n = sensitive_word.size(); for (int i = 0; i &lt; n; i++) { if (!parent) { return false; } const unsigned char c = sensitive_word[i]; TreeNode* child = parent-&gt;findChild(c); if (!child) { parent = parent-&gt;insertChild(c); } else { parent = child; } if (i == n - 1) { parent-&gt;is_end_ = true; } } return true; } int WordsFilterTree::checkSensitiveWord(const TreeNode* node, const std::string&amp; text, const int begin_index, const int match_type) const { bool flag = false; int match_len = 0; const int n = text.size(); for (int i = begin_index; i &lt; n; i++) { const unsigned char c = text[i]; node = node-&gt;findChild(c); if (!node) { break; } else { match_len++; if (node-&gt;is_end_) { flag = true; if (match_type == kMinMatch) { break; } } } } if (match_len &lt; 2 || !flag) { match_len = 0; } return match_len; } 测试 测试代码如下 /************************************************************************* &gt; File Name: test_words_filter.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2019-02-25 16:36:46 ************************************************************************/ #include &lt;iostream&gt; #include &quot;words_filter.hpp&quot; using namespace std; int main() { vector&lt;string&gt; sensitive_words = { &quot;全部&quot;, &quot;手机&quot;, &quot;追逐&quot;, &quot;haha&quot;, &quot;Indian&quot;, &quot;Force&quot; }; string text = &quot;如果流浪是你的天赋，那么你一定是我最美的追逐，&quot; &quot;如果爱情是你的游牧拥有过是不是该满足，谁带我踏&quot; &quot;上孤独的丝路，追逐你的脚步，谁带我离开孤独的丝&quot; &quot;路感受你的温度，我将眼泪流成天山上的湖，羌笛声&quot; &quot;胡旋舞为你笑为你哭，爱上你的全部放弃我的全部，&quot; &quot;爱上了你之后我开始领悟，陪你走了一段最唯美的国&quot; &quot;度.Pakistan Armed Forces spokesperson, Major G&quot; &quot;eneral Asif Ghafoor, on Twitter claimed Indian&quot; &quot; Air Force violated the Line of Control. Pakis&quot; &quot;tan Air Force immediately scrambled. Indian aircraf&quot;; WordsFilterTree words_filter(sensitive_words); cout &lt;&lt; &quot;Init sensitive words are :&quot; &lt;&lt; endl; for (const string&amp; sensitive_word : sensitive_words) { cout &lt;&lt; kBoldRedANSIColorCodePrefix &lt;&lt; sensitive_word &lt;&lt; kBoldRedANSIColorCodeSuffix &lt;&lt; &quot; &quot;; } cout &lt;&lt; endl; string word = &quot;温度&quot;; cout &lt;&lt; &quot;Add new sensitive word :&quot; &lt;&lt; endl; cout &lt;&lt; kBoldRedANSIColorCodePrefix &lt;&lt; word &lt;&lt; kBoldRedANSIColorCodeSuffix; words_filter.addSensitiveWord(word); cout &lt;&lt; endl; set&lt;string&gt; matched_words = words_filter.findAllSensitiveWords(text, 1); cout &lt;&lt; &quot;Matched sensitvie words are :&quot; &lt;&lt; endl; for (const string&amp; matched_word : matched_words) { cout &lt;&lt; kBoldRedANSIColorCodePrefix &lt;&lt; matched_word &lt;&lt; kBoldRedANSIColorCodeSuffix &lt;&lt; &quot; &quot;; } cout &lt;&lt; endl; cout &lt;&lt; &quot;Origin text is :&quot; &lt;&lt; endl; cout &lt;&lt; text &lt;&lt; endl; string replaced_text = words_filter.replaceAllSensitiveWords(text); cout &lt;&lt; &quot;Replaced text is :&quot; &lt;&lt; endl; cout &lt;&lt; replaced_text &lt;&lt; endl; return 0; } 代码地址 https://github.com/ce39906/self-practices/tree/master/cppcode/words_filter 编译 g++ -std=c++11 test_words_filter.cpp words_filter.cpp -o words_filter 结果 ./words_filter ","link":"https://ce39906.github.io/post/C-DFA实现敏感词过滤/"},{"title":"以静态库的方式应用LightGBM","content":"在工程上应用LightGBM 介绍 LightGBM 是微软提供的GBDT模型的一种高效实现。使用LightGBM解决问题的通用方式是首先使用python进行模型训练，完成训练后构建C++工程加载模型，实现回归分类或排序。 但目前微软开源的LightGBM实现中开放的Predict接口只支持从文件读取样本并把结果输出到文件的形式。这种形式直接部署到线上服务时需要将样本写入文件，开启LightGBM子进程，读取结果文件这三个流程。 为了解决以上问题，我对LightGBM的文件结构进行了调整，提供了直接接收一个样本或批量样本进行预测的接口。并把整个工程编译为静态库，线上服务只需要引入静态库以及头文件就可以将LightGBM集成。 代码调整及接口 代码主要调整包括将predictor.hpp移动到include/LightGBM下，在application.h中提供以下预测接口 std::string Predict(const char* sample, const char sep); std::string Predict(const std::vector&lt;std::pair&lt;int, double&gt;&gt;&amp; indexed_features); std::string Predict(const std::vector&lt;double&gt;&amp; features); std::vector&lt;std::string&gt; BatchPredict(const std::vector&lt;std::vector&lt;double&gt;&gt;&amp; batch_features); 在配置中新增online_predict任务类型。 具体实现见https://github.com/ce39906/LightGBM 应用示例 获取代码以及编译 git clone https://github.com/ce39906/LightGBM.git cd LightGBM mkdir -p build &amp;&amp; cd build cmake .. make 应用静态库及头文件 我们以https://github.com/ce39906/LightGBM/tree/master/examples/regression 提供的回归问题为例，说明如何使用编译好的静态库进行预测。 首先我们在工程目录下新建third_party/lightgbm/bin以及third_party/lightgbm/include目录分别存放静态库文件以及头文件。 将编译好的liblgbm.a 拷贝到third_party/lightgbm/bin下，将源代码include目录下的LightGBM目录拷贝到third_party/lightgbm/bin 下。 编写test.cpp 测试预测接口，测试代码如下 /************************************************************************* &gt; File Name: test.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-12-25 10:42:45 ************************************************************************/ #include &lt;iostream&gt; #include &lt;LightGBM/application.h&gt; int main(int argc, char** argv) { LightGBM::Application app(argc, argv); app.InitPredict(); const char* sample = &quot;0.644,0.247,-0.447,0.862,0.374,0.854,-1.126,-0.790,&quot; &quot;2.173,1.015,-0.201,1.400,0.000,1.575,1.807,1.607,&quot; &quot;0.000,1.585,-0.190,-0.744,3.102,0.958,1.061,0.980,&quot; &quot;0.875,0.581,0.905,0.796&quot;; std::string result; result = app.Predict(sample, ','); std::cout &lt;&lt; result &lt;&lt; std::endl; return 0; } CMakeLists.txt 如下 cmake_minimum_required(VERSION 2.6) PROJECT(lgbm_client C CXX) OPTION(USE_OPENMP &quot;Enable OpenMP&quot; ON) find_package(OpenMP REQUIRED) SET(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}&quot;) SET(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -pthread -g -w -O3 --std=c++11&quot;) SET(LIGHTGBM_LIB ${CMAKE_CURRENT_SOURCE_DIR}/third_party/lightgbm/bin/liblgbm.a) MESSAGE(STATUS &quot;SYSTEM_INCLUDE_PATH ${SYSTEM_INCLUDE_PATH}&quot;) SET(SYSTEM_INCLUDE_PATH /usr/include/) INCLUDE_DIRECTORIES(SYSTEM ${SYSTEM_INCLUDE_PATH}) INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/third_party/lightgbm/include) ADD_EXECUTABLE(lgbm_client test.cpp) TARGET_LINK_LIBRARIES(lgbm_client ${LIGHTGBM_LIB}) 编译好之后运行 ./lgbm_client config=online.conf online.conf 内容如下 task = online input_model= LightGBM_model.txt 运行结果如下 ","link":"https://ce39906.github.io/post/yi-jing-tai-ku-de-fang-shi-ying-yong-lightgbm/"},{"title":"隐式马尔科夫模型实现Map-Matching","content":"微软出品的经典地图匹配算法 马尔科夫模型 马尔科夫过程 马尔科夫过程是一类随机过程，他的原始模型是马尔科夫链。该过程具有如下特性：在目前已知的状态下，它未来的状态不依赖于过去的状态。在现实世界中，有很多过程都是马尔科夫过程，如微粒的布朗运动，传染病受感染的人数，车站的候车人数等。 马尔科夫链 马尔科夫链是数学中具有马尔科夫性质的离散时间随机过程。在该过程中，在给定当前知识或信息情况下，过去对于预测未来的状态是无关的，这种性质叫做无后效性。 时间和状态都是离散的马尔科夫过程称为马尔科夫链。假设状态序列为… xt−2x_{t-2}xt−2​,xt−1x_{t-1}xt−1​,xtx_{t}xt​,xt+1x_{t+1}xt+1​,xt+2x_{t+2}xt+2​…，由马尔科夫链定义可知，时刻xt+1x_{t+1}xt+1​的状态只和xtx_{t}xt​有关，也就是： P(xt+1∣…,xt−2,xt−1,xt)=P(xt+1∣xt)P(x_{t+1}|…, x_{t-2},x_{t-1},x_{t}) = P(x_{t+1}|x_t) P(xt+1​∣…,xt−2​,xt−1​,xt​)=P(xt+1​∣xt​) 上面那个恒等式被称作马尔科夫性质。 马尔科夫模型 马尔科夫模型是一种统计模型，广泛引用在语音识别，词性自动标注，概率文法，地图道路匹配等领域。马尔科夫模型由三个要素组成。 状态 初始状态：在初始时间的各个状态的概率 状态转移矩阵：每种状态转移的概率 隐式马尔科夫模型模型 相比马尔科夫模型来讲，隐式马尔科夫模型中的状态不是明确确定的，新增了一个观测状态的概念，一个实际状态可能对应多个观测状态，实际状态到观测状态的概率称为观测概率或发射概率。下图展示隐式马尔科夫模型的联合概率分布。 隐式马尔科夫模型模型包含以下五要素 实际状态 观测状态 初始状态概率分布 转移概率：实际状态之间转移概率 发射概率：实际状态被观测为特定观测状态的概率，也可以称为观测概率 隐式马尔科夫模型的一个应用是根据一系列观测状态来推测最有可能的实际状态。该问题可以用维特比算法来解决。 维特比算法 维特比算法(Viterbi algorithm)是一种动态规划算法，它用于寻找最有可能观测事件序列的维特比路径(隐含状态序列)，特别是在马尔科夫信息源上下文和马尔科夫模型中。 维特比算法的思想是从开始状态之后没走一步就记录下到达该状态所有路径的概率最大值，然后以此最大值为基准继续向后推进。显然，如果这个最大值都不能使该状态称为最大似然路径上的节点的话，那些小于它的概率值以及对应的路径就更没可能。下图摘自wikipedia，展示Viterbi算法的计算过程。 Map-Matching问题描述 Map-Matching 需要解决的问题是由GPS模块收集的经纬度序列转化为落在实际路网中的经纬度序列。产生此问题的原因是GPS模块上报的经纬度序列中含有大量噪声。 隐式马尔科夫模型实现Map-Matching 针对上面描述的问题，我们认为GPS模块中上报的每个经纬度点为一个观测状态。真实轨迹中落在真实道路上的经纬度点为隐藏状态。这个问题我们可以认为是一个典型的隐式马尔科夫模型，在此模型上应用维特比算法就可以解决map-matching问题。除了观测状态以及隐藏状态外，我们还需要建模发射概率以及转移概率 。下面介绍发射概率以及转移概率的计算方法 发射概率 针对一个观测的经纬度点，可能的实际轨迹点我们通过使用Rtree或者kdtree等空间索引搜索出观测点附近的道路，观测经纬度点到道路的投影点就是可能的实际点，实际点到观测点的直线距离服从均值为0，标准差为σzσ_zσz​的高斯分布。 σzσ_zσz​ 代表GPS的定位精度。 转移概率 转移概率根据当前时间和上一时间观测点之间直线距离和实际点的导航绝对距离差来计算。通过绘制柱状图得知此绝对距离差服从均值为β的指数分布，如下图所示： β的估算方法如下： REFERENCE 《机器学习》- 周志华 Hidden Markov Map Matching Through Noise and Sparseness ","link":"https://ce39906.github.io/post/yin-shi-ma-er-ke-fu-mo-xing-shi-xian-map-matching/"},{"title":"Tensorflow 解决路况状态分类问题","content":"初试Tensorflow 背景 路况在地图渲染时候，会针对不同的拥堵情况选择不同颜色。一般来讲，道路拥堵情况分为三个状态，畅通，拥堵，缓行，分别用绿色，黄色，红色来渲染。 我们面临的问题是，已知道路属性以及通行速度，需要对路况状态进行分类。解决方案是依据第三方路况提供的路况状态以及抓取的高德路况状态来训练一个三分类模型。 特征处理 应用的特征如下 feature description speed 路况速度 maxspeed 道路最大速度 highway_level 道路等级,共有17种可能，使用one-hot-encoding lanes 车道数 oneway 是否是单向路，使用one-hot-encoding 路况状态使用 0-1-2 分别表示缓行-拥堵-畅通 处理好的特征使用**\\t**分割的文本处理，最后一列代表路况状态。 模型训练 模型使用TensorFlow 提供的DNN分类器。代码如下 #-*- coding: utf-8 -*- &quot;&quot;&quot; File Name: traffic_status_classifier.py Author: ce39906 mail: ce39906@163.com Created Time: 2018-09-03 19:11:57 &quot;&quot;&quot; import sys import time import numpy as np import tensorflow as tf FEATURES = [ &quot;speed&quot;, &quot;maxspeed&quot;, &quot;level_1&quot;, &quot;level_2&quot;, &quot;level_3&quot;, &quot;level_4&quot;, &quot;level_5&quot;, &quot;level_6&quot;, &quot;level_7&quot;, &quot;level_8&quot;, &quot;level_9&quot;, &quot;level_10&quot;, &quot;level_11&quot;, &quot;level_12&quot;, &quot;level_13&quot;, &quot;level_14&quot;, &quot;level_15&quot;, &quot;level_16&quot;, &quot;level_17&quot;, &quot;lanes&quot;, &quot;oneway_0&quot;, &quot;oneway_1&quot;] def usage(): print &quot;python %s ${train_data_file}&quot; % (sys.argv[0]) def read_data(train_data_file): xy_list = [] with open(train_data_file, 'r') as f: for line in f: line = line.strip('\\n') content = line.split('\\t') xy = [int(float(x)) for x in content] xy_list.append(xy) # 80% as train data, 20% as test data train_xy = xy_list[ : int(len(xy_list) * 0.8)] test_xy = xy_list[int(len(xy_list) * 0.8) : ] train_x = [x[ : -1] for x in train_xy] train_y = [x[-1] for x in train_xy] test_x = [x[ : -1] for x in test_xy] test_y = [x[-1] for x in test_xy] return train_x, train_y, test_x, test_y def list_2_tf_dataset(train_x, train_y, test_x, test_y): train_x = np.array(train_x) train_y_dataset = np.array(train_y) test_x = np.array(test_x) test_y_dataset = np.array(test_y) train_x_cols = [] for col in train_x.T: train_x_cols.append(col) train_x_dataset = {} for i in range(len(FEATURES)): train_x_dataset[FEATURES[i]] = train_x_cols[i] test_x_cols = [] for col in test_x.T: test_x_cols.append(col) test_x_dataset = {} for i in range(len(FEATURES)): test_x_dataset[FEATURES[i]] = test_x_cols[i] return train_x_dataset, train_y_dataset, test_x_dataset, test_y_dataset def train_input_fn(features, labels, batch_size): dataset = tf.data.Dataset.from_tensor_slices((features, labels)) # Shuffle, repeat, and batch the examples. dataset = dataset.shuffle(1000).repeat().batch(batch_size) return dataset def eval_input_fn(features, labels, batch_size): if labels is None: inputs = features else: inputs = (features, labels) dataset = tf.data.Dataset.from_tensor_slices(inputs) # batch the example dataset = dataset.batch(batch_size) return dataset def main(): if len(sys.argv) != 2: usage() sys.exit() batch_size = 100 steps = 10000 train_data_file = sys.argv[1] # adapt to tensorflow format train_x_list, train_y_list, test_x_list, test_y_list = read_data(train_data_file) train_x, train_y, test_x, test_y = \\ list_2_tf_dataset(train_x_list, train_y_list, test_x_list, test_y_list) feature_columns = [] for key in train_x.keys(): feature_columns.append(tf.feature_column.numeric_column(key = key)) start_time = time.time() classifier = tf.estimator.DNNClassifier( feature_columns = feature_columns, hidden_units = [10, 10], n_classes = 3, model_dir = './saved_model') # train the model classifier.train( input_fn = lambda:train_input_fn(train_x, train_y, batch_size), steps = steps) end_time = time.time() print 'Train DNN Classifier cost %fs.' %(end_time - start_time) # evaluate the model eval_result = classifier.evaluate( input_fn = lambda:eval_input_fn(test_x, test_y, batch_size)) print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result)) # begin to saved the model feature_spec = {'speed' : tf.FixedLenFeature([], tf.int64), 'maxspeed' : tf.FixedLenFeature([], tf.int64), 'level_1' : tf.FixedLenFeature([], tf.int64), 'level_2' : tf.FixedLenFeature([], tf.int64), 'level_3' : tf.FixedLenFeature([], tf.int64), 'level_4' : tf.FixedLenFeature([], tf.int64), 'level_5' : tf.FixedLenFeature([], tf.int64), 'level_6' : tf.FixedLenFeature([], tf.int64), 'level_7' : tf.FixedLenFeature([], tf.int64), 'level_8' : tf.FixedLenFeature([], tf.int64), 'level_9' : tf.FixedLenFeature([], tf.int64), 'level_10' : tf.FixedLenFeature([], tf.int64), 'level_11' : tf.FixedLenFeature([], tf.int64), 'level_12' : tf.FixedLenFeature([], tf.int64), 'level_13' : tf.FixedLenFeature([], tf.int64), 'level_14' : tf.FixedLenFeature([], tf.int64), 'level_15' : tf.FixedLenFeature([], tf.int64), 'level_16' : tf.FixedLenFeature([], tf.int64), 'level_17' : tf.FixedLenFeature([], tf.int64), 'lanes' : tf.FixedLenFeature([], tf.int64), 'oneway_0' : tf.FixedLenFeature([], tf.int64), 'oneway_1' : tf.FixedLenFeature([], tf.int64)} def serving_input_receiver_fn(): serialized_tf_example = tf.placeholder( dtype = tf.string, shape = None, name = &quot;input_example_tensor&quot;) receiver_tensors = {'inputs' : serialized_tf_example} features = tf.parse_example(serialized_tf_example, feature_spec) return tf.estimator.export.ServingInputReceiver(features, receiver_tensors) saved_model_dir = classifier.export_savedmodel( '.', serving_input_receiver_fn = serving_input_receiver_fn) print saved_model_dir if __name__ == '__main__': main() 输出如下 Python应用保存的模型 代码如下 #-*- coding: utf-8 -*- &quot;&quot;&quot; File Name: apply_saved_model.py Author: ce39906 mail: ce39906@163.com Created Time: 2018-09-04 18:48:37 &quot;&quot;&quot; import sys import tensorflow as tf import numpy as np def main(): saved_model_dir = sys.argv[1] with tf.Session() as sess: tf.saved_model.loader.load( sess, [tf.saved_model.tag_constants.SERVING], saved_model_dir) predictor = tf.contrib.predictor.from_saved_model(saved_model_dir) features = tf.train.Features(feature = {'speed' : tf.train.Feature(int64_list = tf.train.Int64List(value = [9])), 'maxspeed' : tf.train.Feature(int64_list = tf.train.Int64List(value = [90])), 'level_1' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_2' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_3' : tf.train.Feature(int64_list = tf.train.Int64List(value = [1])), 'level_4' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_5' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_6' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_7' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_8' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_9' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_10' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_11' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_12' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_13' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_14' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_15' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_16' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'level_17' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'lanes' : tf.train.Feature(int64_list = tf.train.Int64List(value = [3])), 'oneway_0' : tf.train.Feature(int64_list = tf.train.Int64List(value = [0])), 'oneway_1' : tf.train.Feature(int64_list = tf.train.Int64List(value = [1]))}) model_input = tf.train.Example(features = features) model_input = model_input.SerializeToString() output_dict = predictor({&quot;inputs&quot; : [model_input]}) classes_list = output_dict['classes'] scores_list = output_dict['scores'] for scores, classes in zip(scores_list, classes_list): scores = scores.tolist() classes = classes.tolist() max_score = max(scores) max_idx = scores.index(max_score) print &quot;Predicted traffic status is %s&quot; % classes[max_idx] if __name__ == '__main__': main() 输出如下 C++应用保存的模型 TODO ","link":"https://ce39906.github.io/post/tensorflow-jie-jue-lu-kuang-zhuang-tai-fen-lei-wen-ti/"},{"title":"读书短评-《万历十五年》","content":"混乱的明朝历史 上大学时李刚老师推荐过的书籍，前段时间拿出来重新读一下，简单记录一下。 年轻的万历虽然贵为天子，但是还是得仰赖张居正先生。从某种程度上讲，张居正担任着小万历严父的角色，他教导小皇帝要勤俭节约，但是在等到被死后清算的时候万历却发现他敬仰的张先生生前过着极度奢华的生活。张居正的继承者是申时行，这位首辅的特长是和稀泥，他的工作就是调和怠政的万历以及文官们。说到万历的怠证，这是他在对抗文官系统失败之后对整个文官系统的报复，这背后让人感觉到皇帝这个职业的孤独以及对待文官的矛盾立场。 书中还写了几个著名的历史人物，海瑞，戚继光，李贽。作者形容海瑞“他虽然被人仰慕，但没有人按照他的榜样办事，他的一生体现了一个有教养的读书人服务于公众而牺牲自我的精神，但这种精神的实际作用却至为微薄”。相比海瑞，戚继光拥有求实的精神，他从当时的条件出发整顿军队，研究战术，清退了东海沿海的海患。最后一个是李贽，黄仁宇评说他的事迹反应明朝在儒家伦理文化趋于僵化下，思想家的苦闷和困局。 最后，我特意去搜了下作者的履历，发现黄仁宇曾是国名党的军官，当时就想通了为什么书中对戚继光操练的武器战术有很详细的介绍。这是一个非常硬核的历史学家！ ","link":"https://ce39906.github.io/post/读书短评-《万历十五年》/"},{"title":"使用TensorFlow构建卷积神经网络","content":"Tensorflow 手写数字识别 本文介绍使用TensorFlow构建卷积神经网络解决kaggle上的digit-recognizer问题。 数据规格 kaggle提供的数据集来自MNIST上的60000条手写数字数据。数据中每个手写数字图像使用28 * 28 的灰度图表示 。 模型结构 本文展示的CNN模型包括1个输入层，2个卷积层，2个池化层，1个全连接层以及1个大小为10的输出层。卷积层使用Relu activation function引入非线性特性。池化层使用max-pooling，大小为2 * 2, 步长设置为2。输出层使用softmax activation function 输出0到1的浮点数(最后一层所有node结果相加结果为1)。 模型构建过程参照Tensorflow Tutorials 代码实现 &quot;&quot;&quot; File Name: tf_cnn.py Author: ce39906 mail: ce39906@163.com Created Time: 2018-10-25 10:55:56 &quot;&quot;&quot; from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np import pandas as pd import tensorflow as tf from sklearn.preprocessing import StandardScaler tf.logging.set_verbosity(tf.logging.INFO) def cnn_model_fn(features, labels, mode): # input layer input_layer = tf.reshape(features[&quot;x&quot;], [-1, 28, 28, 1]) # convolutional layer 1 conv1 = tf.layers.conv2d( inputs = input_layer, filters = 32, kernel_size = [5, 5], padding = &quot;same&quot;, activation = tf.nn.relu) # pooling layer 1 pool1 = tf.layers.max_pooling2d( inputs = conv1, pool_size = [2, 2], strides = 2) # convolutional layer 2 conv2 = tf.layers.conv2d( inputs = pool1, filters = 64, kernel_size = [5, 5], padding = &quot;same&quot;, activation = tf.nn.relu) # pooling layer 2 pool2 = tf.layers.max_pooling2d( inputs = conv2, pool_size = [2, 2], strides = 2) # dense layer pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64]) dense = tf.layers.dense( inputs = pool2_flat, units = 1024, activation = tf.nn.relu) dropout = tf.layers.dropout( inputs = dense, rate = 0.4, training = mode == tf.estimator.ModeKeys.TRAIN) # logits layer logits = tf.layers.dense( inputs = dropout, units = 10) # do predict predictions = { &quot;classes&quot; : tf.argmax(input = logits, axis = 1), &quot;probabilities&quot; : tf.nn.softmax(logits, name = &quot;softmax_tensor&quot;) } if mode == tf.estimator.ModeKeys.PREDICT: return tf.estimator.EstimatorSpec( mode = mode, predictions = predictions) # calculate loss (both TRAIN and EVAL mode) loss = tf.losses.sparse_softmax_cross_entropy(labels = labels, logits = logits) # configure the Trainning Op if mode == tf.estimator.ModeKeys.TRAIN: optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001) train_op = optimizer.minimize( loss = loss, global_step = tf.train.get_global_step()) return tf.estimator.EstimatorSpec( mode = mode, loss = loss, train_op = train_op) # add evaluation metrics (for EVAL mode) eval_metric_ops = { &quot;accuracy&quot; : tf.metrics.accuracy( labels = labels, predictions = predictions[&quot;classes&quot;])} return tf.estimator.EstimatorSpec( mode = mode, loss = loss, eval_metric_ops = eval_metric_ops) def main(): train_data = pd.read_csv('train.csv') test_data = pd.read_csv('test.csv') labels = np.array(train_data.pop('label')) data = StandardScaler().fit_transform(np.float32(train_data.values)) validation_size = 10000 train_data, valid_data = data[ : -validation_size], data[-validation_size : ] train_labels, valid_labels = labels[: -validation_size], labels[-validation_size : ] test_data = StandardScaler().fit_transform(np.float32(test_data.values)) classifier = tf.estimator.Estimator( model_fn = cnn_model_fn, model_dir = &quot;/tmp/cnn_model&quot;) # Set up logging for predictions tensors_to_log = {&quot;probabilities&quot; : &quot;softmax_tensor&quot;} logging_hook = tf.train.LoggingTensorHook( tensors = tensors_to_log, every_n_iter = 50) # train the model train_input_fn = tf.estimator.inputs.numpy_input_fn( x = {&quot;x&quot; : train_data}, y = train_labels, batch_size = 100, num_epochs = None, shuffle = True) classifier.train( input_fn = train_input_fn, steps = 20000, hooks = [logging_hook]) # evaluate the model and print results eval_input_fn = tf.estimator.inputs.numpy_input_fn( x = {&quot;x&quot; : valid_data}, y = valid_labels, num_epochs = 1, shuffle = False) eval_results = classifier.evaluate(input_fn = eval_input_fn) print (eval_results) # evaluate the model and print results predict_input_fn = tf.estimator.inputs.numpy_input_fn( x = {&quot;x&quot; : test_data}, num_epochs = 1, shuffle = False) preidct_results = classifier.predict(input_fn = predict_input_fn) test_labels = [] for predict_result in preidct_results: test_labels.append(predict_result['classes']) test_labels = np.array(test_labels) submission = pd.DataFrame({'ImageId' : (np.arange(test_labels.shape[0]) + 1),'Label' : test_labels}) submission.to_csv('submission.csv', index = False) if __name__ == '__main__': main() 结果 交叉测试准确率为97.58% 提交到kaggle后准确率为97.35% ","link":"https://ce39906.github.io/post/使用TensorFlow构建卷积神经网络/"},{"title":"Tensorflow 编译及应用C++静态库","content":"RT 背景 目前对Tensorflow的主流应用模式是使用python训练模型，使用c++或者java应用训练好的模型。上篇博客介绍了如何在工程中应用Tensorflow 动态库，本博客介绍如何在工程中应用Tensorflow静态库 编译静态链接库 clone tensorflow git 仓库 git clone https://github.com/tensorflow/tensorflow.git cd tensorflow 进入 tensorflow 工程下contrib/makefile路径 tensorflow/contrib/makefile 运行编译脚本 # MacOS 以及Linux 使用此脚本 ./build_all_linux.sh 脚本执行完成后我们得到tensorflow静态库以及相应头文件。 下一步我们将Tensorflow静态库头文件及Tensorflow依赖的静态库头文件整理到统一路径下，其他c++ 工程就可以应用这些库文件及头文件。 # 将tensorflow 头文件以及库文件收集到此路径 mkdir -p ~/tensorflow_libs/tensorflow # include 路径存放tensorflow主要头文件 mkdir -p ~/tensorflow_libs/tensorflow/include/tensorflow # lib 路径存放编译好的静态库 mkdir -p ~/tensorflow_libs/tensorflow/lib # 拷贝tensorflow 主要头文件 cp -r tensorflow/core ~/tensorflow_libs/tensorflow/include # 拷贝tensoflow 静态库 cp tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a ~/tensorflow_libs/tensorflow/lib # 拷贝tensorflow 第三方头文件 mkdir -p ~/tensorflow_libs/tensorflow/tensorflow_third_party cp -r third_party ~/tensorflow_libs/tensorflow/tensorflow_third_party # 拷贝gen目录下文件 cp -r tensorflow/contrib/makefile/gen/host_obj ~/tensorflow_libs/tensorflow/ cp -r tensorflow/contrib/makefile/gen/proto ~/tensorflow_libs/tensorflow/ cp -r tensorflow/contrib/makefile/gen/protobuf ~/tensorflow_libs/tensorflow/ cp -r tensorflow/contrib/makefile/gen/proto_text ~/tensorflow_libs/tensorflow/ # 拷贝downloads下文件 # eigen3 mkdir -p ~/tensorflow_libs/tensorflow/eigen3 cp -r tensorflow/contrib/makefile/downloads/eigen/Eigen ~/tensorflow_libs/tensorflow/eigen3 cp -r tensorflow/contrib/makefile/downloads/eigen/unsupported ~/tensorflow_libs/tensorflow/eigen3 # absl cp -r tensorflow/contrib/makefile/downloads/absl ~/tensorflow_libs/tensorflow/ # nsyc mkdir -p ~/tensorflow_libs/tensorflow/nsyc/include mkdir -p ~/tensorflow_libs/tensorflow/nsyc/lib # 拷贝nsyc 头文件 cp -r tensorflow/contrib/makefile/downloads/nsync/* ~/tensorflow_libs/tensorflow/nsyc/include # 拷贝nsyc 库文件 cp tensorflow/contrib/makefile/downloads/nsync/builds/default.macos.c++11/libnsync.a ~/tensorflow_libs/tensorflow/nsyc/lib 应用静态链接库 示例python model 我们使用一个简单的model作为示例，该model实现了两个字符串拼接的功能。模型保存到test_model.pb。代码如下 #-*- coding: utf-8 -*- &quot;&quot;&quot; File Name: test_model.py Author: ce39906 mail: ce39906@163.com Created Time: 2018-09-11 17:15:32 &quot;&quot;&quot; import tensorflow as tf a = tf.Variable(&quot;hello &quot;, name = &quot;a&quot;) b = tf.Variable(&quot;tensorflow&quot;, name = &quot;b&quot;) result = tf.add(a, b, name = &quot;result&quot;) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) tf.train.write_graph(sess.graph_def, '.', 'test_model.pb', as_text = False) print result.eval() c++工程加载model 首先我们将整理好的tensorflow 头文件以及静态库文件全部拷贝到c++工程目录，本例中存放tensorflow头文件和库文件的路径为tensorflow #将整理好的tensorflow头文件及库文件全部拷贝到c++工程目录 cp -r ~/tensorflow_libs/tensorflow . 示例c++ 代码如下 /************************************************************************* &gt; File Name: load_model.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-09-08 08:28:51 ************************************************************************/ #include &quot;tensorflow/core/public/session.h&quot; #include &quot;tensorflow/core/platform/env.h&quot; #include &lt;iostream&gt; #include &lt;string&gt; const static std::string kModelPath = &quot;test_model.pb&quot;; int main() { using namespace tensorflow; auto session = NewSession(SessionOptions()); if (session == nullptr) { std::cerr &lt;&lt; &quot;Tensorflow session create failded.\\n&quot;; return -1; } else { std::cout &lt;&lt; &quot;Tensorflow session create success.\\n&quot;; } Status status; // Read in the protobuf graph we exported GraphDef graph_def; status = ReadBinaryProto(Env::Default(), kModelPath, &amp;graph_def); if (!status.ok()) { std::cerr &lt;&lt; &quot;Error reading graph definition from &quot; &lt;&lt; kModelPath &lt;&lt; &quot;: &quot; &lt;&lt; status.ToString(); return -1; } else { std::cout &lt;&lt; &quot;Read graph def success.\\n&quot;; } // Add the graph to the session status = session-&gt;Create(graph_def); if (!status.ok()) { std::cerr &lt;&lt; &quot;Error creating graph: &quot; &lt;&lt; status.ToString(); return -1; } else { std::cout &lt;&lt; &quot;Create graph success.\\n&quot;; } // Set model input Tensor hello(DT_STRING, TensorShape()); hello.scalar&lt;string&gt;()() = &quot;hello&quot;; Tensor tensorflow(DT_STRING, TensorShape()); tensorflow.scalar&lt;string&gt;()() = &quot; tensorflow&quot;; // Apply the loaded model std::vector&lt;std::pair&lt;string, tensorflow::Tensor&gt;&gt; inputs = { { &quot;a&quot;, hello }, { &quot;b&quot;, tensorflow }, }; // input std::vector&lt;tensorflow::Tensor&gt; outputs; // output status = session-&gt;Run(inputs, {&quot;result&quot;}, {}, &amp;outputs); if (!status.ok()) { std::cerr &lt;&lt; status.ToString() &lt;&lt; std::endl; return -1; } else { std::cout &lt;&lt; &quot;Run session successfully&quot; &lt;&lt; std::endl; } // Output the result const auto result = outputs[0].scalar&lt;string&gt;()(); std::cout &lt;&lt; &quot;Result value: &quot; &lt;&lt; result &lt;&lt; std::endl; status = session-&gt;Close(); if (!status.ok()) { std::cerr &lt;&lt; &quot;Session closed success&quot;; return -1; } return 0; } CMakeLists.txt 如下 CMAKE_MINIMUM_REQUIRED(VERSION 2.8) SET(TENSORFLOW_INCLUDE_PATH ${CMAKE_SOURCE_DIR}/tensorflow/include) SET(TENSORFLOW_LIBARY ${CMAKE_SOURCE_DIR}/tensorflow/lib/libtensorflow-core.a) MESSAGE(STATUS &quot;TENSORFLOW_INCLUDE_PATH ${TENSORFLOW_INCLUDE_PATH}&quot;) MESSAGE(STATUS &quot;TENSORFLOW_LIBARY ${TENSORFLOW_LIBARY}&quot;) SET(TENSORFLOW_PROTOBUF_INCLUDE_PATH ${CMAKE_SOURCE_DIR}/tensorflow/protobuf/include) SET(TENSORFLOW_PROTOBUF_LIBRARY_PATH ${CMAKE_SOURCE_DIR}/tensorflow/protobuf/lib) SET(TENSORFLOW_PROTOBUF_LIBRARY ${TENSORFLOW_PROTOBUF_LIBRARY_PATH}/libprotobuf.a) SET(TENSORFLOW_PROTOBUF_LITE_LIBRARY ${TENSORFLOW_PROTOBUF_LIBRARY_PATH}/libprotobuf-lite.a) SET(TENSORFLOW_PROTOC_LIBRARY ${TENSORFLOW_PROTOBUF_LIBRARY_PATH}/libprotoc.a) MESSAGE(STATUS &quot;TENSORFLOW_PROTOBUF_INCLUDE_PATH ${TENSORFLOW_PROTOBUF_INCLUDE_PATH}&quot;) MESSAGE(STATUS &quot;TENSORFLOW_PROTOBUF_LIBRARY_PATH ${TENSORFLOW_PROTOBUF_LIBRARY_PATH}&quot;) SET(TENSORFLOW_NSYNC_INCLUDE_PATH ${CMAKE_SOURCE_DIR}/tensorflow/nsync/include) SET(TENSORFLOW_NSYNC_LIBRARY_PATH ${CMAKE_SOURCE_DIR}/tensorflow/nsync/lib) MESSAGE(STATUS &quot;TENSORFLOW_NSYNC_INCLUDE_PATH ${TENSORFLOW_NSYNC_INCLUDE_PATH}&quot;) MESSAGE(STATUS &quot;TENSORFLOW_NSYNC_LIBRARY_PATH ${TENSORFLOW_NSYNC_LIBRARY_PATH}&quot;) SET(TENSORFLOW_NSYNC_LIBRARY ${TENSORFLOW_NSYNC_LIBRARY_PATH}/libnsync.a) SET(TENSORFLOW_PROTO_INCLUDE_PATH ${CMAKE_SOURCE_DIR}/tensorflow/proto) SET(TENSORFLOW_PROTO_TEXT_INCLUDE_PATH ${CMAKE_SOURCE_DIR}/tensorflow/proto_text) SET(TENSORFLOW_HOST_OBJ_INCLUDE_PATH ${CMAKE_SOURCE_DIR}/tensorflow/host_obj) SET(TENSORFLOW_EIGEN_INCLUDE_PATH ${CMAKE_SOURCE_DIR}/tensorflow/eigen3) SET(TENSORFLOW_ABSL_INCLUDE_PATH ${CMAKE_SOURCE_DIR}/tensorflow/absl) SET(TENSORFLOW_THIRD_PARTY_INCLUDE_PATH ${CMAKE_SOURCE_DIR}/tensorflow/tensorflow_third_party) MESSAGE(STATUS &quot;TENSORFLOW_PROTO_INCLUDE_PATH ${TENSORFLOW_PROTO_INCLUDE_PATH}&quot;) MESSAGE(STATUS &quot;TENSORFLOW_PROTO_TEXT_INCLUDE_PATH ${TENSORFLOW_PROTO_TEXT_INCLUDE_PATH}&quot;) MESSAGE(STATUS &quot;TENSORFLOW_HOST_OBJ_INCLUDE_PATH ${TENSORFLOW_HOST_OBJ_INCLUDE_PATH}&quot;) MESSAGE(STATUS &quot;TENSORFLOW_EIGEN_INCLUDE_PATH ${TENSORFLOW_EIGEN_INCLUDE_PATH}&quot;) MESSAGE(STATUS &quot;TENSORFLOW_ABSL_INCLUDE_PATH ${TENSORFLOW_ABSL_INCLUDE_PATH}&quot;) MESSAGE(STATUS &quot;TENSORFLOW_THIRD_PARTY_INCLUDE_PATH ${TENSORFLOW_THIRD_PARTY_INCLUDE_PATH}&quot;) INCLUDE_DIRECTORIES(${TENSORFLOW_PROTOBUF_INCLUDE_PATH}) INCLUDE_DIRECTORIES(${TENSORFLOW_INCLUDE_PATH}) INCLUDE_DIRECTORIES(${TENSORFLOW_PROTO_INCLUDE_PATH}) INCLUDE_DIRECTORIES(${TENSORFLOW_PROTO_TEXT_INCLUDE_PATH}) INCLUDE_DIRECTORIES(${TENSORFLOW_HOST_OBJ_INCLUDE_PATH}) INCLUDE_DIRECTORIES(${TENSORFLOW_EIGEN_INCLUDE_PATH}) INCLUDE_DIRECTORIES(${TENSORFLOW_ABSL_INCLUDE_PATH}) INCLUDE_DIRECTORIES(${TENSORFLOW_NSYNC_INCLUDE_PATH}) INCLUDE_DIRECTORIES(${TENSORFLOW_THIRD_PARTY_INCLUDE_PATH}) ADD_EXECUTABLE(load_model load_model.cpp) SET(LOAD_MODEL_LIBRARIES ${TENSORFLOW_PROTOBUF_LIBRARY} ${TENSORFLOW_PROTOC_LIBRARY} ${TENSORFLOW_NSYNC_LIBRARY} ${TENSORFLOW_LIBARY}) SET(LDFLAGS &quot;-std=c++11 -msse4.1 -fPIC -O3 -march=native -Wall -finline-functions -undefined dynamic_lookup -all_load&quot;) SET(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS}${LDFLAGS}&quot;) MESSAGE(STATUS &quot;CMAKE_CXX_COMPILER: ${CMAKE_CXX_COMPILER}&quot;) MESSAGE(STATUS &quot;CMAKE_CXX_FLAGS: ${CMAKE_CXX_FLAGS}&quot;) TARGET_LINK_LIBRARIES(load_model ${LOAD_MODEL_LIBRARIES} ${CMAKE_CXX_FLAGS}) 编译 mkdir -p build &amp;&amp; cd build cmake .. make # 将编译好的二进制文件拷贝到上级目录 cp load_model .. 执行 ./load_model 输出如下 ","link":"https://ce39906.github.io/post/Tensorflow-编译及应用C-静态库/"},{"title":"Tensorflow 编译及应用C++动态库","content":"RT 背景 Tensorflow 是google开源的目前最流行的深度学习框架。 TensorFlow 源码基于c++ 开发，提供了java, c, c++, python等主流编程语言支持。目前业界主流的应用方式是使用Tensorflow python构建以及训练模型，模型训练完成后使用c++ 或者java将模型应用到生产环境。 本文主要介绍如何将Tensorflow编译成动态库以便于其他工程使用头文件以及动态库的方式引入Tensorflow进行模型训练或者应用训练好的模型。 本文的测试环境基于MacOS，Linux环境也可以按照相同步骤安装依赖，编译TensorFlow 动态库。 依赖 java java --vesrion java 9.0.1 bazel bazel 是google 开源的工程构建工具。 brew install bazel eigen eigen 是一个C++矩阵运算库 brew install eigen automake brew install automake libtool brew install libtool protobuf brew install protobuf 编译 首先我们将整个Tensorflow clone到本地 git clone https://github.com/tensorflow/tensorflow cd tensorflow 编译tensorflow 动态链接库 bazel build //tensorflow:libtensorflow_cc.so bazel build //tensorflow:libtensorflow_framework.so 编译的过程耗时约30mins 编译完成后，可以看到tensorflow路径下多了bazel-out, bazel-tensorflow,bazel-bin, bazel-testlogs,bazel-genfiles 这几个路径 整理头文件及库文件 这一步，我们将编译好动态库以及Tensorflow 头文件拷贝到系统路径以方便其他c++工程引用。 头文件 sudo mkdir /usr/local/include/tf/tensorflow # 拷贝tensorflow项目本身生成的头文件 sudo cp -r tensorflow/core /usr/local/include/tf/tensorflow/ sudo cp -r tensorflow/cc /usr/local/include/tf/tensorflow/ # 拷贝bazel 编译的文件 sudo cp -r bazel-genfiles/ /usr/local/include/tf/ # 拷贝tensorflow依赖的第三方文件 sudo cp -r third_party /usr/local/include/tf/ 动态链接库文件 sudo cp -r bazel-bin/tensorflow/libtensorflow_cc.so /usr/local/lib/ sudo cp -r bazel-bin/tensorflow/libtensorflow_framework.so /usr/local/lib/ 示例以及编译 我们使用Tensorflow 官网https://www.tensorflow.org/api_guides/cc/guide提供的示例使用动态链接库编译测试。 示例代码如下 // tensorflow/cc/example/example.cc #include &quot;tensorflow/cc/client/client_session.h&quot; #include &quot;tensorflow/cc/ops/standard_ops.h&quot; #include &quot;tensorflow/core/framework/tensor.h&quot; int main() { using namespace tensorflow; using namespace tensorflow::ops; Scope root = Scope::NewRootScope(); // Matrix A = [3 2; -1 0] auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} }); // Vector b = [3 5] auto b = Const(root, { {3.f, 5.f} }); // v = Ab^T auto v = MatMul(root.WithOpName(&quot;v&quot;), A, b, MatMul::TransposeB(true)); std::vector&lt;Tensor&gt; outputs; ClientSession session(root); // Run and fetch v TF_CHECK_OK(session.Run({v}, &amp;outputs)); // Expect outputs[0] == [19; -3] LOG(INFO) &lt;&lt; outputs[0].matrix&lt;float&gt;(); return 0; } 使用CMake 编译，CMakeLists.txt 如下 CMAKE_MINIMUM_REQUIRED(VERSION 2.8) SET(TENSORFLOW_INCLUDE_PATH /usr/local/include/tf) SET(TENSORFLOW_LIBARY /usr/local/lib/libtensorflow_cc.so) SET(TENSORFLOW_FRAMEWORK_LIBARY /usr/local/lib/libtensorflow_framework.so) MESSAGE(STATUS &quot;TENSORFLOW_INCLUDE_PATH ${TENSORFLOW_INCLUDE_PATH}&quot;) MESSAGE(STATUS &quot;TENSORFLOW_LIBARY ${TENSORFLOW_LIBARY}&quot;) MESSAGE(STATUS &quot;TENSORFLOW_FRAMEWORK_LIBARY ${TENSORFLOW_FRAMEWORK_LIBARY}&quot;) SET(EIGEN_INCLUDE_PATH /usr/local/include/eigen3) SET(ABSL_INCLUDE_PATH /usr/local/include/tf/absl) MESSAGE(STATUS &quot;ABSL_INCLUDE_PATH&quot; ${ABSL_INCLUDE_PATH}) INCLUDE(FindProtobuf) FIND_PACKAGE(Protobuf REQUIRED) INCLUDE_DIRECTORIES(${PROTOBUF_INCLUDE_DIR}) INCLUDE_DIRECTORIES(${TENSORFLOW_INCLUDE_PATH}) INCLUDE_DIRECTORIES(${EIGEN_INCLUDE_PATH}) INCLUDE_DIRECTORIES(${ABSL_INCLUDE_PATH}) SET(EXAMPLE_LIBRARIES ${PROTOBUF_LIBRARY} ${TENSORFLOW_FRAMEWORK_LIBARY} ${TENSORFLOW_LIBARY}) ADD_EXECUTABLE(example example.cpp) SET(LDFLAGS &quot;-std=c++11 -O3 -ggdb -Wall&quot;) SET(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS}${LDFLAGS}&quot;) MESSAGE(STATUS &quot;CMAKE_CXX_COMPILER: ${CMAKE_CXX_COMPILER}&quot;) MESSAGE(STATUS &quot;CMAKE_CXX_FLAGS: ${CMAKE_CXX_FLAGS}&quot;) TARGET_LINK_LIBRARIES(example ${EXAMPLE_LIBRARIES} ${CMAKE_CXX_FLAGS}) 需要注意的是，除了以上提到的头文件之外，CMakeLists.txt中引入了ABSL_INCLUDE_PATH, 引入的原因是在编译过程中报以下错误 /usr/local/include/tf/tensorflow/core/lib/gtl/array_slice.h:19:10: fatal error: 'absl/types/span.h' file not found 回到TensorFlow工程下，查找span.h find . -name span.h # 输出如下 ./tensorflow/contrib/makefile/downloads/absl/absl/types/span.h 我们将absl 头文件拷贝到系统目录下，然后在CMakeLists.txt 文件引入该路径。 cd ./tensorflow/contrib/makefile/downloads/ cp -r absl /usr/local/include/tf/ 编译 mkdir -p build &amp;&amp; cd build cmake .. make 执行 可以看到执行结果同预期一致。 后记 编译及应用Tensorflow 动态库的思路是安装好Tensorflow所需依赖，然后使用google开源bazel进行编译。 MacOS 依赖于强大的包管理工具brew可以很方便的安装各种依赖。到Linux环境下需要使用各发行版提供的包管理工具安装，如果不能安装成功需要自己进行手动编译。 在编译好Tensorflow 动态库后，使用Tensorflow的时候仍然有可能出现有头文件没找到或者依赖库缺少的情况，这时候我们可以首先执行Tensorflow 工程提供的 tensorflow/contrib/makefile/download_dependencies.sh 脚本下载Tensorflow 依赖，然后到 downloads 路径下查找缺少的头文件或者库文件，然后将缺少的头文件或库文件拷贝到系统路径。 Tensorflow 工程下 tensorflow/contrib/makefile/ 提供了分别在Andorid, iOS, linux, docker环境下部署应用Tensorflow 的脚本，运行相应脚本后，我们可以得到编译好的Tensorflow静态库文件以及所需的头文件。使用编译好的静态库以及头文件部署到相应平台可以运行训练好的模型。需要注意的是，这些静态库以及头文件只包括应用训练好的模型的相关功能，不具备训练模型的相关功能(毕竟我们不想也不可能到Andorid, iOS平台下去训练模型。) 。下一篇博客会详细介绍编译Tensorflow静态库以及应用Tensorflow静态库的整体过程。 ","link":"https://ce39906.github.io/post/Tensorflow-编译及应用C-动态库/"},{"title":"读书短评-《未来简史》","content":"继《今日简史》后又一巨著 断断续续花了三个月才读完，再次（上次是人类简史）惊叹于作者涉猎广阔，博学深刻。kindle上看到进度是90%的时候发现整书就结束了，剩下10%是参考文献。给跪了... 书中前半部分一个核心观点是讲从生物角度来看人类与其他高级哺乳动物并无巨大差别，人类所以胜利是依靠出众的合作能力。另一个核心观点是未来人类的目标是追求快乐以及获得永生，这个目标即使可能实现也一定先在领导身上实现，那时死亡这唯一公平的事也不复存在，因此也就很可能引发大的社会危机。 后半部分的核心观点是未来生物科技可能会让人变成超人类（当然也是领导先）。基于大量数据的人工智能算法在很多领域的表现会超过人类（包括艺术）。 不管啥算法的实现都需要写代码的吧，基于此，我写写代码肯定是足够混口饭吃的。 ","link":"https://ce39906.github.io/post/读书短评-《未来简史》/"},{"title":"g++ warn_unused_result","content":"编译代码要追求no warning, no error 介绍 在编程过程中，有的函数我们需要确保函数的返回值必须被使用。但是如果函数使用者直接调用函数且不使用函数的返回值的话，g++ 不会给出warning。这样可能会导致很难寻觅的bug。如调用realloc函数，函数调用者必须使用函数的返回值获得重新分配内存的指针。 利用g++ common function attributes 中提供的warn_unused_result 可以保证函数的返回值在没有被使用的时候提示warning，如果在编译选项添加**-Werror**, 则在编译时就会提示编译错误。关于更多 g++ common function attributes 参见https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html 示例 测试代码如下 /************************************************************************* &gt; File Name: warn_unused_result.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-08-24 14:04:00 ************************************************************************/ __attribute__ ((warn_unused_result)) int f(int x) { return x / 2; } __attribute__ ((warn_unused_result)) double f1() { return 0.; } class EmptyClass { }; __attribute__ ((warn_unused_result)) EmptyClass f2() { return EmptyClass(); } class NotEmptyClass { int a; }; __attribute__ ((warn_unused_result)) NotEmptyClass f3() { return NotEmptyClass(); } int main() { // cause unused_result warning f(3); // no unused_result warning int x = f(3); (void) x; // cause unused_result warning f1(); // cause unused_result warning f2(); // no unused_result warning. WHY?? f3(); return 0; } 编译 g++ -Wall -Wextra warn_unused_result.cpp -o warn_unused_result 编译输出如下 其中比较奇怪的现象是当函数返回一个非空的对象时，不会产生warning，比较诡异，至今没找到合理解释。 ","link":"https://ce39906.github.io/post/g-warn-unused-result/"},{"title":"C++11 智能指针","content":"三大智能指针 智能指针的思想 c++ 要求程序员自己管理内存，为程序员提供了更高自由度，但更高的自由度同时意味着更多责任。为了减少c++程序员在使用裸指针时可能带来的内存泄露，c++11 引入智能指针帮助程序员管理内存。智能指针背后的设计思想是RAII unique_ptr unique_ptr 设计的目的是保证指针变量只指向一个实体，避免出现有其他指针变量指向相同实体，或者此指针变量指向同类型的其他实体。可以理解为指针变量与实体之间是一对一的关系。 为了实现上述设计目标，unique_ptr 的拷贝构造函数以及赋值构造函数都声明为deleted(c++11引入，和将拷贝构造函数以及赋值构造函数声明为private实现的效果一致)。所以unique_ptr 只允许使用裸指针初始化或者使用其他unique_ptr移动构造。比较特殊的是，一个新的unique_ptr变量可以从返回值为unique_ptr的函数构造。 unique_ptr&lt;int&gt; p(new int(2)); // allowed unique_ptr&lt;int&gt; p1(p); // not allowed unique_ptr&lt;int&gt; p2 = p; // not allowed unique_ptr&lt;int&gt; p3(std::move(p)); // allowed unique_ptr&lt;int&gt; p4 = std::move(p) // allowed unique_ptr&lt;int&gt; foo() { unique_ptr&lt;int&gt; p(new int(2)); return p; } unique_ptr&lt;int&gt; p5 = foo(); // allowed 在引入 unique_ptr 之前，存在一个叫做auto_ptr 的智能指针，auto_ptr 与 unique_ptr 不同点是auto_ptr没有禁止拷贝构造和赋值构造，并且在执行拷贝构造和赋值构造后，之前的指针变量指向null，再次解引用时会产生运行时错误，这就是auto_ptr被广为诟病的地方。unique_ptr只有在被显示移动的时候才会将实体的所有权交给其他变量。现auto_ptr已经弃用。 shared_ptr unique_ptr 实现指针变量与实体之前一一对应关系，但是在实际应用中，我们经常存在多个指针变量共同指向一个实体的场景，当所有指针变量的生命周期结束时再释放相应的资源。shared_ptr 就是为了解决此问题而引入。 简单的来讲，shared_ptr 实现的原理是在内部使用一个引用计数，每当有一个新的指针变量绑定到实体上时内部的引用计数加一，如果有指针变量生命周期结束引用计数减一，当引用计数为0时，释放相应资源。 shared_ptr 可以使用 use_count() 方法查看引用计数大小。 在多线程的环境下使用shared_ptr的const成员方法不需要外部的同步机制，使用shared_ptr的非const方法时需要重载shared_ptr的原子方法防止数据竞争。 需要注意的是，shared_ptr的引用计数内部的实现原理是一个原子变量，因此在多线程的环境下有大量的shared_ptr变量的复制以及销毁的话会带来比较大的性能损耗，所以如果新的shared_ptr 变量不需要改变指向的实体的内容时应该按照 const reference的方式使用。这个问题在我实际开发中遇到过，问题比较隐蔽，花费了大量时间才定位到。 weak_ptr weak_ptr 的定义 weak_ptr 是为了配合shared_ptr而引入，weak_ptr 指向shared_ptr 管理的对象但是不会增加shared_ptr内部的引用计数，也就是说，不管是否有weak_ptr 指向对象，只要是shared_ptr的引用计数为0时，对象也会被销毁。weak_ptr 没有重载 * 以及 -&gt; 方法。weak_ptr 提供了use_count 查看观察的shared_ptr的引用计数，使用expired 方法判断shared_ptr 指向的对象是否被销毁，使用lock方法创建一个新的shared_ptr 变量。 shared_ptr 循环引用问题 考虑以下代码 #include &lt;memory&gt; #include &lt;iostream&gt; class B; class A { public: A() { std::cout &lt;&lt; &quot;A construct.\\n&quot;; } ~A() { std::cout &lt;&lt; &quot;A destruct.\\n&quot;; } std::shared_ptr&lt;B&gt; pb; }; class B { public: B() { std::cout &lt;&lt; &quot;B construct.\\n&quot;; } ~B() { std::cout &lt;&lt; &quot;B destruct.\\n&quot;; } std::shared_ptr&lt;A&gt; pa; }; int main() { std::shared_ptr&lt;A&gt; a(new A()); std::shared_ptr&lt;B&gt; b(new B()); a-&gt;pb = b; b-&gt;pa = a; return 0; } 编译后执行结果如下 A construct. B construct. 可以看到，由于互相引用，shared_ptr a 以及 b 所管理的对象都没有释放。我们把上述代码中的类A 以及类B中成员变量都声明为weak_ptr 类型后重新编译运行后的结果如下 A construct. B construct. B destruct. A destruct. 由此可见，weak_ptr 不会增加shared_ptr 的互相引用的问题，可以保证shared_ptr所管理的资源可以正确释放。 weak_ptr 与观察者模式 利用weak_ptr的特性，使用weak_ptr 以及shared_ptr配合可以实现观察者模式 ","link":"https://ce39906.github.io/post/C-11-智能指针/"},{"title":"Json library implemented by boost variant","content":"强大的boost boost variant 介绍 boost variant 是一个不同union的泛型类，它用于存储和操作不同类型但在使用时存在&lt;相同泛型&gt;的对象。variant 在实现不同类型的泛型的同时，提供对其包含的具体类型的安全访问。 基于此性质，boost variant 可以应用于创建json 这种数据结构，我们把json 中的Object, Array, String, Number, True, False, Null 统一当做同一种variant 类型。需要注意的是，json 中的Object 和 Array 类型是递归的variant 类型，在声明时需要使用 boost::recursive_wrapper 修饰。boost::recursivee_wrapper用于创建包含创建的variant类型的表达式。 在访问varint 类型时，可以使用boost::get 以及 boost::apply_visitor 的形式。 更多关于 boost variant 的介绍见: https://www.boost.org/doc/libs/1_62_0/doc/html/variant/reference.html json 数据结构 json 的数据类型实现如下 /************************************************************************* &gt; File Name: json_type.hpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-07-31 16:25:59 ************************************************************************/ #ifndef JSON_TYPE_HPP #define JSON_TYPE_HPP #include &lt;boost/variant.hpp&gt; #include &lt;string&gt; #include &lt;unordered_map&gt; #include &lt;vector&gt; namespace json { struct Object; struct Array; struct String { String() {} String(const char* value) : value{value} {} String(std::string value) : value{std::move(value)} {} std::string value; }; struct Number { Number() {}; Number(const double value) : value{value} {} double value; }; struct True { }; struct False { }; struct Null { }; using Value = boost::variant&lt;String, Number, boost::recursive_wrapper&lt;Object&gt;, boost::recursive_wrapper&lt;Array&gt;, True, False, Null&gt;; struct Object { bool isMember(const std::string&amp; key) const { return values.count(key) != 0; } const Value&amp; at(const std::string&amp; key) const { return values.at(key); } const Value&amp; operator[](const std::string&amp; key) const { return values.at(key); } std::unordered_map&lt;std::string, Value&gt; values; }; struct Array { const Value&amp; at(const size_t idx) const { return values.at(idx); } const Value&amp; operator[](const size_t idx) const { return values.at(idx); } size_t size() const { return values.size(); } const Value&amp; front() const { return values.front(); } const Value&amp; back() const { return values.back(); } std::vector&lt;Value&gt; values; }; } // ns json #endif json 数据访问 本节只介绍使用boost::get 访问varint数据。boost::apply_visitor 的方式在序列化的部分介绍 代码示例如下 namespace access { inline const Object&amp; asObject(const Value&amp; value) { return boost::get&lt;Object&gt;(value); } inline const Array&amp; asArray(const Value&amp; value) { return boost::get&lt;Array&gt;(value); } inline const String&amp; asString(const Value&amp; value) { return boost::get&lt;String&gt;(value); } inline const Number&amp; asNumber(const Value&amp; value) { return boost::get&lt;Number&gt;(value); } inline const True&amp; asTrue(const Value&amp; value) { return boost::get&lt;True&gt;(value); } inline const False&amp; asFalse(const Value&amp; value) { return boost::get&lt;False&gt;(value); } inline const Null&amp; asNull(const Value&amp; value) { return boost::get&lt;Null&gt;(value); } } // ns access json 序列化 json 序列化利用boost::apply_visitor. boost::apply_visitor需要实现一个visitor 函数对象，函数对象针对不同实际类型实现不同的序列化方式，针对Object以及Array 这两种类型需要递归调用visitor。 示例代码如下 /************************************************************************* &gt; File Name: json_serialize.hpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-07-31 17:23:19 ************************************************************************/ #ifndef JSON_SERIALIZE_HPP #define JSON_SERIALIZE_HPP #include &quot;json_type.hpp&quot; #include &quot;json_util.hpp&quot; #include &lt;vector&gt; #include &lt;ostream&gt; namespace json { struct SerializeToOstream : boost::static_visitor&lt;void&gt; { explicit SerializeToOstream (std::ostream&amp; out) : out(out) {} void operator() (const String&amp; string) const { out &lt;&lt; &quot;\\&quot;&quot;; out &lt;&lt; string.value; out &lt;&lt; &quot;\\&quot;&quot;; } void operator() (const Number&amp; number) const { out &lt;&lt; util::cast::to_string_with_percision(number.value); } void operator() (const Object&amp; object) const { out &lt;&lt; &quot;{&quot;; for (auto it = object.values.begin(); it != object.values.end();) { out &lt;&lt; &quot;\\&quot;&quot; &lt;&lt; it-&gt;first &lt;&lt; &quot;\\&quot;:&quot;; boost::apply_visitor(SerializeToOstream(out), it-&gt;second); if (++it != object.values.end()) { out &lt;&lt; &quot;,&quot;; } } out &lt;&lt; &quot;}&quot;; } void operator() (const Array&amp; array) const { out &lt;&lt; &quot;[&quot;; for (auto it = array.values.cbegin(); it != array.values.cend();) { boost::apply_visitor(SerializeToOstream(out), *it); if (++it != array.values.cend()) { out &lt;&lt; &quot;,&quot;; } } out &lt;&lt; &quot;]&quot;; } void operator() (const True&amp;) const { out &lt;&lt; &quot;ture&quot;; } void operator() (const False&amp;) const { out &lt;&lt; &quot;false&quot;; } void operator() (const Null&amp;) const { out &lt;&lt; &quot;null&quot;; } private: std::ostream&amp; out; }; struct SerializeToString : boost::static_visitor&lt;void&gt; { explicit SerializeToString (std::string&amp; out) : out(out) {} void operator() (const String&amp; string) const { out.push_back('\\&quot;'); out.append(string.value); out.push_back('\\&quot;'); } void operator() (const Number&amp; number) const { const std::string number_str = util::cast::to_string_with_percision(number.value); out.append(std::move(number_str)); } void operator() (const Object&amp; object) const { out.push_back('{'); for (auto it = object.values.begin(); it != object.values.end();) { out.push_back('\\&quot;'); out.append(it-&gt;first); out.push_back('\\&quot;'); out.push_back(':'); boost::apply_visitor(SerializeToString(out), it-&gt;second); if (++it != object.values.end()) { out.push_back(','); } } out.push_back('}'); } void operator() (const Array&amp; array) const { out.push_back('['); for (auto it = array.values.cbegin(); it != array.values.cend();) { boost::apply_visitor(SerializeToString(out), *it); if (++it != array.values.cend()) { out.push_back(','); } } out.push_back(']'); } void operator() (const True&amp;) const { out.append(&quot;true&quot;); } void operator() (const False&amp;) const { out.append(&quot;false&quot;); } void operator() (const Null&amp;) const { out.append(&quot;null&quot;); } private: std::string&amp; out; }; void serialize(std::ostream&amp; out, const Object&amp; object) { Value value = object; boost::apply_visitor(SerializeToOstream(out), value); } void serialize(std::string&amp; out, const Object&amp; object) { Value value = object; boost::apply_visitor(SerializeToString(out), value); } } // ns json #endif 构造json 结构 针对String,Number,True,False,Null 这类简单类型可以直接使用构造函数构造。 Array 类型内部使用vector 类型，构造时使用vector 的 push_back, emplace_back 方法增加Array的元素。 Object 类型内部使用unordered_map 类型，构造时可以使用 unordered_map 的内建方法。 示例代码如下： Object obj; obj.values[&quot;string&quot;] = &quot;v1&quot;; obj.values[&quot;bool&quot;] = True(); obj.values[&quot;null&quot;] = Null(); obj.values[&quot;number&quot;] = Number(9); Array arr; arr.values.emplace_back(Number(1.02)); arr.values.emplace_back(Number(2.2)); arr.values.emplace_back(Number(3)); arr.values.emplace_back(True()); arr.values.emplace_back(False()); obj.values[&quot;array&quot;] = std::move(arr); 示例 示例代码测试构建json对象，访问json对象，以及序列化json 对象。 示例代码如下 /************************************************************************* &gt; File Name: test_json.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-07-31 19:26:17 ************************************************************************/ #include &lt;iostream&gt; #include &quot;json.hpp&quot; using namespace json; int main() { Object obj; obj.values[&quot;string&quot;] = &quot;v1&quot;; obj.values[&quot;bool&quot;] = True(); obj.values[&quot;null&quot;] = Null(); obj.values[&quot;number&quot;] = Number(9); Array arr; arr.values.emplace_back(Number(1.02)); arr.values.emplace_back(Number(2.2)); arr.values.emplace_back(Number(3)); arr.values.emplace_back(True()); arr.values.emplace_back(False()); obj.values[&quot;array&quot;] = std::move(arr); // json access std::cout &lt;&lt; &quot;Test json access.\\n&quot;; const auto&amp; arr1 = access::asArray(obj[&quot;array&quot;]); std::cout &lt;&lt; &quot;first number in arr is &quot; &lt;&lt; access::asNumber(arr1.front()).value &lt;&lt; std::endl; // json serialize to ostream std::cout &lt;&lt; &quot;Test serialize to ostream.\\n&quot;; serialize(std::cout, obj); std::cout &lt;&lt; std::endl; // json serialize to string std::cout &lt;&lt; &quot;Test serialize to string.\\n&quot;; std::string str; serialize(str, obj); std::cout &lt;&lt; str &lt;&lt; std::endl; return 0; } 编译 g++ --std=c++11 test_json.cpp -o test_json 执行 执行结果如下 TODO 使用boost spirit 实现json反序列化 完整代码 https://github.com/ce39906/self-practices/tree/master/cppcode/variant_json ","link":"https://ce39906.github.io/post/Json-library-implemented-by-boost-variant/"},{"title":"c++11 右值引用,移动构造函数,emplace_back 解析","content":"c++11 几个容易混淆的概念 右值引用 C++11 引入了右值引用的概念，使用&amp;&amp;表示。 首先简单介绍右值的概念，简单的讲，凡是真正存在内存当中，而不是寄存器中的值是左值，其余都是右值。更通俗的说法取地址操作可以得到的都是左值，其余都是右值。例如 int a = 2; a 中就是一个左值，相对的，2 就是一个右值。关于右值更详细严谨的介绍见https://en.cppreference.com/w/cpp/language/value_category 移动构造函数 在c++11 之前，类包括构造函数，析构函数，拷贝构造函数，赋值构造函数。对于存在指针变量的类来讲，其拷贝构造函数，赋值构造函数必须实现指针变量的深拷贝，这可能会涉及到比较耗时的操作(比如string 类存储了一个超长字符串，在调用其拷贝构造或赋值构造时需要超长字符串的拷贝)。 移动构造函数相对拷贝构造函数和赋值构造函数而言不会进行成员变量的深拷贝而是交换其所有权，这样就避免的拷贝时带来的性能损耗。 移动构造的函数声明如下 class_name ( class_name &amp;&amp; )； emplace_back c++11 容器新增了 emplace_back, emplace等方法向容器中加入新的元素。以 vector 的emplace_back 为例，其函数声明如下 template&lt; class... Args &gt; void emplace_back( Args&amp;&amp;... args ); emplcae_back 接收一个右值引用，调用其移动构造函数，将对象移动到容器中，而之前的push_back 是调用一次对象的拷贝构造函数， 容器中存储的是拷贝后的副本。 std::move std::move 的作用是将左值转为右值引用类型。 示例 示例代码测试了 移动构造函数，emplace_back, push_back, std::move /************************************************************************* &gt; File Name: emplace.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-07-26 15:03:11 ************************************************************************/ #include &lt;vector&gt; #include &lt;cstring&gt; #include &lt;iostream&gt; class A { public: A (const int size) : size(size) { if (size) { data = new char[size]; } std::cout &lt;&lt; &quot;I'm constructor.\\n&quot;; } A (const A&amp; other) { size = other.size; data = new char[size]; memcpy(data, other.data, size * sizeof(char)); std::cout &lt;&lt; &quot;I'm copy constructor.\\n&quot;; } A (A&amp;&amp; other) { size = other.size; data = other.data; other.size = 0; other.data = nullptr; std::cout &lt;&lt; &quot;I'm move constructor.\\n&quot;; } private: int size; char* data = nullptr; }; int main() { std::vector&lt;A&gt; vec; vec.reserve(1024); A tmp(5); std::cout &lt;&lt; &quot;push_back a left value.\\n&quot;; vec.push_back(tmp); std::cout &lt;&lt; &quot;push_back a right value with std::move.\\n&quot;; vec.push_back(std::move(tmp)); std::cout &lt;&lt; &quot;emplace_back a left value.\\n&quot;; vec.emplace_back(tmp); std::cout &lt;&lt; &quot;emplace_back a right value with std::move.\\n&quot;; vec.emplace_back(std::move(tmp)); std::cout &lt;&lt; &quot;emplace_back in place.\\n&quot;; vec.emplace_back(5); std::cout &lt;&lt; &quot;=========================================\\n&quot;; std::cout &lt;&lt; &quot;test with buildin string move and emplace_back\\n&quot;; std::cout &lt;&lt; &quot;=========================================\\n&quot;; std::vector&lt;std::string&gt; str_vec; str_vec.reserve(1024); std::string str = &quot;I'd like to be inserted to a container&quot;; std::cout &lt;&lt; &quot;before emplace_back to vec, str is:\\n&quot;; std::cout &lt;&lt; str &lt;&lt; std::endl; std::cout &lt;&lt; &quot;c_str address is &quot; &lt;&lt; (void*) str.c_str() &lt;&lt; std::endl; str_vec.emplace_back(std::move(str)); std::cout &lt;&lt; &quot;after emplace_back to vec, str is:\\n&quot;; std::cout &lt;&lt; str &lt;&lt; std::endl; std::cout &lt;&lt; &quot;c_str address is &quot; &lt;&lt; (void*) str.c_str() &lt;&lt; std::endl; std::cout &lt;&lt; &quot;c_str address of the string in container is &quot; &lt;&lt; (void*) str_vec.front().c_str() &lt;&lt; std::endl; } 编译代码 g++ --std=c++11 emplace.cpp -o emplace 执行，输出结果如下 从执行结果中，我们可以得出以下结论 push_back 可以接收左值也可以接受右值，接收左值时使用拷贝构造，接收右值时使用移动构造 emplace_back 接收右值时调用类的移动构造 emplace_back 接收左值时，实际上的执行效果是先对传入的参数进行拷贝构造，然后使用拷贝构造后的副本，也就是说，emplace_back在接收一个左值的时候其效果和push_back一致！所以在使用emplace_back 时需要确保传入的参数是一个右值引用，如果不是，请使用std::move()进行转换 emplace_back 接收多个参数时，可以调用匹配的构造函数实现在容器内的原地构造 使用string 类验证了移动构造函数式对类成员所有权的传递，从上图中看到string 在插入前c_str的地址和使用emplace_back 移动到容器后的c_str的地址一致。并且移动后字符串c_str 的地址指向其他位置。 ","link":"https://ce39906.github.io/post/c-11-右值引用-移动构造函数-emplace-back-解析/"},{"title":"Trie C++ 实现与解析","content":"Sugg 技术的原理 Trie tree 介绍 trie 源自 retrieval ,中文称为前缀树或字典树。具体介绍见wiki trie C++ 实现 以下trie实现支持任何语言(Chinese,English,Janpanse...)。主要包括以下三个接口 // 使用一组词初始化trie. void Init(const std::vector&lt;std::string&gt;&amp; dict); // 在trie 中查找word是否存在. bool Lookup(const std::string&amp; word); // 返回在trie中所有以word为前缀的词. std::vector&lt;std::string&gt; Suggest(const std::string&amp; word); 具体代码实现如下 trie.hpp /************************************************************************* &gt; File Name: trie.hpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-07-19 11:12:09 ************************************************************************/ #ifndef TRIE_HPP #define TRIE_HPP #include &lt;vector&gt; #include &lt;string&gt; namespace trie { class Trie { static constexpr size_t kAsciiCount = 256; struct TrieNode { TrieNode(const char val) : val(val), is_end(false), childrens(kAsciiCount, nullptr) { } char val; bool is_end; std::vector&lt;TrieNode*&gt; childrens; }; public: Trie() { root = new TrieNode('0'); } ~Trie() { ReleaseTrie(root); } Trie(const Trie&amp;) = delete; Trie&amp; operator = (const Trie&amp;) = delete; void Init(const std::vector&lt;std::string&gt;&amp; dict); bool Lookup(const std::string&amp; word) const; std::vector&lt;std::string&gt; Suggest(const std::string&amp; word) const; void PrintSuggs(const std::string&amp; word) const; private: void Insert(const std::string&amp; word); bool Search(const TrieNode* parent, const std::string&amp; word, const size_t idx) const; void Dfs(const TrieNode* cur, std::string&amp; word, std::vector&lt;std::string&gt;&amp; suggs) const; void ReleaseTrie(const TrieNode* root); TrieNode* root; }; } // ns trie #endif trie.cpp /************************************************************************* &gt; File Name: trie.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-07-19 14:04:15 ************************************************************************/ #include &quot;trie.hpp&quot; #include &lt;iostream&gt; namespace trie { void Trie::Init(const std::vector&lt;std::string&gt;&amp; dict) { for (const std::string&amp; word : dict) { Insert(word); } } bool Trie::Lookup(const std::string&amp; word) const { return Search(root, word, 0); } std::vector&lt;std::string&gt; Trie::Suggest(const std::string&amp; word) const { std::vector&lt;std::string&gt; suggs; if (word.empty()) { return suggs; } // find prefix TrieNode* cur = root; // unsigned char range : 0 ~ 255 for (const unsigned char c : word) { const std::vector&lt;TrieNode*&gt;&amp; childrens = cur-&gt;childrens; if (!childrens[c]) { return suggs; } cur = childrens[c]; } if (!cur) { return suggs; } std::string prefix(word.begin(), word.end() - 1); Dfs(cur, prefix, suggs); return suggs; } void Trie::Insert(const std::string&amp; word) { if (word.empty()) return; TrieNode* cur = root; // unsigned char range : 0 ~ 255 for (const unsigned char c : word) { if (!cur-&gt;childrens[c]) { cur-&gt;childrens[c] = new TrieNode(c); } cur = cur-&gt;childrens[c]; } cur-&gt;is_end = true; } bool Trie::Search(const TrieNode* parent, const std::string&amp; word, const size_t idx) const { if (word.empty()) { return false; } const std::vector&lt;TrieNode*&gt;&amp; childrens = parent-&gt;childrens; // explicitly cast to unsigned char is needed const unsigned char c = word[idx]; TrieNode* cur = childrens[c]; if (!cur) { return false; } if (idx == word.size() - 1) { return cur-&gt;is_end; } return Search(cur, word, idx + 1); } void Trie::Dfs(const TrieNode* cur, std::string&amp; word, std::vector&lt;std::string&gt;&amp; suggs) const { if (cur-&gt;is_end) { suggs.emplace_back(word + cur-&gt;val); } word.push_back(cur-&gt;val); const std::vector&lt;TrieNode*&gt;&amp; childrens = cur-&gt;childrens; for (const TrieNode* children : childrens) { if (children) { Dfs(children, word, suggs); } } word.pop_back(); } void Trie::ReleaseTrie(const TrieNode* root) { if (!root) return; bool no_children = true; const std::vector&lt;TrieNode*&gt;&amp; childrens = root-&gt;childrens; for (const TrieNode* children : childrens) { if (children) { no_children = false; ReleaseTrie(children); } } if (no_children) { delete root; } } void Trie::PrintSuggs(const std::string&amp; word) const { const auto quoted_string = [] (const std::string str) { return &quot;\\&quot;&quot; + str + &quot;\\&quot;&quot;; }; const std::vector&lt;std::string&gt;&amp; suggs = Suggest(word); if (suggs.empty()) { std::cout &lt;&lt; &quot;No suggs for &quot; &lt;&lt; quoted_string(word) &lt;&lt; std::endl; return; } std::cout &lt;&lt; &quot;Suggs for &quot; &lt;&lt; quoted_string(word) &lt;&lt; &quot; are :\\n&quot;; for (const std::string&amp; sugg : suggs) { std::cout &lt;&lt; quoted_string(sugg) &lt;&lt; &quot; &quot;; } std::cout &lt;&lt; std::endl; } } // ns trie 测试 测试数据 测试数据如下，本例中存储在文件trie_data 中国人民 中午 中国人 中国梦 伟大复兴 2020中国制造 中国制造2020 军工etf 北京 北京天安门 天气 天气预报 北京天气预报 beijing beijing tiananmen 汉语 韩国人 韩国 韩范 美国热 东京热 苍井空 苍老师 机器学习 机器人 机器猫 机器狗 美团网 美团外卖 美团平台 美团酒旅 美团生鲜 美团大象 be搜搜 测试代码 test_trie.cpp /************************************************************************* &gt; File Name: test_trie.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-07-19 19:53:22 ************************************************************************/ #include &quot;trie.hpp&quot; #include &lt;iostream&gt; #include &lt;fstream&gt; #include &lt;cstdlib&gt; #define NDEBUG #include &lt;cassert&gt; using namespace trie; void usage(const char* bin) { std::cout &lt;&lt; bin &lt;&lt; &quot; : Need a filename as a parameter.\\n&quot;; std::exit(EXIT_FAILURE); } void readFile2Vector(const std::string&amp; file, std::vector&lt;std::string&gt;&amp; vec) { std::fstream infile(file, std::ios_base::in); std::string line; while(getline(infile, line, '\\n')) { vec.emplace_back(line); } } int main(int argc, char* argv[]) { if (argc &lt; 2) { usage(argv[0]); } const std::string data_file(argv[1]); std::vector&lt;std::string&gt; dict; readFile2Vector(data_file, dict); Trie trie; trie.Init(dict); // test trie lookup function for (const std::string&amp; word : dict) { (void) word; assert(trie.Lookup(word) == true); } trie.PrintSuggs(&quot;美&quot;); trie.PrintSuggs(&quot;be&quot;); trie.PrintSuggs(&quot;中&quot;); trie.PrintSuggs(&quot;苍&quot;); trie.PrintSuggs(&quot;null&quot;); return 0; } 编译 g++ --std=c++11 -O2 trie.cpp test_trie.cpp -o trie 测试结果 执行 ./trie trie_data github 地址 https://github.com/ce39906/self-practices/tree/master/cppcode/trie ","link":"https://ce39906.github.io/post/Trie-C-实现与解析/"},{"title":"C++ 将git提交信息编译到可执行文件","content":"很实用的技术，用来甩锅 在生产环境中经常需要查看在线上运行的程序对应git的哪次提交。 我们可以在编译时获取git 最后一次提交信息GIT_SHA1 宏，C++ 程序通过访问GIT_SHA1宏可以输出和git仓库的提交信息。 使用Makefile时，在Makefile 添加以下 CPPFLAGS+=-DGIT_SHA1=&quot;$(shell git log --format='[sha1]:%h [author]:%cn [time]:%ci [commit]:%s [branch]:%d' -1)&quot; 使用cmake exec_program( &quot;git&quot; ${CMAKE_CURRENT_SOURCE_DIR} ARGS &quot;log --format='[sha1]:%h [author]:%cn [time]:%ci [commit]:%s [branch]:%d' -1&quot; OUTPUT_VARIABLE VERSION_SHA1 ) add_definitions( -DGIT_SHA1=&quot;${VERSION_SHA1}&quot; ) 如果不想直接使用宏变量，可以使用cmake 提供的configure_file 访问CMakeLists.txt 中定义的各种变量. configure_file的功能是根据 xxx.hpp.in文件中定义的宏创建 xxx.hpp 文件。关于configure_file 的说明见 configure_file ","link":"https://ce39906.github.io/post/C-将git提交信息编译到可执行文件/"},{"title":"rundeck CentOS 部署以及配置","content":"一个运维组件 简介 rundeck 是一个在多机器环境下实现自动化执行以及调度任务的开源工具。rundeck 提供了web 界面，用户可以通过web 界面定制任务，调度，观察节点的执行情况。 安装与配置 安装 rundeck 的官网位置rundeck rundeck 运行依赖于java，因此需要首先安装并配置java yum install java-1.8.0-openjdk java-1.8.0-openjdk-devel -y 在 /etc/profile.d 目录下创建 java.sh， 在java.sh 中写入 #!/bin/bash JAVA_HOME=/usr/bin/java PATH=$JAVA_HOME/bin:$PATH export PATH JAVA_HOME export CLASSPATH=. 为java.sh 添加执行权限 chmod +x /etc/profile.d/java.sh 使java环境变量生效 source /etc/profile.d/java.sh 使用yum 安装rundeck rpm -Uvh http://repo.rundeck.org/latest.rpm yum install rundeck 如果使用yum 安装失败，可以使用rpm 的方式安装。 wget http://download.rundeck.org/rpm/rundeck-2.11.5-1.56.GA.noarch.rpm wget http://download.rundeck.org/rpm/rundeck-config-2.11.5-1.56.GA.noarch.rpm rpm -i rundeck-2.11.5-1.56.GA.noarch.rpm rundeck-config-2.11.5-1.56.GA.noarch.rpm 启动rundeck 服务 /etc/init.d/rundeckd start 检查rundeck 是否启动 ps -ef | grep rundeck netstat -anp | grep 4440 配置 更改默认用户名密码 rundeck 默认用户名密码都是admin，可以更改/etc/rundeck/realm.properties 文件中相应位置。更改后重启rundeck 服务 service rundeckd restart 更改根url设置 如果只在部署rundeck的机器访问rundeck web界面则不需要 vim /etc/rundeck/rundeck-config.properties vim /etc/rundeck/framework.properties 把这两个文件中所有的localhost 更改为本机的ip地址然后保存 添加node节点 在下面示例中展示 示例 在浏览器中输入rundeck 地址，以我部署的服务为例http://10.4.227.26:4440/ 默认输入 admin admin 登录, 登录后新建一个project 输入新建project 的名称以及描述其他按照默认选择，以test 为例，滚动到页面最后点击create 创建 下一步继续按照默认配置保存 新建工程后，我们查看工程下的node 节点 可以看到默认情况工程下只有server node 节点。为了添加其他节点,我们进入到 /var/rundeck/projects/test/etc 路径，编辑resources.xml 文件，默认情况resources.xml 下只有server node， 如下图 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project&gt; &lt;node name=&quot;10.4.227.26&quot; description=&quot;Rundeck server node&quot; tags=&quot;&quot; hostname=&quot;10.4.227.26&quot; osArch=&quot;amd64&quot; osFamily=&quot;unix&quot; osName=&quot;Linux&quot; osVersion=&quot;3.10.0-123.el7.x86_64&quot; username=&quot;rundeck&quot;/&gt; &lt;/project&gt; 以10.4.227.21为例，添加node节点，添加client node 节点后配置如下 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project&gt; &lt;node name=&quot;10.4.227.26&quot; description=&quot;Rundeck server node&quot; tags=&quot;&quot; hostname=&quot;10.4.227.26&quot; osArch=&quot;amd64&quot; osFamily=&quot;unix&quot; osName=&quot;Linux&quot; osVersion=&quot;3.10.0-123.el7.x86_64&quot; username=&quot;rundeck&quot;/&gt; &lt;node name=&quot;10.4.227.21&quot; description=&quot;Rundeck client node&quot; tags=&quot;&quot; hostname=&quot;10.4.227.21&quot; osArch=&quot;amd64&quot; osFamily=&quot;unix&quot; osName=&quot;Linux&quot; osVersion=&quot;3.10.0-123.el7.x86_64&quot; username=&quot;ce39906&quot;/&gt; &lt;/project&gt; 新增的hostname username 需要配置正确。 完成配置后，刷新web 界面，可以看到新增的10.4.227.21 仅仅在rundeck server 添加如下配置不能实现rundeck server 到client 的无密码连接。因此，我们登录到rundeck client机器(10.4.227.21)，将 rundeck server(10.4.227.26) /var/lib/rundeck/.ssh/id_rsa.pub 的内容追加到 client 机器中的 ~/.ssh/authorized_keys 以上就完成rundeck node 节点的添加。下面我们测试通过web 界面配置在指定的node 节点上执行命令 选择所有节点后，我们以执行 ls /tmp 为例 点击右上角的执行，执行结果如下 除了简单的命令，rundeck job 支持实现更加复杂的任务。我们以执行脚本为例进行说明。首先新建一个job 配置job 的名称以及描述 在step 中选择执行脚本任务 点击后编辑脚本并保存 选择执行job 的node, 这里我们选择client node(10.4.227.21) 执行 其他使用默认配置，点击创建，创建完成后执行job 执行结果如下 可以看到在rundeck client 上执行了job 指定的脚本(执行 whoami 以及 ifconfig) 思考 rundeck 在完成初始配置后可以很方便用于在一组机器上执行各种任务，这种场景在生产环境以及日常运维中非常常见，类似的工具还有jenkins 前段时间公司部门要求开发自动部署工具，全套使用脚本+ ssh 的方式，健壮性和稳定性都存在不少问题。类似的自动部署的功能完全可以使用rundeck 配置job 的方式执行而不需要重新造轮子再搞一套鸡肋的东西。 ","link":"https://ce39906.github.io/post/rundeck-CentOS-部署以及配置/"},{"title":"C++ cout打印uint8_t的正确方式","content":"c++中的细节问题 问题现象 编译运行以下代码 /************************************************************************* &gt; File Name: cout_uint8.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-06-29 13:50:53 ************************************************************************/ #include &lt;cstdint&gt; #include &lt;iostream&gt; int main() { std::uint8_t uint8_num = 10; std::cout &lt;&lt; &quot;uint8_t num is &quot; &lt;&lt; uint8_num &lt;&lt; std::endl; return 0; } 编译 g++ cout_uint8.cpp --std=c++11 -o cout_uint8 运行得到的输出如下 cout没有正确打印数字10 产生这种情况的原因是很多c++ 实现中 uint8_t 是 unsigned char 的 typedef。因此cout 实际调用的函数是 ostream&amp; operator&lt;&lt;(ostream&amp;, unsigned char) ，因此实际的执行结果是打印对应的ASCII 码字符，而其字符是不可以打印的。 解决方案 将uint8_t 转化为unsigned 类型 使用一元运算符**+(和-** 运算符对应) 测试代码如下 /************************************************************************* &gt; File Name: cout_uint8.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-06-29 13:50:53 ************************************************************************/ #include &lt;cstdint&gt; #include &lt;iostream&gt; #include &lt;typeinfo&gt; int main() { std::uint8_t uint8_num = 10; std::cout &lt;&lt; &quot;uint8_t num is &quot; &lt;&lt; uint8_num &lt;&lt; std::endl; std::cout &lt;&lt; &quot;after cast to unsigned, uint8_t num is &quot; &lt;&lt; unsigned(uint8_num) &lt;&lt; std::endl; std::cout &lt;&lt; &quot;with a unary + operator, uint8_t num is &quot; &lt;&lt; +uint8_num &lt;&lt; std::endl; std::cout &lt;&lt; &quot;type of '+uint8_num' is &quot; &lt;&lt; typeid(+uint8_num).name() &lt;&lt; std::endl; return 0; } 运行结果如下 可见使用+运算符的原理也是进行类型转换(把uint8_t 转为 int) ","link":"https://ce39906.github.io/post/C-cout打印uint8-t的正确方式/"},{"title":"C++利用宏实现统计运行时间工具","content":"源自OSRM中的经典代码 背景 OSRM backend 代码中有一个timing_util.hpp的头文件，其利用宏以及c++11 chrono 实现了统计代码运行时间的工具。 在工程中统计代码运行时间非常常用，本文介绍OSRM timing_util的实现原理，并用示例来说明。 实现解析 timing_util.hpp #ifndef TIMING_UTIL_HPP #define TIMING_UTIL_HPP #include &lt;chrono&gt; #include &lt;cstdint&gt; namespace osrm { namespace util { // 用TIMER_START 定义一个变量记录开始的时间 #define TIMER_START(_X) auto _X##_start = std::chrono::steady_clock::now(), _X##_stop = _X##_start // 用TIMER_STOP 定义一个变量记录结束的时间 #define TIMER_STOP(_X) _X##_stop = std::chrono::steady_clock::now() // TIMER_NSEC 定义start到stop经历了多少纳秒 #define TIMER_NSEC(_X) \\ std::chrono::duration_cast&lt;std::chrono::nanoseconds&gt;(_X##_stop - _X##_start).count() // TIMER_USEC 定义start到stop历经多少微秒 #define TIMER_USEC(_X) \\ std::chrono::duration_cast&lt;std::chrono::microseconds&gt;(_X##_stop - _X##_start).count() // TIMER_MSEC 定义start到stop经历多少毫秒 #define TIMER_MSEC(_X) \\ (0.000001 * \\ std::chrono::duration_cast&lt;std::chrono::nanoseconds&gt;(_X##_stop - _X##_start).count()) // TIMER_SEC 定义start到stop经历多少秒 #define TIMER_SEC(_X) \\ (0.000001 * \\ std::chrono::duration_cast&lt;std::chrono::microseconds&gt;(_X##_stop - _X##_start).count()) // TIMER_MIN 定义start到stop经历多少分钟 #define TIMER_MIN(_X) \\ std::chrono::duration_cast&lt;std::chrono::minutes&gt;(_X##_stop - _X##_start).count() } } #endif // TIMING_UTIL_HPP timing_util在定义宏变量时使用 ##，##的作用是把宏参数和相邻的字符进行字符串连接，# 的作用是把宏参数当做一个字符串。以下是关于宏中# 以及## 用法的示例。 /************************************************************************* &gt; File Name: test_macro.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-06-05 17:38:01 ************************************************************************/ #include &lt;iostream&gt; #define STR(s) #s #define CONS(a, b) int(a##e##b) int main() { // 输出字符串abc std::cout &lt;&lt; STR(abc) &lt;&lt; std::endl; // 输出2000 (2e3 == 2000) std::cout &lt;&lt; CONS(2, 3) &lt;&lt; std::endl; return 0; } TIMING_UTIL统计运行时间示例 timing_util.cpp /************************************************************************* &gt; File Name: timing_util.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-06-05 17:46:50 ************************************************************************/ #include &quot;timing_util.hpp&quot; #include &lt;iostream&gt; using namespace osrm::util; int main() { TIMER_START(x); for (int i = 0; i &lt; 10000; i++) { for (int j = 0; j &lt; 10000; j++) { int ij = i * j; } } TIMER_STOP(x); std::cout &lt;&lt; &quot;Two Level Loop Cost &quot; &lt;&lt; TIMER_NSEC(x) &lt;&lt; &quot; ns.\\n&quot;; std::cout &lt;&lt; &quot;Two Level Loop Cost &quot; &lt;&lt; TIMER_USEC(x) &lt;&lt; &quot; us.\\n&quot;; std::cout &lt;&lt; &quot;Two Level Loop Cost &quot; &lt;&lt; TIMER_MSEC(x) &lt;&lt; &quot; ms.\\n&quot;; std::cout &lt;&lt; &quot;Two Level Loop Cost &quot; &lt;&lt; TIMER_SEC(x) &lt;&lt; &quot; s.\\n&quot;; std::cout &lt;&lt; &quot;Two Level Loop Cost &quot; &lt;&lt; TIMER_MIN(x) &lt;&lt; &quot; min.\\n&quot;; return 0; } 编译 g++ timing_util.cpp -o timing_util --std=c++11 运行 ./timing_util 运行结果如下 ","link":"https://ce39906.github.io/post/C-利用宏实现统计运行时间工具/"},{"title":"Linux cmake 静态链接boost","content":"静态编译boost 背景 使用动态链接编译的二进制程序在执行时要求开发环境与生产环境严格一致，因此我们更倾向于使用静态链接的方式链接第三方库。本文介绍如何在Linux 环境下使用cmake 静态链接Boost 库。 示例 我们将编译好boost静态库.a 文件和头文件放入third_party 目录，在CMakeLists.txt 中使用find_package 方法查找boost静态库。 我自己在CentOS 6.6 编译的boost 1.63.0 静态库以及头文件 boost static library // 加入boost头文件路径 INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/third_party/boost_1_63_0/include) // 设置boost使用静态链接 set(Boost_USE_STATIC_LIBS ON) // 设置需要的boost 组件 set(BOOST_COMPONENTS date_time chrono filesystem iostreams program_options regex system thread unit_test_framework) // 使用cmake find_package 查找boost库位置 find_package(Boost REQUIRED COMPONENTS ${BOOST_COMPONENTS}) // 编译的bin 文件链接boost 库 TARGET_LINK_LIBRARIES(your_bin_name ${Boost_LIBRARIES}) 需要注意的是，仅在CMakeLists.txt 中这样设置的话cmake find_package 无法找到boost 静态库的位置。在cmake 前加入如下参数 # 需要指定boost静态库的绝对路径 cmake -DBOOST_INCLUDEDIR=$(workspace)/third_party/boost_1_63_0/include \\ -DBOOST_LIBRARYDIR=$(workspace)/third_party/boost_1_63_0/lib .. 编译helloboost 程序静态链接boost库的完整示例如下。 helloboost.cpp /************************************************************************* &gt; File Name: helloboost.cpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-06-05 16:02:08 ************************************************************************/ #include &lt;iostream&gt; #include &lt;string&gt; #include &lt;vector&gt; // test boost split #include &lt;boost/algorithm/string.hpp&gt; // include other boost header file you need. int main() { std::string s(&quot;1;2;3;4&quot;); std::vector&lt;std::string&gt; v; std::cout &lt;&lt; &quot;Before boost split, size of v is &quot; &lt;&lt; v.size() &lt;&lt; std::endl; boost::split(v, s, boost::is_any_of(&quot;;&quot;)); std::cout &lt;&lt; &quot;After boost split, size of v is &quot; &lt;&lt; v.size() &lt;&lt; std::endl; return 0; } CmakeLists.txt cmake_minimum_required(VERSION 2.6) project(helloboost C CXX) SET(CMAKE_CXX_FLAGS &quot;-g -w -O2&quot;) #default binary and lib path SET(EXECUTABLE_OUTPUT_PATH ${CMAKE_SOURCE_DIR}) #begin to set boost static library INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/third_party/boost_1_63_0/include) set(Boost_USE_STATIC_LIBS ON) set(BOOST_COMPONENTS date_time chrono filesystem iostreams program_options regex system thread unit_test_framework) find_package(Boost REQUIRED COMPONENTS ${BOOST_COMPONENTS}) ADD_EXECUTABLE(helloboost helloboost.cpp) TARGET_LINK_LIBRARIES(helloboost ${Boost_LIBRARIES}) install.sh #!/bin/bash workspace=$(pwd) mkdir -p build cd build cmake -DBOOST_INCLUDEDIR=${workspace}/third_party/boost_1_63_0/include \\ -DBOOST_LIBRARYDIR=${workspace}/third_party/boost_1_63_0/lib .. make 编译并执行 ./install.sh ./helloboost 执行结果如下 ","link":"https://ce39906.github.io/post/Linux-cmake-静态链接boost/"},{"title":"CRP算法以及OSRM实现","content":"源自微软硅谷研究院 ","link":"https://ce39906.github.io/post/CRP算法以及OSRM实现/"},{"title":"GCC __buildin_except 解析","content":"追求极致的性能 概念 摘自gcc的官方文档OtherBuildinFunction. —Built-in Function: long __builtin_expect (long exp, long c) You may use __builtin_expect to provide the compiler with branch prediction information. In general, you should prefer to use actual profile feedback for this (-fprofile-arcs), as programmers are notoriously bad at predicting how their programs actually perform. However, there are applications in which this data is hard to collect. The return value is the value of exp, which should be an integral expression. The value of c must be a compile-time constant. The semantics of the built-in are that it is expected that exp == c. For example: if (__builtin_expect (x, 0)) foo (); would indicate that we do not expect to call foo, since we expect x to be zero. Since you are limited to integral expressions for exp, you should use constructions such as if (__builtin_expect (ptr != NULL, 1)) error (); when testing pointer or floating-point values. 我们可以使用 __buildin_except 向编译器提供分支预测信息，从而帮助编译器进行代码优化。 引入原因 CPU 流水线技术可以提高CPU执行效率，但是程序中的跳转指令会打乱CPU流水线。因此，跳转次数少的程序拥有更高的执行效率。 示例 使用__buildin_except 定义LIKELY和UNLIKELY宏，分别代表bool型变量或表达式有很大可能性为真或者很大可能性为假。以下是测试代码。 // 两个感叹号的作用是将所有的非零值转化为1 #define LIKELY(x) __builtin_expect(!!(x), 1) #define UNLIKELY(x) __builtin_expect(!!(x), 0) int likely(int x) { if(LIKELY(x)) { x = 5; } else { x = 6; } return x; } int unlikely(int x) { if(UNLIKELY(x)) { x = 5; } else { x = 6; } return x; } int normal(int x) { if(x) { x = 5; } else { x = 6; } return x; } 编译出.o文件，并使用objdump查看汇编代码 gcc -O2 -fprofile-arcs -c test_builtin_except.cpp objdump -d test_builtin_except.o 结果以及解释如下图 总结 Linux 内核中大量使用LIKELY UNLIKELY宏提升程序运行效率，在C/C++工程可以引入此宏提供分支预测提示编译器进行代码优化。比如在工程中经常会存在处理程序错误的分支，但是出错分支又是不经常进入，这种场景下可以使用**__build_except**进行代码优化。 ","link":"https://ce39906.github.io/post/GCC-buildin-except-解析/"},{"title":"Leveldb LRUCache 解析","content":"RT LRU原理 LRU 是一种经典的缓存淘汰策略，其原理以及实现可以查看我之前的博客LRU Cache 解析及实现。本文主要解析Leveldb LRU Cache。 Leveldb 实现 Leveldb 实现的LRUCache 使用自己实现的简单hashtable存储键值对，循环双链表记录每个元素的访问时间，为了提升多线程环境下的读写性能，Leveldb 内部使用LRUCache 数组对外提供服务。 Cache Entry struct LRUHandle { void* value; void (*deleter)(const Slice&amp;, void* value); // next_hash 代表在hash桶中下一个元素的位置 LRUHandle* next_hash; // 双链表中下个元素的位置 LRUHandle* next; // 双链表中上个元素的位置 LRUHandle* prev; size_t charge; // TODO(opt): Only allow uint32_t? size_t key_length; // 使用次数 uint32_t refs; // key 的hash值用于在LRUCache数组以及hash表中定位 uint32_t hash; // Hash of key(); used for fast sharding and comparisons char key_data[1]; // Beginning of key Slice key() const { // For cheaper lookups, we allow a temporary Handle object // to store a pointer to a key in &quot;value&quot;. if (next == this) { return *(reinterpret_cast&lt;Slice*&gt;(value)); } else { return Slice(key_data, key_length); } } }; 简单hash表 Leveldb LRU Cache 实现了线程安全的拉链式的hashtable class HandleTable { public: HandleTable() : length_(0), elems_(0), list_(NULL) { Resize(); } ~HandleTable() { delete[] list_; } // 查找，直接调用FindPointer方法 LRUHandle* Lookup(const Slice&amp; key, uint32_t hash) { return *FindPointer(key, hash); } // 插入，使用FindPointer方法找到插入位置，在hashtable 对位位置插入，当hashtable 中的元素个数大于桶的个数时候触发hashtable 的扩容 LRUHandle* Insert(LRUHandle* h) { LRUHandle** ptr = FindPointer(h-&gt;key(), h-&gt;hash); LRUHandle* old = *ptr; h-&gt;next_hash = (old == NULL ? NULL : old-&gt;next_hash); *ptr = h; if (old == NULL) { ++elems_; if (elems_ &gt; length_) { // Since each cache entry is fairly large, we aim for a small // average linked list length (&lt;= 1). Resize(); } } return old; } // 删除hashtable中的元素，同样使用FindPointer方法定位到在hashtable 中的位置 LRUHandle* Remove(const Slice&amp; key, uint32_t hash) { LRUHandle** ptr = FindPointer(key, hash); LRUHandle* result = *ptr; if (result != NULL) { *ptr = result-&gt;next_hash; --elems_; } return result; } private: // The table consists of an array of buckets where each bucket is // a linked list of cache entries that hash into the bucket. uint32_t length_; uint32_t elems_; LRUHandle** list_; // Return a pointer to slot that points to a cache entry that // matches key/hash. If there is no such cache entry, return a // pointer to the trailing slot in the corresponding linked list. // 找到hash 元素的地址 LRUHandle** FindPointer(const Slice&amp; key, uint32_t hash) { //找到hash 桶的位置，由于length_是2的指数次幂，所以使用 按位与替换mod操作进行加速 LRUHandle** ptr = &amp;list_[hash &amp; (length_ - 1)]; // 遍历hash桶中的单链表直到找到相应节点或单链表结束 while (*ptr != NULL &amp;&amp; ((*ptr)-&gt;hash != hash || key != (*ptr)-&gt;key())) { ptr = &amp;(*ptr)-&gt;next_hash; } return ptr; } // hash 表扩容 void Resize() { // 每次扩容都是翻倍，保证hash桶的个数是2的指数次幂 uint32_t new_length = 4; while (new_length &lt; elems_) { new_length *= 2; } // 创建一个新的hashtable LRUHandle** new_list = new LRUHandle*[new_length]; memset(new_list, 0, sizeof(new_list[0]) * new_length); uint32_t count = 0; // 逐个桶迁移到新的hashtable for (uint32_t i = 0; i &lt; length_; i++) { LRUHandle* h = list_[i]; while (h != NULL) { LRUHandle* next = h-&gt;next_hash; uint32_t hash = h-&gt;hash; LRUHandle** ptr = &amp;new_list[hash &amp; (new_length - 1)]; h-&gt;next_hash = *ptr; *ptr = h; h = next; count++; } } assert(elems_ == count); // 释放掉老的hashtable delete[] list_; // 替换为新的hashtable list_ = new_list; length_ = new_length; } }; 单个LRUCache实现 LRUCache 类声明 class LRUCache { public: LRUCache(); ~LRUCache(); // Separate from constructor so caller can easily make an array of LRUCache void SetCapacity(size_t capacity) { capacity_ = capacity; } // Like Cache methods, but with an extra &quot;hash&quot; parameter. Cache::Handle* Insert(const Slice&amp; key, uint32_t hash, void* value, size_t charge, void (*deleter)(const Slice&amp; key, void* value)); Cache::Handle* Lookup(const Slice&amp; key, uint32_t hash); void Release(Cache::Handle* handle); void Erase(const Slice&amp; key, uint32_t hash); void Prune(); size_t TotalCharge() const { MutexLock l(&amp;mutex_); return usage_; } private: void LRU_Remove(LRUHandle* e); void LRU_Append(LRUHandle* e); void Unref(LRUHandle* e); // Initialized before use. // LRUCache 存储的元素个数，超过此个数触发淘汰机制 size_t capacity_; // mutex_ protects the following state. mutable port::Mutex mutex_; size_t usage_; // Dummy head of LRU list. // lru.prev is newest entry, lru.next is oldest entry. // 按照访问时间顺序存储的双向循环链表 LRUHandle lru_; // 自己实现的拉链式的hash table HandleTable table_; }; LRUCache 插入 Cache::Handle* LRUCache::Insert( const Slice&amp; key, uint32_t hash, void* value, size_t charge, void (*deleter)(const Slice&amp; key, void* value)) { MutexLock l(&amp;mutex_); // 构造一个新的LRUHandle 对象 LRUHandle* e = reinterpret_cast&lt;LRUHandle*&gt;( malloc(sizeof(LRUHandle)-1 + key.size())); e-&gt;value = value; e-&gt;deleter = deleter; e-&gt;charge = charge; e-&gt;key_length = key.size(); e-&gt;hash = hash; e-&gt;refs = 2; // One from LRUCache, one for the returned handle memcpy(e-&gt;key_data, key.data(), key.size()); // 插入到双向链表表头的位置 LRU_Append(e); usage_ += charge; LRUHandle* old = table_.Insert(e); // 替换掉LRUCache中重复的元素 if (old != NULL) { LRU_Remove(old); Unref(old); } // 触发LRU 淘汰 while (usage_ &gt; capacity_ &amp;&amp; lru_.next != &amp;lru_) { // 淘汰双向链表最后的元素 LRUHandle* old = lru_.next; // 删除双向链表中的元素 LRU_Remove(old); // 删除hashtable中的元素 table_.Remove(old-&gt;key(), old-&gt;hash); Unref(old); } // 返回刚刚插入的Handle对象，此对象引用计数为2 return reinterpret_cast&lt;Cache::Handle*&gt;(e); } LRUCache 删除 void LRUCache::Erase(const Slice&amp; key, uint32_t hash) { MutexLock l(&amp;mutex_); // 删除hashtable中对应的元素， // 如果该元素存在在hashtable中则在双向链表中进行删除 LRUHandle* e = table_.Remove(key, hash); if (e != NULL) { LRU_Remove(e); Unref(e); } } LRUCache Prune 删除LRUCache中ref为1 的元素，最初插入的元素的引用为2，外部消费者对使用结束的元素ref减一。LRUCache中ref 为1 的handle表示此元素只在LRUCache中出现，外部并不使用。此方法的作用就是删除这样的元素。 void LRUCache::Prune() { MutexLock l(&amp;mutex_); for (LRUHandle* e = lru_.next; e != &amp;lru_; ) { LRUHandle* next = e-&gt;next; if (e-&gt;refs == 1) { table_.Remove(e-&gt;key(), e-&gt;hash); LRU_Remove(e); Unref(e); } e = next; } } LRU_Remove 删除双向链表中一个节点 void LRUCache::LRU_Remove(LRUHandle* e) { e-&gt;next-&gt;prev = e-&gt;prev; e-&gt;prev-&gt;next = e-&gt;next; } LRU_Append 插入到双向链表表头位置 void LRUCache::LRU_Append(LRUHandle* e) { // Make &quot;e&quot; newest entry by inserting just before lru_ e-&gt;next = &amp;lru_; e-&gt;prev = lru_.prev; e-&gt;prev-&gt;next = e; e-&gt;next-&gt;prev = e; } ShardedLRUCache static const int kNumShardBits = 4; // 16个 lru cache，目的在多线程的环境下，每个线程的访问都会锁住缓冲区 // 如果缓冲区比较大的话， 多线程情况下的性能比较差， // 所以使用hash 的方式将一块cache 缓冲区 划分为多个小块的缓冲区 static const int kNumShards = 1 &lt;&lt; kNumShardBits; class ShardedLRUCache : public Cache { private: // LRUCache 数组，每个LRUCache对象使用自己的锁 LRUCache shard_[kNumShards]; // 这个锁只保护id port::Mutex id_mutex_; uint64_t last_id_; static inline uint32_t HashSlice(const Slice&amp; s) { return Hash(s.data(), s.size(), 0); } // Shard 函数，根据key计算的hash 值定位LRUCache数组的位置 static uint32_t Shard(uint32_t hash) { // 不需要使用mod运算，此数值一定是在0~kNumShards这个区间 return hash &gt;&gt; (32 - kNumShardBits); } public: explicit ShardedLRUCache(size_t capacity) : last_id_(0) { // 向上取整，为每个LRUCache 分配容量 const size_t per_shard = (capacity + (kNumShards - 1)) / kNumShards; for (int s = 0; s &lt; kNumShards; s++) { shard_[s].SetCapacity(per_shard); } } virtual ~ShardedLRUCache() { } // 插入时，先使用Shard方法定位到相应的LRUCache virtual Handle* Insert(const Slice&amp; key, void* value, size_t charge, void (*deleter)(const Slice&amp; key, void* value)) { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter); } virtual Handle* Lookup(const Slice&amp; key) { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Lookup(key, hash); } virtual void Release(Handle* handle) { LRUHandle* h = reinterpret_cast&lt;LRUHandle*&gt;(handle); shard_[Shard(h-&gt;hash)].Release(handle); } virtual void Erase(const Slice&amp; key) { const uint32_t hash = HashSlice(key); shard_[Shard(hash)].Erase(key, hash); } virtual void* Value(Handle* handle) { return reinterpret_cast&lt;LRUHandle*&gt;(handle)-&gt;value; } virtual uint64_t NewId() { MutexLock l(&amp;id_mutex_); return ++(last_id_); } virtual void Prune() { for (int s = 0; s &lt; kNumShards; s++) { shard_[s].Prune(); } } virtual size_t TotalCharge() const { size_t total = 0; for (int s = 0; s &lt; kNumShards; s++) { total += shard_[s].TotalCharge(); } return total; } }; 总结 Leveldb 实现的LRUCahe 基于hashtable以及双向链表实现，为了提升性能以及保证跨平台特性，Leveldb 实现了一个线程安全的拉链式hashtable,此hashtable 的缺点是当触发扩容操作的时候需要将老hashtable 的全部元素复制到新的hashtable 中，这会期间会一直占用锁，此时多线程环境下的读写会阻塞。这一点Memcached 中实现的hashtable是有一个线程专门负责扩展，每次只扩展一个元素。 Leveldb 为了提升多线程环境下的读写性能，将固定容量的Cache分摊到多个LRUCache，每个LRUCache保证自己的线程安全，这就降低了多线程环境下锁的竞争。这种做法与分段锁的思想类似。 ","link":"https://ce39906.github.io/post/Leveldb-LRUCache-解析/"},{"title":"Linux shell 颜色输出","content":"Linux shell 也可以花里胡哨 原理 可以利用ANSI escape code 实现linux终端输出颜色文本。下面是几个ANSI escape code 对应的颜色。 Black 0;30 Dark Gray 1;30 Red 0;31 Light Red 1;31 Green 0;32 Light Green 1;32 Brown/Orange 0;33 Yellow 1;33 Blue 0;34 Light Blue 1;34 Purple 0;35 Light Purple 1;35 Cyan 0;36 Light Cyan 1;36 Light Gray 0;37 White 1;37 示例 以下是一个输出ANSI Rainbow 的示例。 #!/bin/bash for (( i = 30; i &lt; 38; i++ )) do echo -e &quot;\\033[0;&quot;$i&quot;m Normal: (0;$i); \\033[1;&quot;$i&quot;m Light: (1;$i)&quot;; done 输出结果如下 如果想设置固定区间的颜色，可以将'\\033[0m'放在对应区间的结尾。如下示例 #!/bin/bash NC='\\033[0m' # No Color for (( i = 30; i &lt; 38; i++ )) do echo -e &quot;I \\033[0;${i}mlove${NC} Linux&quot; done 输出结果如下 ","link":"https://ce39906.github.io/post/Linux-shell-颜色输出/"},{"title":"Leveldb Arena解析","content":"RT 简介 Arena 是leveldb 实现的简单的内存池，以最小4096bytes 为单位申请block, 使用指针记录当前block 中空余内存起始位置以及当前block剩余空间。将所有的block 放到blocks_ 数组中。Arena 提供了分配内存以及分配对齐的内存的两种接口，没有释放内存的接口，当Arena 的生命周期结束时，由Arena 的析构函数统一释放内存。Arena 的主要结构如下图所示： Leveldb 代码实现 leveldb arena 实现在util/arena.h 以及 util/arena.cc 接口以及成员 class Arena { public: Arena(); ~Arena(); // Return a pointer to a newly allocated memory block of &quot;bytes&quot; bytes. // 分配指定大小的内存，并返回分配内存的首地址 char* Allocate(size_t bytes); // 分配指定大小并且对齐的内存 // Allocate memory with the normal alignment guarantees provided by malloc char* AllocateAligned(size_t bytes); // Returns an estimate of the total memory usage of data allocated // by the arena. // 统计使用了多少内存 size_t MemoryUsage() const { return reinterpret_cast&lt;uintptr_t&gt;(memory_usage_.NoBarrier_Load()); } private: // 分配内存，根据情况决定是否使用新的block char* AllocateFallback(size_t bytes); // 分配一个新的block char* AllocateNewBlock(size_t block_bytes); // Allocation state // 指向一个block中未被使用的内存首地址 char* alloc_ptr_; // 表示一个block还剩余多少空间 size_t alloc_bytes_remaining_; // Array of new[] allocated memory blocks // 内存池数组 std::vector&lt;char*&gt; blocks_; // Total memory usage of the arena. // 统计内存使用，原子变量 port::AtomicPointer memory_usage_; // No copying allowed // 将拷贝构造和赋值构造设置为私有 Arena(const Arena&amp;); void operator=(const Arena&amp;); }; 分配指定bytes 的内存 首先根据alloc_bytes_remaining_判断是否需要分配一个新的block ,如果不需要的话，则重新设置alloc_ptr_ 以及 alloc_bytes_remaining_，否则调用AllocFallback方法分配新的block,并且判断新的block 是否可以被复用。 inline char* Arena::Allocate(size_t bytes) { // The semantics of what to return are a bit messy if we allow // 0-byte allocations, so we disallow them here (we don't need // them for our internal use). assert(bytes &gt; 0); // 判断是否有可被复用的block if (bytes &lt;= alloc_bytes_remaining_) { char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; } return AllocateFallback(bytes); } 分配指定bytes 对齐的内存 与上一个接口不同的地方在于，将传入的bytes 化为可对齐内存的大小 char* Arena::AllocateAligned(size_t bytes) { // 获取当前系统指针大小 const int align = (sizeof(void*) &gt; 8) ? sizeof(void*) : 8; // 指针大小必须是2的整数次幂，2的整数次幂的二进制表示中 // 有且只有1位是1 assert((align &amp; (align-1)) == 0); // Pointer size should be a power of 2 // 判断bytes是不是align 的整数倍，由于align是2的 // 整数次幂，所以对align的取模运算可以转化为 // 对（align - 1）进行按位与 size_t current_mod = reinterpret_cast&lt;uintptr_t&gt;(alloc_ptr_) &amp; (align-1); // 为了对齐内存需要新增的大小 size_t slop = (current_mod == 0 ? 0 : align - current_mod); // needed 表示分配对齐的内存所需的大小 // 后面的逻辑同前 size_t needed = bytes + slop; char* result; if (needed &lt;= alloc_bytes_remaining_) { result = alloc_ptr_ + slop; alloc_ptr_ += needed; alloc_bytes_remaining_ -= needed; } else { // AllocateFallback always returned aligned memory result = AllocateFallback(bytes); } assert((reinterpret_cast&lt;uintptr_t&gt;(result) &amp; (align-1)) == 0); return result; } AllocateFallback 函数的作用是分配一个新的block。不同的是，如果bytes 小于blocksize 的四分之一，则此新分配的block 可以被继续复用。否则的话直接分配新的block, Arena 中可被复用的block 保持不变。 char* Arena::AllocateFallback(size_t bytes) { // 分配的bytes较小，此新block可以被复用 if (bytes &gt; kBlockSize / 4) { // Object is more than a quarter of our block size. Allocate it separately // to avoid wasting too much space in leftover bytes. char* result = AllocateNewBlock(bytes); return result; } // We waste the remaining space in the current block. // 此新分配的block可以被复用，重新设置 // alloc_ptr_ 以及 alloc_bytes_remaining_ alloc_ptr_ = AllocateNewBlock(kBlockSize); alloc_bytes_remaining_ = kBlockSize; char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; } AllocateNewBlock 直接使用new分配符分配指定大小的内存，并且更新memory_usage_ char* Arena::AllocateNewBlock(size_t block_bytes) { // 分配空间 char* result = new char[block_bytes]; // 新的block 加入内存池 blocks_.push_back(result); // 更新memory_usage_ memory_usage_.NoBarrier_Store( reinterpret_cast&lt;void*&gt;(MemoryUsage() + block_bytes + sizeof(char*))); return result; } 析构 逐个释放内存池中的block Arena::~Arena() { for (size_t i = 0; i &lt; blocks_.size(); i++) { delete[] blocks_[i]; } } 总结 Leveldb Arena 实现简单，Arena 的设计与leveldb 特定的应用场景相关，比如一个memtable 使用一个Arena对象统一获取内存，当memtable对象声明周期结束时，由Arena 统一释放内存，不需要消费者每次new一片内存就要自己delete 掉。 ","link":"https://ce39906.github.io/post/Leveldb-Arena解析/"},{"title":"Leveldb BloomFilter 解析","content":"一个设计的非常巧妙的数据结构 BloomFilter 原理 布隆过滤器由巴顿布隆于1970年提出，由一个很长的bit数组以及一系列hash函数组成。bloomfilter可以用于检索一个元素是否出现在一个集合中，bloomfilter的优点是相比hash表拥有极大的空间效率，缺点是会出现一定的错误概率(False positive,一个不在集合中的元素被误认为处于集合中）。 bloomfilter 的原理是，当一个元素在加入集合时，通过k个hash 函数映射到bit数组的k个位置，并将相应的位置置1。在查找一个元素是否存在于集合中时，使用相同的k个hash 函数查看bit数组中的位置是否为全为1，如果出现一个0，则该key 一定不存在集合中，否则有可能出现在集合中。 优点 Bloomfilter 可以使用较少的空间存储大量数据的全集，并且存储的不是每个元素的原本的数值，只是设置对应的bit位，一定程度上实现了加密的效果。除空间效率之外，bloomfilter的时间效率都是常数级别O(k),其中k 代表使用的hash 函数个数，由于k个hash 函数式相互独立的性质，在进行k个hash 计算时可以并行计算进一步加速插入查找。 缺点 Bloomfilter 的缺点是会出现False positive 的误判率，而且随着元素的增加，误算率也随之增加。因此尽可能降低误判率需要一些额外工作。 BloomFilter参数选择 以下均参考wikipedia bloomfilter 中关于误差率的计算章节。直接说结论，当k(hash 函数个数)，m(bit数组大小)，n(插入元素个数)满足下式的时候，可以保证最低的误差率。$$k=\\cfrac{m}{n}ln2$$下图(摘自wikipedia) 表示在最优hash函数个数的情况下，不同m,n之间误差率关系。 从上图可以看到当存储10亿个元素时使用4GB的存储空间可以保证不到1e-06的错误率。可以看到bloomfilter在实现高空间利用率的同时可以保证较低误差率。 Leveldb 实现 leveldb bloomfilter 实现在util/bloom.cc 成员变量以及构造函数 class BloomFilterPolicy : public FilterPolicy { private: // 平均每个key拥有的bit 数目 size_t bits_per_key_; // hash function 的个数 size_t k_; public: explicit BloomFilterPolicy(int bits_per_key) : bits_per_key_(bits_per_key) { // We intentionally round down to reduce probing cost a little bit // 按照上面的公式设定hash函数的个数 k_ = static_cast&lt;size_t&gt;(bits_per_key * 0.69); // 0.69 =~ ln(2) if (k_ &lt; 1) k_ = 1; if (k_ &gt; 30) k_ = 30; } }; CreateFilter 将传入的n个key 存储到bloomfilter 中，bloomfilter结果使用string存储。 virtual void CreateFilter(const Slice* keys, int n, std::string* dst) const { // Compute bloom filter size (in both bits and bytes) // bloomfilter 需要多少bit size_t bits = n * bits_per_key_; // For small n, we can see a very high false positive rate. Fix it // by enforcing a minimum bloom filter length. if (bits &lt; 64) bits = 64; // 对齐,方便内存读写以及后续位置索引 size_t bytes = (bits + 7) / 8; bits = bytes * 8; // 在string 中分配空间 const size_t init_size = dst-&gt;size(); dst-&gt;resize(init_size + bytes, 0); // string 的最后一个byte存储使用的hash 函数的个数 dst-&gt;push_back(static_cast&lt;char&gt;(k_)); // Remember # of probes in filter // 获得string内部的char 型数组 char* array = &amp;(*dst)[init_size]; // 逐个将每个key 写入bloom fliter for (int i = 0; i &lt; n; i++) { // Use double-hashing to generate a sequence of hash values. // See analysis in [Kirsch,Mitzenmacher 2006]. // leveldb 使用一个hash 函数，每次对hash值向右循环移位17个bit来模拟实现多个hash 函数的效果 uint32_t h = BloomHash(keys[i]); // 每次向右循环移位17个bit const uint32_t delta = (h &gt;&gt; 17) | (h &lt;&lt; 15); // Rotate right 17 bits for (size_t j = 0; j &lt; k_; j++) { // 在整个bit 数组的位置 const uint32_t bitpos = h % bits; // 在char型数组的位置 array[bitpos/8] |= (1 &lt;&lt; (bitpos % 8)); // 更新获得一个新的hash 数值 h += delta; } } } KeyMayMatch 判断一个 key 在bloomfilter中是否存在。 virtual bool KeyMayMatch(const Slice&amp; key, const Slice&amp; bloom_filter) const { const size_t len = bloom_filter.size(); if (len &lt; 2) return false; const char* array = bloom_filter.data(); // 最后一个byte数值代表使用了多少hash函数 // 除最后一个byte之外代表bit数组 const size_t bits = (len - 1) * 8; // Use the encoded k so that we can read filters generated by // bloom filters created using different parameters. const size_t k = array[len-1]; if (k &gt; 30) { // Reserved for potentially new encodings for short bloom filters. // Consider it a match. return true; } // 使用相同的方法模拟多个hash函数计算的hash值 uint32_t h = BloomHash(key); const uint32_t delta = (h &gt;&gt; 17) | (h &lt;&lt; 15); // Rotate right 17 bits for (size_t j = 0; j &lt; k; j++) { const uint32_t bitpos = h % bits; // 找到一个bit位置不匹配，提前返回false // 在bit数组中位置的索引和设置时的方法一致 if ((array[bitpos/8] &amp; (1 &lt;&lt; (bitpos % 8))) == 0) return false; // 更新获得下一个hash value h += delta; } // 全部匹配return true return true; } BloomFilter 应用场景 由于其高效的空间效率，bloomfilter 可以应用于以下场景： 爬虫系统记录以经爬取过的url 垃圾邮件过滤 p2p 网络中查找资源操作: 使用一个bloomfilter 保存拥有此资源的网络通路 广播信息时检查某个ip是否发包 字典纠错：将所有单词存储到bloomfilter中，如果不存在则认为是一个错误拼写 CDN 代理缓存: 每个cache 服务器上使用bloomfilter 存储兄弟cache 服务器上是否有缓存关键字，如果没有则可以避免一次查找 总结 Bloomfilter 是一种设计巧妙的数据结构，由于其良好的空间效率，可以用于判断一个元素是否包含于海量元素集合的场景。 Leveldb 的实现的bloomfilter 可以灵活配置hash 函数的个数，使用一个hash 函数模拟任意多个hash 函数的场景，并将使用hash函数的个数存储到bloomfilter编码结果中。除此之外，leveldb bloomfilter 实现bit数组个数，hash函数个数以及存储元素个数的最优配置，保证最低的误差率。 ","link":"https://ce39906.github.io/post/Leveldb-BloomFilter-解析/"},{"title":"Leveldb skiplist 实现以及解析","content":"RT skiplist 原理介绍 skiplist 由William Pugh 在论文Skip Lists: A Probabilistic Alternative to Balanced Trees 中提出的一种数据结构，skiplist 是一种随机化存储的多层线性链表结构，插入，查找，删除的都是对数级别的时间复杂度。skiplist 和平衡树有相同的时间复杂度，但相比平衡树，skip实现起来更简单。 下图是wikipedia 上一个一个高度为4的skiplist 从垂直角度看，skiplist 的第0层以单链表的形式按照从小到大的顺序存储全部数据，越高层的链表的节点数越少，这样的特点实现了skiplist 在定位某个位置时，通过在高层较少的节点中查找就可以确定需要定位的位置处于哪个区间，从高层到低层不断缩小查找区间。以上图为例，比如我们需要在skiplist中查找2，查找过程如下，首先在最高层确定到2只可能处于1-&gt;NULL 这个区间，然后在第三层查找确定 2 只可能处于 1-&gt;4 这个区间，继续在第二层查找确定2 只可能处于1-3 这区间，最后在最底层1-&gt;3 这个区间查找可以确定2 是否存在于skiplist之中。 下图是wikipedia上提供的表示skiplist插入过程的一张gif,此图形象的说明了skiplist 定位以及插入节点的过程。 从水平角度来看，skiplist实现在链表开始的时候设置名为head 的哨兵节点，每一层链表的结束为止全部指向NULL。 leveldb 实现 leveldb 实现的skiplist位于db/skiplist.h。 skiplist Node 类型定义 // Implementation details follow template&lt;typename Key, class Comparator&gt; struct SkipList&lt;Key,Comparator&gt;::Node { explicit Node(const Key&amp; k) : key(k) { } // Node 存储的内容 Key const key; // Accessors/mutators for links. Wrapped in methods so we can // add the appropriate barriers as necessary. // 获取当前节点在指定level的下一个节点 Node* Next(int n) { assert(n &gt;= 0); // Use an 'acquire load' so that we observe a fully initialized // version of the returned Node. return reinterpret_cast&lt;Node*&gt;(next_[n].Acquire_Load()); } // 将当前节点在指定level的下一个节点设置为x void SetNext(int n, Node* x) { assert(n &gt;= 0); // Use a 'release store' so that anybody who reads through this // pointer observes a fully initialized version of the inserted node. next_[n].Release_Store(x); } // 无内存屏障版本set。关于leveldb 内存屏障在新一篇博客介绍 // No-barrier variants that can be safely used in a few locations. Node* NoBarrier_Next(int n) { assert(n &gt;= 0); return reinterpret_cast&lt;Node*&gt;(next_[n].NoBarrier_Load()); } void NoBarrier_SetNext(int n, Node* x) { assert(n &gt;= 0); next_[n].NoBarrier_Store(x); } private: // Array of length equal to the node height. next_[0] is lowest level link. // 当前节点的下一个节点数组 port::AtomicPointer next_[1]; }; skiplist 类成员变量 private: // 使用枚举类型定义skiplist 最高高度 enum { kMaxHeight = 12 }; // Immutable after construction // 用户定制的比较器 Comparator const compare_; // leveldb 实现的简单的内存分配器 Arena* const arena_; // Arena used for allocations of nodes // skiplist 的前置哨兵节点 Node* const head_; // Modified only by Insert(). Read racily by readers, but stale // values are ok. // 记录当前skiplist使用的最高高度 port::AtomicPointer max_height_; // Height of the entire list skiplist 插入 template&lt;typename Key, class Comparator&gt; void SkipList&lt;Key,Comparator&gt;::Insert(const Key&amp; key) { // TODO(opt): We can use a barrier-free variant of FindGreaterOrEqual() // here since Insert() is externally synchronized. // 声明prev节点，代表插入位置的前一个节点 Node* prev[kMaxHeight]; // 使用FindGreaterOrEqual函数找到第一个大于等于插入key的位置 Node* x = FindGreaterOrEqual(key, prev); // Our data structure does not allow duplicate insertion assert(x == NULL || !Equal(key, x-&gt;key)); // 使用随机数获取该节点的插入高度 int height = RandomHeight(); if (height &gt; GetMaxHeight()) { // 大于当前skiplist 最高高度的话，将多出的来的高度的prev 设置为哨兵节点 for (int i = GetMaxHeight(); i &lt; height; i++) { prev[i] = head_; } //fprintf(stderr, &quot;Change height from %d to %d\\n&quot;, max_height_, height); // It is ok to mutate max_height_ without any synchronization // with concurrent readers. A concurrent reader that observes // the new value of max_height_ will see either the old value of // new level pointers from head_ (NULL), or a new value set in // the loop below. In the former case the reader will // immediately drop to the next level since NULL sorts after all // keys. In the latter case the reader will use the new node. // 跟新max_height_ max_height_.NoBarrier_Store(reinterpret_cast&lt;void*&gt;(height)); } // 创建要插入的节点对象 x = NewNode(key, height); for (int i = 0; i &lt; height; i++) { // NoBarrier_SetNext() suffices since we will add a barrier when // we publish a pointer to &quot;x&quot; in prev[i]. // 首先将x的next 指向prev 的下一个节点 x-&gt;NoBarrier_SetNext(i, prev[i]-&gt;NoBarrier_Next(i)); // 将prev 指向x prev[i]-&gt;SetNext(i, x); } } skiplist 查找 template&lt;typename Key, class Comparator&gt; bool SkipList&lt;Key,Comparator&gt;::Contains(const Key&amp; key) const { // 找到大于等于当前key的第一个node,然后判断node 的key // 和传入的key 是否相等 Node* x = FindGreaterOrEqual(key, NULL); if (x != NULL &amp;&amp; Equal(key, x-&gt;key)) { return true; } else { return false; } } FindGreaterOrEqual 函数的作用是找到第一个大于或等于指定的key 的node,以及该node的前一个node template&lt;typename Key, class Comparator&gt; typename SkipList&lt;Key,Comparator&gt;::Node* SkipList&lt;Key,Comparator&gt;::FindGreaterOrEqual(const Key&amp; key, Node** prev) const { Node* x = head_; // level 从0 开始编码 int level = GetMaxHeight() - 1; while (true) { // 定位到当前level的下一个节点 Node* next = x-&gt;Next(level); // key 没有在当前区间 if (KeyIsAfterNode(key, next)) { // Keep searching in this list x = next; } else { // key 在当前区间，在低level 继续查找， // 在查找的同时设置prev 节点 if (prev != NULL) prev[level] = x; // 在最低level找到相应位置 if (level == 0) { return next; } else { // Switch to next list level--; } } } } RandomHeight 利用随机数实现每次有4分之一的概率增长高度。 template&lt;typename Key, class Comparator&gt; int SkipList&lt;Key,Comparator&gt;::RandomHeight() { // Increase height with probability 1 in kBranching static const unsigned int kBranching = 4; int height = 1; while (height &lt; kMaxHeight &amp;&amp; ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height &gt; 0); assert(height &lt;= kMaxHeight); return height; } FindLessThan template&lt;typename Key, class Comparator&gt; typename SkipList&lt;Key,Comparator&gt;::Node* SkipList&lt;Key,Comparator&gt;::FindLessThan(const Key&amp; key) const { Node* x = head_; int level = GetMaxHeight() - 1; while (true) { assert(x == head_ || compare_(x-&gt;key, key) &lt; 0); \\// 在当前level 查找 Node* next = x-&gt;Next(level); // if 分支为true 的时候表示需要查找的位置在当前区间 if (next == NULL || compare_(next-&gt;key, key) &gt;= 0) { // 在最后一层停止查找 if (level == 0) { return x; } else { // Switch to next list level--; } } else { // 在当前level 就找到了比key 小的节点 x = next; } } } 总结 skiplist最底层单链表有序存储全部元素，利用多层有序链表的结构实现加速索引的功能，处于越高level 节点的链表越稀疏查找速度越快，在不断向下查找的过程中不断缩小查找空间。 总的来说，skiplist 是一种设计巧妙的数据结构，相比红黑树实现简单，插入查找删除的时间复杂度和红黑树一致，但是顺序遍历的时间复杂度优于红黑树。leveldb 的实现可读性高，容易理解。 ","link":"https://ce39906.github.io/post/Leveldb-skiplist-实现以及解析/"},{"title":"C++ 拼接长字符串","content":"如何拼接字符串效率最高 C++ 拼接长字符串 c++ string 类型提供 opearator+= 以及 append 方法进行字符串拼接，本文探讨c++拼接长字符串执行效率最高的方法。以下是四种实现方式。 实现方式 operator += 使用 string 类提供重载 += 方法拼接字符串。示例： // length 参数代表拼接的字符串长度 void composeLongstringWithOperator(const unsigned int length,std::string&amp; long_string) { for (size_t i = 0; i &lt; length / 9; i++) { char str[10]; // randStr 方法构造长度为9的随机字符串 long_string += (randStr(str,9)); } } append 使用 string 类提供的append 方法拼接字符串。示例： void composeLongstringWithAppend(const unsigned int length,std::string&amp; long_string) { for (size_t i = 0; i &lt; length / 9; i++) { char str[10]; long_string.append(randStr(str,9)); } } reserve &amp;&amp; operator += 在拼接字符串之前为string 对象提前分配空间，然后使用 += 方法进行拼接，示例： void composeLongstringWithReserveAndOperator(const unsigned int length,std::string&amp; long_string) { long_string.reserve(length); for (size_t i = 0; i &lt; length / 9; i++) { char str[10]; long_string += (randStr(str,9)); } } reserve &amp;&amp; append 在拼接字符串之前为string 对象提前分配空间，然后使用 append 方法进行拼接，示例： void composeLongstringWithAppend(const unsigned int length,std::string&amp; long_string) { for (size_t i = 0; i &lt; length / 9; i++) { char str[10]; long_string.append(randStr(str,9)); } } 性能测试 测试方法 进行10000次长字符串拼接，统计每种方式下耗时，示例代码如下： #include &lt;iostream&gt; #include &lt;string&gt; #include &lt;ctime&gt; #include &lt;chrono&gt; char* randStr(char* str,const int len) { int i; for(i = 0; i &lt; len; ++i) { str[i] = 'A' + rand() % 26; } str[++i] = '\\0'; return str; } int main(int argc, char* argv[]) { (void) argc; // 第一个参数代表生成的字符串的长度 const unsigned int length = atoi(argv[1]); // 第二个参数代表使用哪种方法进行拼接 const unsigned int type = atoi(argv[2]); srand(time(NULL)); auto start = std::chrono::high_resolution_clock::now(); switch(type) { case 1: std::cout &lt;&lt; &quot;composeLongstringWithReserveAndAppend&quot;; for(int i = 0; i &lt; 10000; i++) { std::string long_string; composeLongstringWithReserveAndAppend(length,long_string); } break; case 2: std::cout &lt;&lt; &quot;composeLongstringWithReserveAndOperator&quot;; for(int i = 0; i &lt; 10000; i++) { std::string long_string; composeLongstringWithReserveAndOperator(length,long_string); } break; case 3: std::cout &lt;&lt; &quot;composeLongstringWithAppend&quot;; for(int i = 0; i &lt; 10000; i++) { std::string long_string; composeLongstringWithAppend(length,long_string); } break; case 4: std::cout &lt;&lt; &quot;composeLongstringWithOperator&quot;; for(int i = 0; i &lt; 10000; i++) { std::string long_string; composeLongstringWithOperator(length,long_string); } break; default: return 0; } auto end = std::chrono::high_resolution_clock::now(); std::chrono::duration&lt;double&gt; diff = end - start; std::cout &lt;&lt; &quot; cost &quot; &lt;&lt; 1000 * diff.count() &lt;&lt; &quot; ms\\n&quot;; return 0; } 编译 g++ -std=c++11 -O3 compose_long_string.cpp -o compose_long_string 性能表现 长字符串长度为1000000，每种方法进行10000次拼接，四种方法的耗时如下： method cost (ms) reserve &amp;&amp; append 117304 reserve &amp;&amp; operator 122998 append 125682 operator 129071 结论 针对较短字符串，使用reserve提前分配空间对性能提升意义不大，当字符串的长度很长是，使用reserve方法提前分配空间可以带来比较大的性能提升。 operator+= 和 append 方法在进行字符串拼接时性能表现几乎一致。原因是stl 实现的operator+= 方式实际是直接调用了append 方法。 综上，拼接长字符串时最优方式是 reserve &amp;&amp; append。 ","link":"https://ce39906.github.io/post/C-拼接长字符串/"},{"title":"读书短评","content":"之前读的一些书 2018-04-19 《人类简史》 现代社会建立在人类的重重假设，回到非洲大草原摘个野果子吃就很快乐~ 从物种繁衍的角度来看，小麦，家畜是比人要更成功的。但是物种繁衍的成功不等同于个体的快乐。有人可能会说小麦，家畜的一生都很短暂，但相对浩瀚宇宙来说，人的一生也是沧海一粟。都是匆匆过客，如果能更多卸下捆绑获得真正的快乐才是生活的真谛。 《活着》 所有的困难都会过去，活着本身就是最大的意义。 《围城》 很多现象都可以用围城思想来解释，里面的人想出去，外边的人想进去 《明朝那些事》 除了朱元璋打天下，朱棣篡位，剩下大部分篇幅都在讲官场政治，当年明月本是公务员，写官场政治这些也非常熟悉，所以后面那几本叫《明朝官场那些事》可能更合适。 《解忧杂货店》 高产如东野，可能写悬疑探案类更费精力，所以来碗鸡汤喂喂。书中所有故事都说明了影射一个道理，向别人寻求建议的时候，自己心理已经有答案，只不过是希望别人加强一下自己的信念，如果建议和自己想的相反，你会找别的人再给你建议~ 《白鹿原》 具有西北特色的低配百年孤独。读的过程中可以感受到西北作家的朴实的文风，稍微结合一点贵party的black历史就可以如此精彩！另外，读的过程中经常想吃羊肉泡馍~ 《苏东坡传》 上学课本上说的伟大的文学家，书画家。而借用林语堂在前言中对苏东坡的描述: “苏东坡是一个无可救药的乐天派、一个伟大的人道主义者、一个百姓的朋友、一个大文豪、大书法家、创新的画家、造酒试验家、一个工程师、一个憎恨清教徒主义的人、一位瑜伽修行者佛教徒、巨儒政治家、一个皇帝的秘书、酒仙、厚道的法官、一位在政治上专唱反调的人。一个月夜徘徊者、一个诗人、一个小丑”,相比之下自己的生活单调了好多，所以应该在自己身上点更多天赋。不写了，中午吃东坡肉。 2018-04-20 《天龙八部》 乔峰，段誉，虚竹，慕容复所有主要角色无不苦难。众生皆苦，无欲则刚，修炼成佛的还是没几个。 《百年孤独》 人生来孤独，所以需要把雕刻好小金鱼后融掉重新再雕，需要吃土排解孤独。随便拉出个人来，对他说“我想你才是真正的孤独吧！”他都会感到你是他的知音~ 《追风筝的人》 “为你，千千万万遍” 对主人的忠诚和对友谊的坚持要有多深才能说出这样的话。 《鹿鼎记》 “老子不干了” 在康熙皇帝和天地会之间脚踏两条船太累了！在这两个没法做决定的选择面前，干脆不选了，全都qtmd！ 《黄金时代》 在混乱的年代，黄金时代年纪，有个人一起实践伟大革命友谊也很幸福吧！ 2018-04-25 《万寿寺》 万寿寺的主要作用之一是为了老佛爷乘船去颐和园游玩的途中休息一下。之所以老佛爷成为了老佛爷是因为咸丰帝从她身上爬起来时那条射过精的疲软的jb,王小波称之为历史的脐带(hahaha)。所以他在万寿寺搞一些历史的脐带考，领导很不喜欢！ 《了不起的盖茨比》 对黛西的爱情是盖茨比先生的一道绿光，绿光指引他前行，我猜在他临死之前，绿光也不曾熄灭。 《指数基金投资指南》 钉大的扫盲科普书籍，1 h 看完，后面继续有选择的抄E大和钉大的作业。 2018-05-02 《笑傲江湖》 金庸武侠里面政治色彩很重的一本，前面甚嚣尘土的左盟主还是被岳君子阴了。伪君子比真小人更可怕。 《神雕侠侣》 侠之大者为国为民，杨过在最后领悟此道理，也解开了杀父之仇的心结。在经过无数挫折之后终于和小龙女走到一起，只是苦了那些“一见杨过误终身”的少女们~ 2018-05-03 《月亮与六便士》 “忘了是谁说过，为了让灵魂受益，每天应该做两件自己不喜欢的事情，说着这句话的是一个富有智慧的人，我把这句格言谨记在新，遵照行事，因此每天我醒来起床，每晚上床睡下” 毛姆这句话描述的境界是生活中所做的任何事都是自己发自肺腑开心的事，这比诗和远方的境界高多了。我承认自己不可能达到那种境界，只希望自己在低头捞便士的同时不要忘了抬头看一下明亮的月亮。 《1984》 &quot;Big brother is watching you.&quot;,原本影射苏联的一书放在今日天朝也并无太多违和。修宪之后，海康威视大涨就让我想到1984中的场景，依据相同的逻辑，中证传媒一定不会有大发展，军工企业虽然垃圾，但会有很大机会。不知道这书啥时候成为禁书。 《寻找无双》 王仙客在寻找无双的过程中困难重重，我们追寻的理想也同样如此吧，不过无所谓，重要的反而是追寻理想的过程吧！ 《革命时期的爱情》 所有混乱的事情，如果是放在革命时期也就显得不那么难以解释。X海鹰作为先进青年在挽救堕落青年的王二的时候怎么可以动歪心思！ 《爱你就像爱生命》 王小波李银河书信整理合集。李银河在书中表示说自己是回应了王小波热烈的感情，同时对他的爱也变得炽烈。最喜欢里面的两句活，“一想起你我这张丑脸上就泛起笑容”，“你好哇，李银河”。 2018-05-08 《蒋介石与现代中国》 讲述了委员长上位，完成中国形式上统一，抗日战争，国共内战，退守台湾五个阶段的大事。感受到那个动荡的时代波兰壮阔的历史，比历史课本中看到人物丰富立体。书中对抗日初期我党对抗日的态度，委员长对美玲的追求，退守台湾后两位大佬心照不宣共同薅美苏羊毛给我留下很深印象。最后留下在青岛花石楼里偶然看到的委员长一张诡异的微笑图。 《天才在左，疯子在右》 初读之后非常震撼，每个人的精神世界都是非常奇妙，即使是本人无法完全洞悉自己的全部想法。所谓精神病的概念，更多的是说那帮人的想法和社会主流价值观所要求的不一致。心理学是如此奇妙。 《三体》 印象最深的是第二部中黑暗丛林法则，这个原则放到现在的社会也不无道理。看书过程中不断为作者磅礴而又严谨的想象所感叹，科幻的同时还可以看到作者对人性的思考，很赞。 2018-05-15 《霍乱时期的爱情》 男主一生的主题就是对女主爱情的追求，在认识女主之后男主做的所有事情都是以赢得爱情为目标，和神雕侠侣里一见杨过毁终生的少女有些相似的感觉。书中描写了各种种类的爱情，读的同时还是感觉到和百年孤独类似的荒芜感，不同的是从这本书里还读到老爷子幽默的地方(比如女主从无限讨厌茄子，到无限热爱茄子，哈哈)。 看到豆瓣里很有意思的一个书评，“屌丝战胜高帅富的方法就是活得比高帅富久，所以屌丝们赶紧锻炼身体去吧” 2018-05-22 《谁在世界中心》 书中一直在强调地缘政治的重要性，大部分篇幅都在讲现在是海权时代，海洋强国才是真的强国。感到新奇的一点是如果继续全球变暖，北冰洋加速融化，俄罗斯会成为另外一个海洋大国，地缘优势会大大增加，那时候老毛子会更跳~ 《精进》 作为非常讨厌鸡汤以及成功学的我，在采铜的这本书中看到了超多的干货，里面很多思想和刘未鹏博客中的观点不谋而合。总之就是信息量超大的一本书。最近逛知乎看到采铜已经离开知乎了，这也并不奇怪，知乎现在已经是段子手和装逼犯集中的地方了，没那么多大佬了。 2018-05-25 《送你一颗子弹》 刘瑜老师才是真正文艺女青年，整本书大部分是刘瑜老师在06-08年的生活见闻，其中充满了对生活，爱情，政治，自由的思考，文笔朴实犀利又不乏幽默，很赞。相比起来，生活中大部分自诩文艺的女青年还是差距太大了，建议她们多读读马克思，少看些张小娴吧。 2018-06-01 《撒哈拉的故事》 上高中还是本科的时候看过一遍。感觉到三毛作为一个文艺女青年也是够燥够任性，好好的在西班牙享受现代资本主义的灯红酒绿突然要跑到撒哈拉沙漠去。到撒哈拉之后自力更生经营自己的生活，见识了好多的奇奇怪怪的习俗同时又冒了好多险。总的来说，再次读的过程中不断感受到这位文艺女青年强大的气场。 2018-07-06 《小王子》 小王子在各个星球旅行，遇到形形色色各式各样不能理解的大人。这对应到现实生活中也正是如此，我们做的大部分事情可能都是没啥实际意义或者说已经脱离原本的想法，拥有孩子的天真以及直接了当实在难能可贵。 ","link":"https://ce39906.github.io/post/读书短评/"},{"title":"Leveldb varint 解析","content":"内存那么大，不用就是浪费？ varint 介绍 我们知道 uint32_t 类型占用4个byte，uint64_t 占用8个byte, 但是对于比较小的数字来说，使用uint32_t 或者uint64_t 存储会比较浪费，varint 的思想是根据数字所需大小使用unsigned char* 指针存储数据，节约内存。 leveldb 实现 leveldb 中的varint实现原理简单，每个byte 使用最高bit的 0/1值代表此整数值是否结束，用剩下的7个bit 存储实际的数值。知道最后一个byte 的最高bit 是0 表示整数结束。因此小于128的数据都可以用一个byte 来表示，大于128的，比如说300，使用varint 编码的话需要两个字节10101100 0000 0010 leveldb 32位int变长编码实现 正常情况下，32位int占用4个byte, varint 编码中每个byte 中的最高bit 用来表示该byte 是不是最后一个byte,所以针对大的数varint 编码可能需要5个byte才能表示。leveldb 实现中5个if 分支对应varint占用1到5个byte 的情况。 char* EncodeVarint32(char* dst, uint32_t v) { // Operate on characters as unsigneds unsigned char* ptr = reinterpret_cast&lt;unsigned char*&gt;(dst); static const int B = 128; if (v &lt; (1&lt;&lt;7)) { *(ptr++) = v; } else if (v &lt; (1&lt;&lt;14)) { *(ptr++) = v | B; *(ptr++) = v&gt;&gt;7; } else if (v &lt; (1&lt;&lt;21)) { *(ptr++) = v | B; *(ptr++) = (v&gt;&gt;7) | B; *(ptr++) = v&gt;&gt;14; } else if (v &lt; (1&lt;&lt;28)) { *(ptr++) = v | B; *(ptr++) = (v&gt;&gt;7) | B; *(ptr++) = (v&gt;&gt;14) | B; *(ptr++) = v&gt;&gt;21; } else { *(ptr++) = v | B; *(ptr++) = (v&gt;&gt;7) | B; *(ptr++) = (v&gt;&gt;14) | B; *(ptr++) = (v&gt;&gt;21) | B; *(ptr++) = v&gt;&gt;28; } return reinterpret_cast&lt;char*&gt;(ptr); } 对应的varint 解码的思路是从低byte 到高byte遍历，直到找到最后一个表示编码结束的byte(判断条件是 byte &amp; 128 == 1)，代码如下 // 针对一个byte情况直接处理，大于一个byte 时， // 使用 GetVarint32PtrFallback 处理 inline const char* GetVarint32Ptr(const char* p, const char* limit, uint32_t* value) { if (p &lt; limit) { uint32_t result = *(reinterpret_cast&lt;const unsigned char*&gt;(p)); if ((result &amp; 128) == 0) { *value = result; return p + 1; } } return GetVarint32PtrFallback(p, limit, value); } const char* GetVarint32PtrFallback(const char* p, const char* limit, uint32_t* value) { uint32_t result = 0; for (uint32_t shift = 0; shift &lt;= 28 &amp;&amp; p &lt; limit; shift += 7) { uint32_t byte = *(reinterpret_cast&lt;const unsigned char*&gt;(p)); p++; if (byte &amp; 128) { // More bytes are present result |= ((byte &amp; 127) &lt;&lt; shift); } else { result |= (byte &lt;&lt; shift); *value = result; return reinterpret_cast&lt;const char*&gt;(p); } } return NULL; } 64位变长编码的实现 64位整形最多需要**(10 * 8 - 10 &gt; 64)** 10个byte 来保存，类似于32位int的编码，需要写10个if-else 分支，针对64位整形的编码，leveldb 给出了更优雅的解决方案。 char* EncodeVarint64(char* dst, uint64_t v) { static const int B = 128; unsigned char* ptr = reinterpret_cast&lt;unsigned char*&gt;(dst); while (v &gt;= B) { *(ptr++) = (v &amp; (B-1)) | B; v &gt;&gt;= 7; } *(ptr++) = static_cast&lt;unsigned char&gt;(v); return reinterpret_cast&lt;char*&gt;(ptr); } 针对64位整形的解码，leveldb 实现同样是类似的逻辑。也是从低位byte 开始向高位byte遍历判断编码是否结束。代码实现如下 const char* GetVarint64Ptr(const char* p, const char* limit, uint64_t* value) { uint64_t result = 0; for (uint32_t shift = 0; shift &lt;= 63 &amp;&amp; p &lt; limit; shift += 7) { uint64_t byte = *(reinterpret_cast&lt;const unsigned char*&gt;(p)); p++; if (byte &amp; 128) { // More bytes are present result |= ((byte &amp; 127) &lt;&lt; shift); } else { result |= (byte &lt;&lt; shift); *value = result; return reinterpret_cast&lt;const char*&gt;(p); } } return NULL; } 总结 varint 的思想是针对32位或者64位整形类型来说存储的大部分数据都是多余的，因此使用每个byte的最高位来标识编码是否结束，使用一个char 型指针存储编码的结果，针对很多情况可以节约存储空间。 ","link":"https://ce39906.github.io/post/Leveldb-varint-解析/"},{"title":"Memcached hashtable解析","content":"一问看懂hash表的原理 介绍 Memcached中实现了高性能的hashtable。其解决hash冲突的方法采用拉链法。当hashtable 中存储的item个数大于容器大小的1.5倍的时候通知线程进行hashtable 扩容，为了保证在扩容期间的读写性能，扩容线程默认每次只迁移一个bucket。设置一个变量标识当前的迁移进度，在进行读写操作时根据此变量确定是去 old_hashtable 还是 primary_hashtable 进行操作。 代码解析 主要接口 初始化 /*数组的大小都是2指数次幂，这样的好处是可以将 index % hashsize 变为 index &amp; hashmask*/ #define hashsize(n) ((ub4)1&lt;&lt;(n)) #define hashmask(n) (hashsize(n)-1) // 初始化函数的作用是为主表分配空间 void assoc_init(const int hashtable_init) { if (hashtable_init) { hashpower = hashtable_init; } primary_hashtable = calloc(hashsize(hashpower), sizeof(void *)); if (! primary_hashtable) { fprintf(stderr, &quot;Failed to init hashtable.\\n&quot;); exit(EXIT_FAILURE); } STATS_LOCK(); stats_state.hash_power_level = hashpower; stats_state.hash_bytes = hashsize(hashpower) * sizeof(void *); STATS_UNLOCK(); } 查找 hash 查找的逻辑是优先使用hash 预算定位到bucket,然后循环bucket 链表找到指定的key。需要理解的地方在于查找时可能存在hashtable 正在进行扩展，所以需要确定是在old_hashtable还是 primary_hashtable 进行查找。 item *assoc_find(const char *key, const size_t nkey, const uint32_t hv) { item *it; unsigned int oldbucket; /* expanding 标识扩展是否完成。 expand_bucket表示当前扩展的进度，使用位与操作查找bucket位置 */ if (expanding &amp;&amp; (oldbucket = (hv &amp; hashmask(hashpower - 1))) &gt;= expand_bucket) { it = old_hashtable[oldbucket]; } else { it = primary_hashtable[hv &amp; hashmask(hashpower)]; } item *ret = NULL; int depth = 0; // 循环单链表查找指定key while (it) { if ((nkey == it-&gt;nkey) &amp;&amp; (memcmp(key, ITEM_key(it), nkey) == 0)) { ret = it; break; } it = it-&gt;h_next; ++depth; } MEMCACHED_ASSOC_FIND(key, nkey, depth); return ret; } 插入 插入的主要逻辑是找到指定桶的位置，将当前插入的节点设置为桶中位置的链表头结点位置，并且重新设置桶中元素的value int assoc_insert(item *it, const uint32_t hv) { unsigned int oldbucket; // assert(assoc_find(ITEM_key(it), it-&gt;nkey) == 0); /* shouldn't have duplicately named things defined */ if (expanding &amp;&amp; (oldbucket = (hv &amp; hashmask(hashpower - 1))) &gt;= expand_bucket) { it-&gt;h_next = old_hashtable[oldbucket]; old_hashtable[oldbucket] = it; } else { it-&gt;h_next = primary_hashtable[hv &amp; hashmask(hashpower)]; primary_hashtable[hv &amp; hashmask(hashpower)] = it; } MEMCACHED_ASSOC_INSERT(ITEM_key(it), it-&gt;nkey); return 1; } 删除 删除接口的主要逻辑是使用**_hashitem_before** 函数找到要删除item前一个item指针位置，然后将此指针的位置直接指向被删除item 的下一个item 位置 void assoc_delete(const char *key, const size_t nkey, const uint32_t hv) { item **before = _hashitem_before(key, nkey, hv); if (*before) { item *nxt; /* The DTrace probe cannot be triggered as the last instruction * due to possible tail-optimization by the compiler */ MEMCACHED_ASSOC_DELETE(key, nkey); nxt = (*before)-&gt;h_next; (*before)-&gt;h_next = 0; /* probably pointless, but whatever. */ *before = nxt; return; } /* Note: we never actually get here. the callers don't delete things they can't find. */ assert(*before != 0); } 其他辅助函数 _hashitem_before 函数的作用是查找给定item的前一个节点的指针，在delete 接口中调用 static item** _hashitem_before (const char *key, const size_t nkey, const uint32_t hv) { item **pos; unsigned int oldbucket; // 同理是确定是在old_hashtable 还是在primary_hashtable if (expanding &amp;&amp; (oldbucket = (hv &amp; hashmask(hashpower - 1))) &gt;= expand_bucket) { pos = &amp;old_hashtable[oldbucket]; } else { pos = &amp;primary_hashtable[hv &amp; hashmask(hashpower)]; } // 从头结点的位置开始顺序遍历单链表中的节点 while (*pos &amp;&amp; ((nkey != (*pos)-&gt;nkey) || memcmp(key, ITEM_key(*pos), nkey))) { pos = &amp;(*pos)-&gt;h_next; } return pos; } assoc_expand 函数的作用是执行hash表的扩容，执行的过程是将当前primary_hashtable 指定为old_hashtable, 为primary_hashtable 分配内存,primary_hashtable的大小是old_hashtable 的两倍，将标识是否在扩展的bool型变量 expanding 设置为true。将标识扩展进度的变量expand_bucket设置为0 /* grows the hashtable to the next power of 2. */ static void assoc_expand(void) { old_hashtable = primary_hashtable; primary_hashtable = calloc(hashsize(hashpower + 1), sizeof(void *)); if (primary_hashtable) { if (settings.verbose &gt; 1) fprintf(stderr, &quot;Hash table expansion starting\\n&quot;); hashpower++; expanding = true; expand_bucket = 0; STATS_LOCK(); stats_state.hash_power_level = hashpower; stats_state.hash_bytes += hashsize(hashpower) * sizeof(void *); stats_state.hash_is_expanding = true; STATS_UNLOCK(); } else { primary_hashtable = old_hashtable; /* Bad news, but we can keep running. */ } } assoc_start_expand 函数的作用判断是否进行扩展，进行扩展的临界条件是hashtable 中item 个数大于hash 桶数的1.5倍。满足此临界条件时通知扩展线程进行扩展 void assoc_start_expand(uint64_t curr_items) { if (started_expanding) return; if (curr_items &gt; (hashsize(hashpower) * 3) / 2 &amp;&amp; hashpower &lt; HASHPOWER_MAX) { started_expanding = true; pthread_cond_signal(&amp;maintenance_cond); } } start_assoc_maintenance_thread 函数的作用是创建hash 扩展线程，可以根据用户指定的参数设置每次扩展多少个bucket。如果不指定此参数的话，默认每次只扩展一个bucket int start_assoc_maintenance_thread() { int ret; char *env = getenv(&quot;MEMCACHED_HASH_BULK_MOVE&quot;); if (env != NULL) { hash_bulk_move = atoi(env); if (hash_bulk_move == 0) { hash_bulk_move = DEFAULT_HASH_BULK_MOVE; } } pthread_mutex_init(&amp;maintenance_lock, NULL); if ((ret = pthread_create(&amp;maintenance_tid, NULL, assoc_maintenance_thread, NULL)) != 0) { fprintf(stderr, &quot;Can't create thread: %s\\n&quot;, strerror(ret)); return -1; } return 0; } assoc_maintenance_thread 函数的作用是执行实际的bucket 扩展。具体解释见注释 static void *assoc_maintenance_thread(void *arg) { mutex_lock(&amp;maintenance_lock); while (do_run_maintenance_thread) { int ii = 0; /* There is only one expansion thread, so no need to global lock. */ // 循环每次扩展的全部bucket for (ii = 0; ii &lt; hash_bulk_move &amp;&amp; expanding; ++ii) { item *it, *next; unsigned int bucket; void *item_lock = NULL; /* bucket = hv &amp; hashmask(hashpower) =&gt;the bucket of hash table * is the lowest N bits of the hv, and the bucket of item_locks is * also the lowest M bits of hv, and N is greater than M. * So we can process expanding with only one item_lock. cool! */ /* expand_bucket需要锁保护,由于处于同一个bucket 中的特性是这些item 的hv 的低N位是完全相同，对应的 item_lock 的位置靠hv 的低M位确定，由于item_lock 数组大小小于桶数组的大小，所以有 M &lt; N ,也就是说处 于同一个桶中的item拥有相同item_lock,所以在遍历桶中 所有的item 的时候不需要在额外获取item_lock。这里的 设计非常精妙~ */ if ((item_lock = item_trylock(expand_bucket))) { /* 遍历bucket 中全部item,插入到 primary_hashtable 中相应bucket */ for (it = old_hashtable[expand_bucket]; NULL != it; it = next) { next = it-&gt;h_next; bucket = hash(ITEM_key(it), it-&gt;nkey) &amp; hashmask(hashpower); it-&gt;h_next = primary_hashtable[bucket]; primary_hashtable[bucket] = it; } // old_hashtable 中bucket 内容设置为空 old_hashtable[expand_bucket] = NULL; // 维护当前扩展的进度 expand_bucket++; /* 如果扩展已经全部完成则设置expanding为 false ,释放old_hashtable 的内存*/ if (expand_bucket == hashsize(hashpower - 1)) { expanding = false; free(old_hashtable); STATS_LOCK(); stats_state.hash_bytes -= hashsize(hashpower - 1) * sizeof(void *); stats_state.hash_is_expanding = false; STATS_UNLOCK(); if (settings.verbose &gt; 1) fprintf(stderr, &quot;Hash table expansion done\\n&quot;); } } else { usleep(10*1000); } // 释放资源 if (item_lock) { item_trylock_unlock(item_lock); item_lock = NULL; } } // 如果不在进行扩展，则设置条件变量，等待被触发扩展 if (!expanding) { /* We are done expanding.. just wait for next invocation */ started_expanding = false; pthread_cond_wait(&amp;maintenance_cond, &amp;maintenance_lock); /* assoc_expand() swaps out the hash table entirely, so we need * all threads to not hold any references related to the hash * table while this happens. * This is instead of a more complex, possibly slower algorithm to * allow dynamic hash table expansion without causing significant * wait times. */ pause_threads(PAUSE_ALL_THREADS); assoc_expand(); pause_threads(RESUME_ALL_THREADS); } } return NULL; } 线程安全 memcached 使用分段锁实现hashtable 线程安全，分段锁避免了hashtable 中全部的item公用一个锁，公用一个锁的会降低hashtable 的读写性能。下面部分代码是memcached 初始化分段锁数组的逻辑。 if (nthreads &lt; 3) { power = 10; } else if (nthreads &lt; 4) { power = 11; } else if (nthreads &lt; 5) { power = 12; } else if (nthreads &lt;= 10) { power = 13; } else if (nthreads &lt;= 20) { power = 14; } else { /* 32k buckets. just under the hashpower default. */ power = 15; } /* 保证分段锁的数目小于hashtable 桶的个数，这样设计的好处之一 是在扩展的时候针对一个桶中的所有item 对应的是同一个 item_lock*/ if (power &gt;= hashpower) { fprintf(stderr, &quot;Hash table power size (%d) cannot be equal to or less than item lock table (%d)\\n&quot;, hashpower, power); fprintf(stderr, &quot;Item lock table grows with `-t N` (worker threadcount)\\n&quot;); fprintf(stderr, &quot;Hash table grows with `-o hashpower=N` \\n&quot;); exit(1); } item_lock_count = hashsize(power); item_lock_hashpower = power; // 分配分段锁数组 item_locks = calloc(item_lock_count, sizeof(pthread_mutex_t)); if (! item_locks) { perror(&quot;Can't allocate item locks&quot;); exit(1); } 在对hashtable 进行多线程读写时，首先需要根据hash 算法计算出hv 值，然后根据hv 获取item_lock,获取到item_lock 之后再进行读写操作。这也从侧面解释了为什么memcached在扩展时默认每次只扩展一个bucket，因为在进行扩展的时候需要占有item_lock，每次执行扩展的bucket 数多会影响读写性能。 总结 memcached 的hashtable是典型的拉链式hashtable,实现代码短小易读，使用一个线程进行hashtable的扩展以保证不会出现item增多导致哈希冲突激增降低读写性能的现象，除此之外使用分段锁来保证多线程的读写安全，相比全局锁也可以提升读写性能。memcached hashsize设置为2的整数次幂的设计非常精妙，首先这样可以将查找hash bucket索引的取余操作转化为对（hashsize-1）取按位与操作，在加上分段锁的数目大小小于hashsize 的设置可以保证一个bucket 中所有的item 对应于同一个分段锁，进而保证在扩展bucket中全部内容时只需要获取一次分段锁! ","link":"https://ce39906.github.io/post/Memcached-hashtable解析/"},{"title":"C++ 内存数据结构与二进制文件之间的序列化和反序列化","content":"二进制读写效率高 应用场景 许多后端检索server启动时候需要从文件加载到内存中构建索引，这个过程往往会消耗比较多的时间，这样会造成sever启动消耗比较多的时间，在存在多台服务器的时候会更加明显。 我们可以将够构建索引的过程独立成一个单独的进程，此进程实现的功能是根据原始文件构建索引结构，并将索引结构序列化到本地二进制文件，Server在启动的时候只需要读取二进制文件就可以构造出索引结构，可以大大提高启动速度。 示例代码 io.hpp ,对std::ifstream 以及std::ofstream 的封装，提供从vector序列化到二进制文件和从二进制文件反序列化到vector等接口 #ifndef IO_HPP #define IO_HPP #include &lt;string&gt; #include &lt;vector&gt; #include &lt;fstream&gt; class FileReader { public: FileReader(const std::string&amp; filename) : input_stream(filename,std::ios::binary) { } /* Read count objects of type T into pointer dest */ template &lt;typename T&gt; void ReadInto(T *dest, const std::size_t count) { static_assert(std::is_trivially_copyable&lt;T&gt;::value, &quot;bytewise reading requires trivially copyable type&quot;); if (count == 0) return; const auto &amp;result = input_stream.read(reinterpret_cast&lt;char *&gt;(dest), count * sizeof(T)); const std::size_t bytes_read = input_stream.gcount(); if (bytes_read != count * sizeof(T) &amp;&amp; !result) { return; } } template &lt;typename T&gt; void ReadInto(std::vector&lt;T&gt; &amp;target) { ReadInto(target.data(), target.size()); } template &lt;typename T&gt; void ReadInto(T &amp;target) { ReadInto(&amp;target, 1); } template &lt;typename T&gt; T ReadOne() { T tmp; ReadInto(tmp); return tmp; } std::uint32_t ReadElementCount32() { return ReadOne&lt;std::uint32_t&gt;(); } std::uint64_t ReadElementCount64() { return ReadOne&lt;std::uint64_t&gt;(); } template &lt;typename T&gt; void DeserializeVector(std::vector&lt;T&gt; &amp;data) { const auto count = ReadElementCount64(); data.resize(count); ReadInto(data.data(), count); } private: std::ifstream input_stream; }; class FileWriter { public: FileWriter(const std::string&amp; filename) : output_stream(filename,std::ios::binary) { } /* Write count objects of type T from pointer src to output stream */ template &lt;typename T&gt; void WriteFrom(const T *src, const std::size_t count) { static_assert(std::is_trivially_copyable&lt;T&gt;::value, &quot;bytewise writing requires trivially copyable type&quot;); if (count == 0) return; const auto &amp;result = output_stream.write(reinterpret_cast&lt;const char *&gt;(src), count * sizeof(T)); } template &lt;typename T&gt; void WriteFrom(const T &amp;target) { WriteFrom(&amp;target, 1); } template &lt;typename T&gt; void WriteOne(const T tmp) { WriteFrom(tmp); } void WriteElementCount32(const std::uint32_t count) { WriteOne&lt;std::uint32_t&gt;(count); } void WriteElementCount64(const std::uint64_t count) { WriteOne&lt;std::uint64_t&gt;(count); } template &lt;typename T&gt; void SerializeVector(const std::vector&lt;T&gt; &amp;data) { const auto count = data.size(); WriteElementCount64(count); return WriteFrom(data.data(), count); } private: std::ofstream output_stream; }; #endif binary_io.cpp #include &quot;io.hpp&quot; #include &lt;iostream&gt; struct Data { int a; double b; friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; out,const Data&amp; data) { out &lt;&lt; data.a &lt;&lt; &quot;,&quot; &lt;&lt; data.b; return out; } }; template&lt;typename T&gt; void printData(const std::vector&lt;T&gt;&amp; data_vec) { for (const auto data : data_vec) { std::cout &lt;&lt; &quot;{&quot; &lt;&lt; data &lt;&lt; &quot;} &quot;; } std::cout &lt;&lt; std::endl; } template&lt;typename T&gt; void serializeVector(const std::string&amp; filename,const std::vector&lt;T&gt;&amp; data_vec) { FileWriter file_writer(filename); file_writer.SerializeVector&lt;T&gt;(data_vec); } template&lt;typename T&gt; void deserializeVector(const std::string&amp; filename,std::vector&lt;T&gt;&amp; data_vec) { FileReader file_reader(filename); file_reader.DeserializeVector&lt;T&gt;(data_vec); } int main() { std::vector&lt;Data&gt; vec1 = {{1,1.1},{2,2.2},{3,3.3},{4,4.4}}; std::cout &lt;&lt; &quot;before write to binary file.\\n&quot;; printData(vec1); const std::string filename = &quot;vector_data&quot;; std::cout &lt;&lt; &quot;serialize vector to binary file.\\n&quot;; serializeVector&lt;Data&gt;(filename,vec1); std::vector&lt;Data&gt; vec2; deserializeVector&lt;Data&gt;(filename,vec2); std::cout &lt;&lt; &quot;vector read from binary file.\\n&quot;; printData(vec2); return 0; } 编译代码 g++ -std=c++11 binary_io.cpp -o binary_io 执行程序 ./binary_io 执行结果 程序将内存中vector 数据写入二进制文件，并从二进制文件中反序列化到一个新的vector。可以看到序列化前和序列化后的结果一致。 注意 序列化到文件的数据结构需要满足 is_trivially_copyable。std::is_trivially_copyable 在c++11 引入，TriviallyCopyable类型对象有以下性质 每个拷贝构造函数是trivial 或者是deleted 每个移动构造函数是trivial 或者是deleted 每个拷贝赋值运算符是trivial 或者是deleted 每个移动赋值运算符是trivial 或者是deleted 以上至少有一个是non-deleted 析构函数是trivial 并且non-deleted 对于is_trivially_copyable 类型对象的性质，解释如下 Objects of trivially-copyable types are the only C++ objects that may be safely copied with std::memcpy or serialized to/from binary files with std::ofstream::write()/std::ifstream::read(). In general, a trivially copyable type is any type for which the underlying bytes can be copied to an array of char or unsigned char and into a new object of the same type, and the resulting object would have the same value as the original 只有满足trivially-copyable的对象才可以保证序列化到二进制文件后， 从二进制文件反序列化到内存后的值保持不变。 ","link":"https://ce39906.github.io/post/C-内存数据结构与二进制文件之间的序列化和反序列化/"},{"title":"LRU Cache 解析及实现","content":"听说今日头条面试很喜欢问这个 LRU Cache 原理 LRU是Least Recently Used 的缩写，是一种缓存替换算法，当系统中的缓存满时，通过删除最近最少使用的元素来填充新的内容。LRU 算法在操作系统中有广泛应用，如CPU与物理内存之间的Cache替换算法， 物理内存与硬盘之间Cache替换算法。 LRU Cache 实现 我们可以使用一个双向链表和hash map 实现LRU Cache，其中hash map 存储缓存的内容，hash map 的value 存储指向双向链表节点的指针，双向链表存储缓存的内容，当有节点被加入时，该节点放到双向列表的头部，当访问已经存在的节点时，把该节点移动到双向链表头部，当双向链表满时，删除双向链表最后一个元素。简单的讲，双向链表的作用是记录缓存内容的使用顺序。 C++ 实现 /************************************************************************* &gt; File Name: lru_cache_template.hpp &gt; Author: ce39906 &gt; Mail: ce39906@163.com &gt; Created Time: 2018-04-02 16:44:58 ************************************************************************/ #ifndef LRU_CACHE_TEMPLATE_HPP #define LRU_CACHE_TEMPLATE_HPP #include &lt;unordered_map&gt; template &lt;class Key,class Value&gt; struct Node { Node(Key k,Value v) : key(k), value(v), prev(NULL), next(NULL) { } Node() { } Key key; Value value; Node* prev; Node* next; }; template &lt;class Key, class Value&gt; class LruCache { public: LruCache(int capacity) : size_(0), capacity_(capacity) { head = new Node&lt;Key,Value&gt;; tail = new Node&lt;Key,Value&gt;; head-&gt;next = tail; head-&gt;prev = NULL; tail-&gt;next = NULL; tail-&gt;prev = head; container_.clear(); }; ~LruCache() { while(head) { Node&lt;Key,Value&gt;* temp = head-&gt;next; delete head; head = temp; } }; void Put(Key key ,Value value) { //insert if (container_.find(key) == container_.end()) { //not full if (size_ != capacity_) { Node&lt;Key,Value&gt;* data = new Node&lt;Key,Value&gt;(key,value); attach(data); container_.insert(std::make_pair(key,data)); size_++; } else { Node&lt;Key,Value&gt;* temp = tail-&gt;prev; container_.erase(temp-&gt;key); detach(temp); if (temp) delete temp; Node&lt;Key,Value&gt;* data = new Node&lt;Key,Value&gt;(key,value); attach(data); container_.insert(std::make_pair(key,data)); } } else //update { Node&lt;Key,Value&gt;* data = container_[key]; detach(data); if (data) delete data; data = new Node&lt;Key,Value&gt;(key,value); container_[key] = data; attach(data); } } Value Get(Key key) { //find if (container_.find(key) != container_.end()) { Node&lt;Key,Value&gt;* data = container_[key]; detach(data); attach(data); return data-&gt;value; } else // not find { return Value(); } } private: int size_; int capacity_; std::unordered_map&lt;Key,Node&lt;Key,Value&gt;*&gt; container_; Node&lt;Key,Value&gt;* head; Node&lt;Key,Value&gt;* tail; void attach(Node&lt;Key,Value&gt;* node) { Node&lt;Key,Value&gt;* temp = head-&gt;next; head-&gt;next = node; node-&gt;next = temp; node-&gt;prev = head; temp-&gt;prev = node; } void detach(Node&lt;Key,Value&gt;* node) { node-&gt;prev-&gt;next = node-&gt;next; node-&gt;next-&gt;prev = node-&gt;prev; } }; #endif ","link":"https://ce39906.github.io/post/LRU-Cache-解析及实现/"},{"title":"C++ Thread Pool 使用解析","content":"各种线程池实现 boost::threadpool 按照boost标准开发的第三方库。下载地址在boost::threadpool 使用方法较为简单。例子如下 #include &lt;iostream&gt; #include &quot;boost/bind.hpp&quot; #include &quot;boost/threapool.hpp&quot; using namespace std; using namespace threadpool; void first_task() { cout &lt;&lt; &quot;first task&quot; &lt;&lt; endl; } void second_task() { cout &lt;&lt; &quot;second task&quot; &lt;&lt; endl; } void task_with_parameter(int value,string str) { cout &lt;&lt; &quot;task with parameter,value is: &quot; &lt;&lt; value &lt;&lt;&quot;,str is: &quot; &lt;&lt; str; } int main() { // 声明线程池 pool thread_pool(2); // 向线程池中添加任务 thread_pool.schedule(&amp;first_task); // 等待线程函数执行完成 thread_pool.wait(); thread_pool.schedule(&amp;second_task); thread_pool.schedule(boost::bind(task_with_parameter,8,&quot;hello&quot;)); thread_pool.wait(); return 0; } boost::threadpool 添加任务，同步方式都相对简单，在添加多参数的任务时候需要注意 boost::bind() 传递的参数是按照拷贝的方式传递的。如果想使用引用的方式传递的话，需要使用boost::ref() 或者boost::cref()。还有需要注意的是boost::bind()最多接受九个参数 boost::threadpool 是相对比较老的一个库,在2005-2008年间更新 boost::thread_group 以及io_service 例子 #include &lt;boost/asio/io_service.hpp&gt; #include &lt;boost::bind.hpp&gt; #include &lt;boost::thread/thread_group.hpp&gt; boost::asio::io_service ioService; boost::thread_group threadpool; /*The work class is used to inform the io_service when work starts and finishes. This ensures that the io_service object's run() function will not exit while work is underway, and that it does exit when there is no unfinished work remaining */ /* 声明一个ioService work 的原因是为了保证io service 的run方法在这个work销毁之前不会退出 */ boost::asio::io_service::work work(ioService); // 放在for循环中，根据线程池中线程中个数创建 // ioService 可以理解为任务队列 //Run the io_service object's event processing loop. threadpool.create_thread(boost::bind(&amp;boost::asio::ioservice::run,&amp;ioService)); threadpool.create_thread(boost::bind(&amp;boost::asio::ioservice::run,&amp;ioService)); // 向ioService 中提交任务 ioService.post(boost::bind(myTask,&quot;hello world&quot;)); ioService.post(boost::bind(myTask2,&quot;clear&quot;)); ... // ioService 在stop 之后，post到ioService中的task 都不会被执行 ioService.stop(); threadpool.join_all(); boost::ioservice 以及 boost::thread_pool 实现的thread pool没有提供wait() 方法，因此需要调用者主动判断所提交的任务有没有完成 基于c++11 实现的线程池 主要步骤如下： 设定线程池中所提供的服务线程数 int threads = thread::hardware_concurrency(); 每个线程都应该执行一个无限循环，无限循环中等待新任务到达，并执行任务 vector&lt;thread&gt; pool; for (int i = 0; i &lt; threads; i++) { pool.push_back(thread(Infinite_loop_function)); } 无限循环function while(true) { { unique_lock&lt;mutex&gt; lock(queue_mutex); condition.wait(lock,[]{return !Queue.empty()}); Task = Queue.front(); Queue.pop(); } Task(); } 向任务队列中添加任务 void enqueue(function&lt;void()&gt; new_task) { { unique_lock&lt;mutex&gt; lock(queue_mutex); Queue.push(new_task); } condition.notify_one(); } 具体实现例子 class ThreadPool { public: ThreadPool(size_t threads) : stop(false) { for(size_t i = 0;i&lt;threads;++i) workers.emplace_back( [this] { for(;;) { std::function&lt;void()&gt; task; { std::unique_lock&lt;std::mutex&gt; lock(this-&gt;queue_mutex); this-&gt;condition.wait(lock, [this]{ return this-&gt;stop || !this-&gt;tasks.empty(); }); if(this-&gt;stop &amp;&amp; this-&gt;tasks.empty()) return; task = std::move(this-&gt;tasks.front()); this-&gt;tasks.pop(); } task(); } } ); } // add new work item to the pool void enqueue(std::function&lt;void()&gt;&amp; task) { { std::unique_lock&lt;std::mutex&gt; lock(queue_mutex); // don't allow enqueueing after stopping the pool if(stop) throw std::runtime_error(&quot;enqueue on stopped ThreadPool&quot;); tasks.emplace(task); } condition.notify_one(); } ~ThreadPool() { { std::unique_lock&lt;std::mutex&gt; lock(queue_mutex); stop = true; } condition.notify_all(); for(std::thread &amp;worker: workers) worker.join(); } private: std::vector&lt; std::thread &gt; workers; // the task queue std::queue&lt; std::function&lt;void()&gt; &gt; tasks; // synchronization std::mutex queue_mutex; std::condition_variable condition; bool stop; }; 总结 从线程池的实现可以看到，线程在任务队列中获取任务以及向任务队列中提交任务都需要抢占队列的互斥锁，会造成时间损耗，尤其在***任务数多，每个任务需要的时间不是很长***的情况下，抢占任务队列互斥锁的时间损耗就显得更加明显。例如，在16核机器，线程池开启14个线程，向线程池中提交2000个task(每个task耗时1ms 左右)的情况下，向线程池提交任务所需时间约20ms。 因此，线程池的方式更适合***每个task消耗的时间比较长，任务数不是特别多的场景*** ","link":"https://ce39906.github.io/post/C-Thread-Pool-使用解析/"},{"title":"Boost Spirit","content":"我见过的最难懂的boost 库 What is boost spirit boost spirit is an object-oriented,recursive-descent parser and output generation library for C++.It allows you to write grammars and format descripting using a format similar to Extended Backs Naur Form(EBNF) directly in C++. The figure below shows the overall structure of Boost Spirit library. The three components Qi,Kama and Lex are designed to be used either stand alone or together. The general methodology is to use the token sequence generated by Lex as the input for a parser generated by Qi. On the opposite side of the equation,the hierarchical data structures generated by Qi are used for the output generators created using Kama. The place of Spirit.Qi and Spirit.Kama in a data transformation flow of a typical application. Qi- Writing Parsers Spirit.Qi is designed to be a practical parsing tool. Scanf ,boost::regex or boost::tokenizer do not scale well when we need to write more elaborate parsers. Examples Example #1 parsing a number Create a parser that will parse a floating-point number. double_ Example #2 parsing 2 numbers Create a parser that will accept a line consisting of 2 floating-point numbers. double_ &gt;&gt; double_ Example #3 parsing zero or more numbers Create a parser that will accept zero or more floating-point numbers. *double_ Example #4 parsing a comma-delimited list of numbers This example will create a parser that accepts a comma-delimited list of numbers. double_ &gt;&gt; *(char_(',') &gt;&gt; double_) Notice char_(',') is a literal charcter parser that can recognize the comma ','. Let's Parse We are done with defining the parser.So the next step is invoking this parser to do its work.Here we will use phrase_parse function. One overload of this function accepts four arguments. iterator pointing to the start of the input. iterator pointing to the end of the input. parser object another parser called the skip parser template&lt;typename Iterator&gt; bool parse_numbers(Iterator first, Iterator last) { using qi::double_; using qi::phrase_parse; using ascii::space; bool r = pharse_parse( first, last, double_ &gt;&gt; *(',' &gt;&gt; double_), space ); if (fisrt != last) return false; return r; } Parser Semantic Actions The previous example was very simple.It only recognized data,but did noting with it.Now we want to extract information from what was parsed. Semantic actions may be attached to any point in the grammar specification. These functions are C++ functions or function objects that are called whenever a part of the parser successfully recognizes a portion of the input. Example of Semantic Actions using plain function pointer using simple function object using boost.bind with a plain function using boost.bind with a member function using boost.lambda Such as: namespace client { namespace qi = boost::qi; // A plain function void print(const int&amp; i) { std::cout &lt;&lt; i &lt;&lt; std::endl; } // A member function struct writer { void print(const int&amp; i) const { std::cout &lt;&lt; i &lt;&lt; std::endl; } }; // A function object struct print_action { void operator()(const int&amp; i,qi::unset_type,qi::unset_type) const { std::cout &lt;&lt; i &lt;&lt; std::endl; } }; } All examples parse inputs of the form: &quot;{integer}&quot; These below shows the usages using boost::spirit::qi::int_; using boost::spirit::qi::parse; using client::print; using client::writer; using clinet::print_action; const char* fisrt = &quot;{43}&quot;; const char* last = first + std::strlen(first); // example using plain function parse(first,last,'{'&gt;&gt;int_[&amp;print]&gt;&gt;'}'); // example using simple function object parse(first,last,'{'&gt;&gt;int_[print_action()]&gt;&gt;'}'); // example using boost bind with a plain function parse(fisrt,last,'{' &gt;&gt; int_[boost::bind(&amp;print,_1)]&gt;&gt;'}'); // example using boost bind with a member function parse(first,last,'{'&gt;&gt; int_[boost::bind(&amp;writer::print,&amp;w,_1)]&gt;&gt;'}'); // example using boost lambda namespace lambda = boost::lambda; using lambda::_1; parse(first,last,'{' &gt;&gt; int_[std::cout &lt;&lt; _1 &lt;&lt; '\\n'] &gt;&gt; '}'); Phoenix Phoenix , a companion library bundled with Spirit. is sepcifically suited for binding semantic actions. It is like Boost.lambad on sterodis, with spiecial custom featrues that make it easy to inergrate semantic action with Spirit. Complex - Our first complex parser A parser that parses complex numbers.This time we are using Phoenix to do the semantic actions. Here is a simple parser expression for complex numbers. '(' &gt;&gt; double_ &gt;&gt; -(',' &gt;&gt; double_) &gt;&gt; ')' | double_ This parser can parse complex numbers of the form: (123.22,2121.21) (213.33) 212.33 This below shows example of action with phoniex namespace client { template&lt;typename Iterator&gt; bool parse_complex(Iterator first,Iterator last,std::complex&lt;double&gt;&amp; c) { using boost::spirit::qi::double_; using boost::spirit::qi::_1; using boost::spirit::qi::phrase_parse; using boost::spirit::ascii::space; using boost::phoenix::ref; double rN = 0.0; double iN = 0.0; bool r = phrase_parse( first, last, ( '(' &gt;&gt; double_[ref(rN) = _1] &gt;&gt; -(',' &gt;&gt; double_[ref(iN) = _1]) &gt;&gt; ')' | double_[ref(rN) = _1] ), space ); if (!r || first != last) return false; c = std::complex&lt;double&gt;(rN,iN); return r; } } Sum - adding numbers Here is a parser that sums a comma-separated list of numbers. namespace qi = boost::spirit::qi; namespace ascii = boost::spirit::ascii; namespace phoenix = boost::spirit::phoenix; using qi::double_; using qi::_1; using ascii::space; using phoenix::ref; template&lt;typename Iterator&gt; bool adder(Iterator first,Iterator last,double&amp; n) { bool r = qi::phrase_parse( first, last, ( double_[ref(n) = _1] &gt;&gt; *(',' &gt;&gt; double_[ref(n) += _1]) ), space ); if (fisrt != last) return false; return r; } Number List - stuffing numbers into a std::vector This sample demonstrates a parser for a comma separated list of numbers. The numbers are inserted in a vector using phoenix. template &lt;typename Iterator&gt; bool parse_numbers(Iterator fisrt,Iterator last,std::vector&lt;double&gt;&amp; v) { using qi::double_; using qi::phrase_parse; using qi::_1; using ascii::space; using phoenix::push_back; using phoenix::ref; bool r = phrase_parse( first, last, ( double_[push_back(ref(v),_1)] &gt;&gt; *(',' &gt;&gt; double_[push_back(ref(v),_1)]) ), space ); if(first != last) return false; return r; } Number List Redux - list syntax So far, we've been using the syntax: double_ &gt;&gt; *(',' &gt;&gt; double_) to parse a comma-delimited list of numbers.Such lists are common in parsing and Spirit provides a simpler shortcut for them double_ % ',' reads as a list of doubles separted by ','. The last example could be done as this: template&lt;typename Iterator&gt; bool parse_numbers(Iterator first,Iterator last,std::vector&lt;double&gt;&amp; v) { using qi::double_; using qi::phrase_parse; using qi::_1; using ascii::space; using phoenix::push_back; using phoenix::ref; bool r = phrase_parse( first, last, ( double_[ref(v),_1] % ',' ), space ); if (first != last) return false; return r; } Number List Attribute - one more,with style As we know,the double_ parser has a doubel attribute. All parsers have an attribute,even complex parsers. Our parser doubel_ % ',' has an attribute of std::vector. So we can simply pass in a std::vector to our number of list parser.the overload of phrase_parse has five arguments: iterator pointing to start of the input iterator pointing to last of the input the parser object another parser called skip parser the parse's attribute bool r = phrase_parse( first, last, ( double_ % ',' ), space, v ); Roman Numerals This example demonstrates: symbol table rule grammar Symbol table Each entry in a symbol table has an associated mutable data solt. In this regard, one can view the symbol table as an associative container of key-value pairs where the keys are strings. Here is a parser for roman hundreds(100..900) using the symbol table. Keep in mind that the data associated with each slot is the parser's attribute(which is passed to attached semantic actions). struct hundreds_ : qi::symbols&lt;char, unsigned&gt; { hundreds_() { add (&quot;C&quot;,100) (&quot;CC&quot;,200) (&quot;CCC&quot;,300) (&quot;CD&quot;,400) (&quot;D&quot;, 500) (&quot;DC&quot;, 600) (&quot;DCC&quot;,700) (&quot;DCCC&quot;,800) (&quot;CM&quot;,900); } } hundreds; we also can define tens ones symbol table.They are all parsers. Rules Up until now,we have been inlining our parser expressions, passing them directly to the phrase_parse function. The expression evalutes into a temporary,unnamed parser which is passed into the phrase_parse function, used and then destroyed. This is fine for small parsers. When the expressions get complicated, you'd want to break the expressions into smaller easier-to-understand pieces, name them, and refer to them from other expressions by name. A parser expression can be assigned to what is called a &quot;rule&quot;.Threr are various ways to declare rules.The simplest form is : rule&lt;Iterator&gt; r; // this rule cannot used by phrase_parse function, // it can only be used by parse function -- a version that does not do white space skipping. // If you want to have it skip white spaces,you need to pass in the type skip parser. rule&lt;Iterator,Skipper&gt; r; rule&lt;string::iterator,space_type&gt; r; // This type of rule can be used for both // phrase_parse and parse. There is one more rule form you should know about. rule&lt;Iterator,Signature,Skipper&gt; r; The Signature specifies athe attributes of the rule. Recall that the double_ parser has an attribute of double. To be precise, these are synthesized attributes.The parser &quot;synthesized&quot; the attribute value.Think of them as the function return value. There is another type of attribute called &quot;inherited&quot; attribute. You can think them as function arguments. And,rightly so, the rule signature is a function signature . After having declared a rule,you can now assign any parser expression to it. Example: r = double_ &gt;&gt; *(',' &gt;&gt; double_); Grammars A grammar encapsulates one or more rules.It has the same template parameters as the rule.You can declare a grammar by: deriving a struct from the grammar class template declare one or more rules as member variables initialize the base grammar class by giving it the start rule(its the first rule that gets called when the grammar starts parsing) initalize your rules in your constrctor. The rommon numeral grammar is a very nice and simple example of a grammar. template&lt;typename Iterator&gt; struct roman :qi::grammar&lt;Iterator,unsigned()&gt; { roman() : roman::base_type(start) { using qi::eps; using qi::lit; using qi::_val; using qi::_1; using ascii::char_; start = eps [_val = 0] &gt;&gt; ( +lit('M') [_val += 1000] || hundreds [_val += _1] || tens [_val += _1] || ones [_val += _1] ) } qi::rule&lt;Iterator,unsigned()&gt; start; } the grammar and start rule sinature is unsigned() it has a synthesized attribute(return value) of type unsigned whith on inherited attributeds(argumnents). roman::base_type is a typedef for grammar&lt;Iterator,unsigned()&gt; _val is another Phoenix placeholder representing the rule's synthesized attribute eps is a special spirit parser that consumes no input but is always successful. We use it to initialize _val,the rule's synthesized attribute, to zero before anything else. Using eps this way is good for doing pre and post initializations. the example a || b reads, match a or b and in sequence.That is if both a and b match, it must be in sequence; this is equivalen to a &gt;&gt; -b | b,but more efficient. Usage bool r = parse(iter,end,roman_parser,result); if (r &amp;&amp; iter == end) { std::cout &lt;&lt; &quot;Parse success.\\n&quot;; std::cout &lt;&lt; &quot;Result = &quot; &lt;&lt; result &lt;&lt; std::endl; } Employee - Pasing into structs This section shows how to parse and place the reult into a C++ struct. fisrtly, let's create a struct representing an employee. struct employee { int age; std::string surname; std::string forename; double salary; }; now we will write a parser for our employee. Inputs will be of the form. employee{ age,&quot;surname&quot;,&quot;forename&quot;,salary } template&lt;typename Iterator&gt; struct employee_parser : qi::grammar&lt;Iterator,employee(),ascii::space_type&gt; { employee_parser():employee_parser::base_type(start) { using qi::int_; using qi::lit; using qi::double_; using qi::lexeme; using ascii::char_; quoted_string %= lexeme['&quot;' &gt;&gt; +(char_ - '&quot;') &gt;&gt; '&quot;']; start %= lit(&quot;employee&quot;) &gt;&gt; '{' &gt;&gt; int_ &gt;&gt; ',' &gt;&gt; quoted_string &gt;&gt; ',' &gt;&gt; quoted_string &gt;&gt; ',' &gt;&gt; double_ &gt;&gt; '}' ; } qi::rule&lt;Iterator,std::string(),ascii::space_type&gt; quoted_string; qi::rule&lt;Iterator,employee(),ascii::space_type&gt; start; }; Lexeme lexeme['&quot;' &gt;&gt;(char_ - '&quot;') &gt;&gt; '&quot;'] lexeme inhibits sapce skipping from the open brace to the closing space.The expression parses quoted strings. +(char_ - '&quot;') parses one or more chars,except the double quote. It stops when it sees a double quote. +a matches one or more,its attribute is a std::vector where A is the attribute of a . Sequence Attribute now what's the attribute of '&quot;' &gt;&gt; (char_ - '&quot;') &gt;&gt;'&quot;' a &gt;&gt; b &gt;&gt; c fusion::vector&lt;A,B,C&gt; // is a tuple Some parser,especially those very little parsers like '&quot;' do not have attributes. Nodes without attributes are disregarded. so, '&quot;' &gt;&gt; (char_ - '&quot;') &gt;&gt;'&quot;' 's attribue is fusion::vector&lt;std::vector&lt;char&gt;&gt; but there is one more collpase rule.if the attribute is followed by a single element fusion::vector, The element is stripped naked from its container. so the attribute come to this std::vector&lt;char&gt; Auto Rules it is typical to see rules like r = p[_val = _1] if you have a rule definiton such as the above,where the attribute of the right hand side of the rule is compitable with the left hand side.then you can rewrite is as: r %= p; so quoted_string %= lexeme['&quot;' &gt;&gt; +( char_ -'&quot;') &gt;&gt; '&quot;']; is simple version of quoted_string = lexeme['&quot;' &gt;&gt; + (char_ - '&quot;') &gt;&gt; '&quot;'][_val = _1]; Note: r %= p and r = p are equivalent if there are no semantic actions associated with p. In case you are wondering, lit(&quot;employee&quot;) is the same as &quot;employee&quot;. We had to wrap it inside lit because immediately after it is &gt;&gt; '{'. You can't right-shift a char[] and a char - you know, C++ syntax rules. Karma - Writing Generators Spirit.Karma - what's that? Spirit.Karma is the counterpart to spirit.qi. Some people say it's the Yin to Spirit.Qi's Yang. Spirit.karma is generating byte sequences from internal data structures as Spirit.Qi is parsing byte sequences into those internal data structures. Why should you use a generator library for such a simple thing as output generation? Programmers have been using printf, std::stream formatting, or boost::format for quite some time. The answer is - yes, for simple output formatting tasks those familiar tools might be a quick solution. But experience shows: as soon as the formatting requirements are becoming more complex output generation is getting more and more challenging in terms of readability, maintainability, and flexibility of the code. Last, but not least, it turns out that code using Spirit.Karma runs much faster than equivalent code using either of the 'straight' methods mentioned above. In terms of development simplicity and ease in deployment, the same is true for Spirit.Karma as has been described elsewhere in this documentation for Spirit.Qi: the entire library consists of only header files, with no libraries to link against or build. Just put the spirit distribution in your include path, compile and run. Code size? Very tight, essentially comparable to hand written code. Examples Trivial Example Generating a number double_ Generating two numbers double_ &lt;&lt; double_ Generating one or more numbers *double_ Generating a comma-delimited list of numbers double_ &lt;&lt; *(lit(',')) &lt;&lt; double_ Let's generate template&lt;typename OutputIterator&gt; bool generate_numbers(OutputIterator&amp; sink,std::list&lt;double&gt; const&amp; v) { using karma::double_; using karma::generate_delimited; using ascii::space; bool r = genated_delimited( sink, double_ &lt;&lt; *(',' &lt;&lt; double_), v ); return r; } ","link":"https://ce39906.github.io/post/Boost-Spirit/"},{"title":"OpenMp Tutorial","content":"简单易用的多线程工具 What is openmp easy multithreading programing for c++. it is a simple C/C++/Fortran complier extension that allows to add parallelism into existing source code without significantly having to rewrite it. Example example for init an array #include&lt;iostream&gt; #include&lt;vector&gt; int main() { vector&lt;int&gt; arr; arr.reserve(1000); #pargma omp parallel for for(int i=0;i&lt;1000;i++) { arr[i] = i * 2; } return 0; } you can compile it like this g++ tmp.cpp -fopenmp if you remove the #pragma lines,the result is still a valid C++ program that runs and dose the expected thing. if the compiler encounters a #pragma that it dose not support,it will ignore it. The syntax the parallel construct it creates a team of N threads(where N is the number of CPU cores by default) . after the statement, the threads join back into one. #pragma omp parallel { // code inside thie region runs in parallel printf(&quot;Hello\\n&quot;); } Loop construct: for the for construct splits the for-loop so that each thread in the current team handles a different portion of the loop #pragma omp for for(int n=0;n&lt;10;++n) { printf(&quot; %d&quot;,n); } Note:#pragma omp for onlt delegates portions of the loop for different threads in current team. a team is the group of threads excuting the program.At program start,the team only consists the main thread. To create a new team of threads,you need to specify the parallel keyword #pragma omp parallel { #pragma omp for for(int n=0;n&lt;10;n++) printf(&quot; %d&quot;,n) } or use #pragma omp parallel for you can explicitly specify the number of threads to be created in the team. using num_threads #pragma omp parallel for num_threads(3) scheduling The scheduling algorithm for the for-loop can explicity controlled default #pargma omp for schedule(static) in the dynamic schedule,each thread ask the omp runtime library for an iteration number,then handles it. the chunk size can also be specified to lessen the number of calls to the runtime library #pargma omp parallel for schedule(dynamic,3) the ordered clause it is possible to force that certain events within the loop happen in a predicted order, using ordered clause #pargma omp parallel for ordered shcedule(dynamic) for(int n = 0;i &lt; 100;i++) { files[n].compress(); #pragma omp ordered send(files[n]); } the collapse clause when you have nested loops.you can use collapse #pargma omp parallel for collapse(2) for(int i = 0;i &lt; 10;i++) { for(int j = 0;j &lt; 10;j++) { doSth(); } } section sometimes,it is handy to indicate that &quot;this and this can run in parallel&quot; the sectiongs is just for that #pragma omp parallel sections { { work1(); } #pragma omp section { work2(); work3(); } #pragma omp section { work4(); } } This code indicates that any of tasks work1,work2+work3,work4 can run in parallel. Thread-safety Atomicity #pragma omp atomic couter += value; the atomic keyword in OpenMP specifies that denoted action happens atomically. atomic read expression #pragma omp atomic read var = x; atomic write expression #pragma omp atomic write x = expr; atomic update expression #pragma omp atomic update ++x;--x;x++;x--; +=,-= ... atomic capture expression capture expression combine the read and update features #pragma omp atomic capture var = x++; the critical construct the critical construct restricts the execution of the associated statement / block to a single thread at a time #pragma omp critical { doSth(); } Note:the critical section names are global to the entire program. locks The openmp runtime library provides a lock type,omp_lock_t in omp.h the lock type has five manipulator functions omp_init_lock : initializes the lock omp_destory_lock : the lock must be unset before the call omp_set_lock: get the lock omp_unset_lock: release the lock omp_test_lock: attempts to set the lock.if the lock is already set by another thread it returns 0;if it managed to set the lock,it return 1 the flush directive Even when variables used by threads are supposed to be shared,the compiler may take liberties and optimize them as register variables. This can skew concurrent observations of variable. The flush directive can be used to forbid this. /*first thread*/ b=1; #pragma omp flush(a,b) if(a == 0) { /* critical section*/ } /*second thread*/ a = 1; #pragma omp flush(a,b) if(b==0) { /* critical section*/ } Controlling which data to share between threads int a,b =0; #pragma omp parallel for private(a) shared(b) for(a=0;a&lt;50;++a) { #pragma omp atomic b += a; } a is private(each thread has their own copy of it) and b is shared(each thread accesses the same variable) the difference between private and firstprivate private does not copy the value of the variable that was in the surrounding context. #include&lt;string&gt; #include&lt;iostream&gt; int main() { std::string a = &quot;x&quot;,b=&quot;y&quot;; #pragma omp parallel private(a,c) shared(b) num_threads(2) { a+=&quot;k&quot;; c+= 7; std::cout &lt;&lt; &quot;A is &quot; &lt;&lt; a &lt;&lt;&quot;, b is &quot;&lt;&lt; b; } } // eaquls this below OpenMP_thread_fork(2); { // Start new scope std::string a; // Note: It is a new local variable. int c; // This too. a += &quot;k&quot;; c += 7; std::cout &lt;&lt; &quot;A becomes (&quot; &lt;&lt; a &lt;&lt; &quot;), b is (&quot; &lt;&lt; b &lt;&lt; &quot;)\\n&quot;; } // End of scope for the local variables OpenMP_join(); If you actually need a copy of the original value, use the firstprivate clause instead. #include &lt;string&gt; #include &lt;iostream&gt; int main() { std::string a = &quot;x&quot;, b = &quot;y&quot;; int c = 3; #pragma omp parallel firstprivate(a,c) shared(b) num_threads(2) { a += &quot;k&quot;; c += 7; std::cout &lt;&lt; &quot;A becomes (&quot; &lt;&lt; a &lt;&lt; &quot;), b is (&quot; &lt;&lt; b &lt;&lt; &quot;)\\n&quot;; } } Execution synchronization the barrier directive and the nowait clause The barrier directive causes threads encoutering the barrier to wait until all the other threads in the same team have encountered the barrier. #pragma omp parrllel { // all threads execute this SomeCode(); #pragma omp barrier // all threads execute this,but not before all threads have finished executing SomeCode() SomeMoreCode(); } Note:there is an implicit barrier at the end of each parallel block,at the end of each section for statement #pragma omp parallel { #pragma omp for for(int n=0; n&lt;10; ++n) Work(); // This line is not reached before the for-loop is completely finished SomeMoreCode(); } // This line is reached only after all threads from // the previous parallel block are finished. CodeContinues(); #pragma omp parallel { #pragma omp for nowait for(int n=0; n&lt;10; ++n) Work(); // This line may be reached while some threads are still executing the for-loop. SomeMoreCode(); } // This line is reached only after all threads from // the previous parallel block are finished. CodeContinues(); the single and master constructs The single construct specifies that the given statement/block is executed by only one thread. It is unspecified which thread. Other threads skip the statement/block and wait at an implicit barrier at the end of the construct. #pragma omp parallel { Work1(); #pragma omp single { Work2(); } Work3(); } The master construct is similar, except that the statement/block is run by the master thread, and there is no implied barrier; other threads skip the construct without waiting. static const char* FindAnyNeedle(const char* haystack, size_t size, char needle) { const char* result = haystack+size; #pragma omp parallel { unsigned num_iterations=0; #pragma omp for for(size_t p = 0; p &lt; size; ++p) { ++num_iterations; if(haystack[p] == needle) { #pragma omp atomic write result = haystack+p; // Signal cancellation. #pragma omp cancel for } // Check for cancellations signalled by other threads: #pragma omp cancellation point for } // All threads reach here eventually; sooner if the cancellation was signalled. printf(&quot;Thread %u: %u iterations completed\\n&quot;, omp_get_thread_num(), num_iterations); } return result; } Loop nesting this code will not do the excepted thing #pragma omp parallel for for(int y=0; y&lt;25; ++y) { #pragma omp parallel for for(int x=0; x&lt;80; ++x) { tick(x,y); } } solution #pragma omp parallel for collapse(2) for(int y=0; y&lt;25; ++y) { for(int x=0; x&lt;80; ++x) { tick(x,y); } } Readmore http://www.openmp.org/wp-content/uploads/openmp-4.5.pdf https://en.wikipedia.org/wiki/OpenMP ","link":"https://ce39906.github.io/post/OpenMp-Tutorial/"}]}